Springer Series in Statistics
Trevor Hastie • Robert Tibshirani • Jerome Friedman
The Elements of Statictical Learning
During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book’s coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting—the first comprehensive treatment of this topic in any book.
This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression & path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for “wide” data (p bigger than n), including multiple testing and false discovery rates.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie codeveloped much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many datamining tools including CART, MARS, projection pursuit and gradient boosting.
S TAT I S T I C S
 ----
› springer.com

The Elements of Statistical Learning

Hastie • Tibshirani • Friedman

Springer Series in Statistics
Trevor Hastie Robert Tibshirani Jerome Friedman
The Elements of Statistical Learning
Data Mining, Inference, and Prediction
Second Edition
Corrected 12th printing - Jan 13, 2017

This is page v Printer: Opaque this
To our parents: Valerie and Patrick Hastie Vera and Sami Tibshirani Florence and Harry Friedman
and to our families: Samantha, Timothy, and Lynda Charlie, Ryan, Julie, and Cheryl Melanie, Dora, Monika, and Ildiko

vi

Preface to the Second Edition

This is page vii Printer: Opaque this

In God we trust, all others bring data.
–William Edwards Deming (1900-1993)1
We have been gratiﬁed by the popularity of the ﬁrst edition of The Elements of Statistical Learning. This, along with the fast pace of research in the statistical learning ﬁeld, motivated us to update our book with a second edition.
We have added four new chapters and updated some of the existing chapters. Because many readers are familiar with the layout of the ﬁrst edition, we have tried to change it as little as possible. Here is a summary of the main changes:
1On the Web, this quote has been widely attributed to both Deming and Robert W. Hayden; however Professor Hayden told us that he can claim no credit for this quote, and ironically we could ﬁnd no “data” conﬁrming that Deming actually said this.

viii Preface to the Second Edition

Chapter 1. Introduction 2. Overview of Supervised Learning 3. Linear Methods for Regression
4. Linear Methods for Classiﬁcation 5. Basis Expansions and Regularization 6. Kernel Smoothing Methods 7. Model Assessment and Selection
8. Model Inference and Averaging 9. Additive Models, Trees, and Related Methods 10. Boosting and Additive Trees
11. Neural Networks
12. Support Vector Machines and Flexible Discriminants 13. Prototype Methods and Nearest-Neighbors 14. Unsupervised Learning
15. Random Forests 16. Ensemble Learning 17. Undirected Graphical Models 18. High-Dimensional Problems

What’s new
LAR algorithm and generalizations of the lasso Lasso path for logistic regression Additional illustrations of RKHS
Strengths and pitfalls of crossvalidation
New example from ecology; some material split oﬀ to Chapter 16. Bayesian neural nets and the NIPS 2003 challenge Path algorithm for SVM classiﬁer
Spectral clustering, kernel PCA, sparse PCA, non-negative matrix factorization archetypal analysis, nonlinear dimension reduction, Google page rank algorithm, a direct approach to ICA New New New New

Some further notes:

• Our ﬁrst edition was unfriendly to colorblind readers; in particular, we tended to favor red/green contrasts which are particularly troublesome. We have changed the color palette in this edition to a large extent, replacing the above with an orange/blue contrast.

• We have changed the name of Chapter 6 from “Kernel Methods” to “Kernel Smoothing Methods”, to avoid confusion with the machinelearning kernel method that is discussed in the context of support vector machines (Chapter 12) and more generally in Chapters 5 and 14.

• In the ﬁrst edition, the discussion of error-rate estimation in Chapter 7 was sloppy, as we did not clearly diﬀerentiate the notions of conditional error rates (conditional on the training set) and unconditional rates. We have ﬁxed this in the new edition.

Preface to the Second Edition ix
• Chapters 15 and 16 follow naturally from Chapter 10, and the chapters are probably best read in that order.
• In Chapter 17, we have not attempted a comprehensive treatment of graphical models, and discuss only undirected models and some new methods for their estimation. Due to a lack of space, we have speciﬁcally omitted coverage of directed graphical models.
• Chapter 18 explores the “p ≫ N ” problem, which is learning in highdimensional feature spaces. These problems arise in many areas, including genomic and proteomic studies, and document classiﬁcation.
We thank the many readers who have found the (too numerous) errors in the ﬁrst edition. We apologize for those and have done our best to avoid errors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry Wasserman for comments on some of the new chapters, and many Stanford graduate and post-doctoral students who oﬀered comments, in particular Mohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Maleki, Donal McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and Hui Zou. We thank John Kimmel for his patience in guiding us through this new edition. RT dedicates this edition to the memory of Anna McPhee.
Trevor Hastie Robert Tibshirani Jerome Friedman
Stanford, California August 2008

x Preface to the Second Edition

Preface to the First Edition

This is page xi Printer: Opaque this

We are drowning in information and starving for knowledge.
–Rutherford D. Roger
The ﬁeld of Statistics is constantly challenged by the problems that science and industry brings to its door. In the early days, these problems often came from agricultural and industrial experiments and were relatively small in scope. With the advent of computers and the information age, statistical problems have exploded both in size and complexity. Challenges in the areas of data storage, organization and searching have led to the new ﬁeld of “data mining”; statistical and computational problems in biology and medicine have created “bioinformatics.” Vast amounts of data are being generated in many ﬁelds, and the statistician’s job is to make sense of it all: to extract important patterns and trends, and understand “what the data says.” We call this learning from data.
The challenges in learning from data have led to a revolution in the statistical sciences. Since computation plays such a key role, it is not surprising that much of this new development has been done by researchers in other ﬁelds such as computer science and engineering.
The learning problems that we consider can be roughly categorized as either supervised or unsupervised. In supervised learning, the goal is to predict the value of an outcome measure based on a number of input measures; in unsupervised learning, there is no outcome measure, and the goal is to describe the associations and patterns among a set of input measures.

xii Preface to the First Edition
This book is our attempt to bring together many of the important new ideas in learning, and explain them in a statistical framework. While some mathematical details are needed, we emphasize the methods and their conceptual underpinnings rather than their theoretical properties. As a result, we hope that this book will appeal not just to statisticians but also to researchers and practitioners in a wide variety of ﬁelds.
Just as we have learned a great deal from researchers outside of the ﬁeld of statistics, our statistical viewpoint may help others to better understand diﬀerent aspects of learning:
There is no true interpretation of anything; interpretation is a vehicle in the service of human comprehension. The value of interpretation is in enabling others to fruitfully think about an idea.
–Andreas Buja
We would like to acknowledge the contribution of many people to the conception and completion of this book. David Andrews, Leo Breiman, Andreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton, Werner Stuetzle, and John Tukey have greatly inﬂuenced our careers. Balasubramanian Narasimhan gave us advice and help on many computational problems, and maintained an excellent computing environment. Shin-Ho Bang helped in the production of a number of the ﬁgures. Lee Wilkinson gave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Maya Gupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bogdan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu Zhu, two reviewers and many students read parts of the manuscript and oﬀered helpful suggestions. John Kimmel was supportive, patient and helpful at every phase; MaryAnn Brickner and Frank Ganz headed a superb production team at Springer. Trevor Hastie would like to thank the statistics department at the University of Cape Town for their hospitality during the ﬁnal stages of this book. We gratefully acknowledge NSF and NIH for their support of this work. Finally, we would like to thank our families and our parents for their love and support.
Trevor Hastie Robert Tibshirani Jerome Friedman
Stanford, California May 2001
The quiet statisticians have changed our world; not by discovering new facts or technical developments, but by changing the ways that we reason, experiment and form our opinions ....
–Ian Hacking

Contents

This is page xiii Printer: Opaque this

Preface to the Second Edition

vii

Preface to the First Edition

xi

1 Introduction

1

2 Overview of Supervised Learning

9

2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9

2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9

2.3 Two Simple Approaches to Prediction:

Least Squares and Nearest Neighbors . . . . . . . . . . . 11

2.3.1 Linear Models and Least Squares . . . . . . . . 11

2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14

2.3.3 From Least Squares to Nearest Neighbors . . . . 16

2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18

2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22

2.6 Statistical Models, Supervised Learning

and Function Approximation . . . . . . . . . . . . . . . . 28

2.6.1 A Statistical Model

for the Joint Distribution Pr(X, Y ) . . . . . . . 28

2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29

2.6.3 Function Approximation . . . . . . . . . . . . . 29

2.7 Structured Regression Models . . . . . . . . . . . . . . . 32

2.7.1 Diﬃculty of the Problem . . . . . . . . . . . . . 32

xiv Contents

2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33 2.8.1 Roughness Penalty and Bayesian Methods . . . 34 2.8.2 Kernel Methods and Local Regression . . . . . . 34 2.8.3 Basis Functions and Dictionary Methods . . . . 35
2.9 Model Selection and the Bias–Variance Tradeoﬀ . . . . . 37 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

3 Linear Methods for Regression

43

3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43

3.2 Linear Regression Models and Least Squares . . . . . . . 44

3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49

3.2.2 The Gauss–Markov Theorem . . . . . . . . . . . 51

3.2.3 Multiple Regression

from Simple Univariate Regression . . . . . . . . 52

3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56

3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57

3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57

3.3.2 Forward- and Backward-Stepwise Selection . . . 58

3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60

3.3.4 Prostate Cancer Data Example (Continued) . . 61

3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61

3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61

3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68

3.4.3 Discussion: Subset Selection, Ridge Regression

and the Lasso . . . . . . . . . . . . . . . . . . . 69

3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73

3.5 Methods Using Derived Input Directions . . . . . . . . . 79

3.5.1 Principal Components Regression . . . . . . . . 79

3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80

3.6 Discussion: A Comparison of the Selection

and Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82

3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84

3.8 More on the Lasso and Related Path Algorithms . . . . . 86

3.8.1 Incremental Forward Stagewise Regression . . . 86

3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89

3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89

3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90

3.8.5 Further Properties of the Lasso . . . . . . . . . . 91

3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92

3.9 Computational Considerations . . . . . . . . . . . . . . . 93

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94

Contents xv

4 Linear Methods for Classiﬁcation

101

4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101

4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103

4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106

4.3.1 Regularized Discriminant Analysis . . . . . . . . 112

4.3.2 Computations for LDA . . . . . . . . . . . . . . 113

4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113

4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119

4.4.1 Fitting Logistic Regression Models . . . . . . . . 120

4.4.2 Example: South African Heart Disease . . . . . 122

4.4.3 Quadratic Approximations and Inference . . . . 124

4.4.4 4.4.5

L1 Regularized Logistic Regression . . . . . . . . 125 Logistic Regression or LDA? . . . . . . . . . . . 127

4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129

4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130

4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135

5 Basis Expansions and Regularization

139

5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139

5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141

5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144

5.2.2 Example: South African Heart Disease (Continued)146

5.2.3 Example: Phoneme Recognition . . . . . . . . . 148

5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150

5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151

5.4.1 Degrees of Freedom and Smoother Matrices . . . 153

5.5 Automatic Selection of the Smoothing Parameters . . . . 156

5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158

5.5.2 The Bias–Variance Tradeoﬀ . . . . . . . . . . . . 158

5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161

5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162

5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167

5.8.1 Spaces of Functions Generated by Kernels . . . 168

5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170

5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174

5.9.1 Wavelet Bases and the Wavelet Transform . . . 176

5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181

Appendix: Computational Considerations for Splines . . . . . . 186

Appendix: B-splines . . . . . . . . . . . . . . . . . . . . . 186

Appendix: Computations for Smoothing Splines . . . . . 189

xvi Contents

6 Kernel Smoothing Methods

191

6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192

6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194

6.1.2 Local Polynomial Regression . . . . . . . . . . . 197

6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198 6.3 Local Regression in IRp . . . . . . . . . . . . . . . . . . . 200 6.4 Structured Local Regression Models in IRp . . . . . . . . 201

6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203

6.4.2 Structured Regression Functions . . . . . . . . . 203

6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205

6.6 Kernel Density Estimation and Classiﬁcation . . . . . . . 208

6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208

6.6.2 Kernel Density Classiﬁcation . . . . . . . . . . . 210

6.6.3 The Naive Bayes Classiﬁer . . . . . . . . . . . . 210

6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212

6.8 Mixture Models for Density Estimation and Classiﬁcation 214

6.9 Computational Considerations . . . . . . . . . . . . . . . 216

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216

7 Model Assessment and Selection

219

7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219

7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219

7.3 The Bias–Variance Decomposition . . . . . . . . . . . . . 223

7.3.1 Example: Bias–Variance Tradeoﬀ . . . . . . . . 226

7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228

7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230

7.6 The Eﬀective Number of Parameters . . . . . . . . . . . . 232

7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233

7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235

7.9 Vapnik–Chervonenkis Dimension . . . . . . . . . . . . . . 237

7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239

7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241

7.10.1 K-Fold Cross-Validation . . . . . . . . . . . . . 241

7.10.2 The Wrong and Right Way

to Do Cross-validation . . . . . . . . . . . . . . . 245

7.10.3 Does Cross-Validation Really Work? . . . . . . . 247

7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249

7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252

7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257

8 Model Inference and Averaging

261

8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261

Contents xvii

8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261 8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261 8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265 8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267
8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267 8.4 Relationship Between the Bootstrap
and Bayesian Inference . . . . . . . . . . . . . . . . . . . 271 8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272
8.5.1 Two-Component Mixture Model . . . . . . . . . 272 8.5.2 The EM Algorithm in General . . . . . . . . . . 276 8.5.3 EM as a Maximization–Maximization Procedure 277 8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279 8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282 8.7.1 Example: Trees with Simulated Data . . . . . . 283 8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288 8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293

9 Additive Models, Trees, and Related Methods

295

9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295

9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297

9.1.2 Example: Additive Logistic Regression . . . . . 299

9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304

9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305

9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305

9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307

9.2.3 Classiﬁcation Trees . . . . . . . . . . . . . . . . 308

9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310

9.2.5 Spam Example (Continued) . . . . . . . . . . . 313

9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317

9.3.1 Spam Example (Continued) . . . . . . . . . . . 320

9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 321

9.4.1 Spam Example (Continued) . . . . . . . . . . . 326

9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327

9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328

9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329

9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332

9.7 Computational Considerations . . . . . . . . . . . . . . . 334

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335

10 Boosting and Additive Trees

337

10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337

10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340

xviii Contents

10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341 10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342 10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343 10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345 10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346 10.7 “Oﬀ-the-Shelf” Procedures for Data Mining . . . . . . . . 350 10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352 10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353 10.10 Numerical Optimization via Gradient Boosting . . . . . . 358
10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358 10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359 10.10.3 Implementations of Gradient Boosting . . . . . . 360 10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361 10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364 10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364 10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365 10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367 10.13.1 Relative Importance of Predictor Variables . . . 367 10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369 10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371 10.14.1 California Housing . . . . . . . . . . . . . . . . . 371 10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375 10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384

11 Neural Networks

389

11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389

11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389

11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392

11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395

11.5 Some Issues in Training Neural Networks . . . . . . . . . 397

11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397

11.5.2 Overﬁtting . . . . . . . . . . . . . . . . . . . . . 398

11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398

11.5.4 Number of Hidden Units and Layers . . . . . . . 400

11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400

11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401

11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404

11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408

11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409

11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410

11.9.2 Performance Comparisons . . . . . . . . . . . . 412

11.10 Computational Considerations . . . . . . . . . . . . . . . 414

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415

Contents xix

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415

12 Support Vector Machines and

Flexible Discriminants

417

12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417

12.2 The Support Vector Classiﬁer . . . . . . . . . . . . . . . . 417

12.2.1 Computing the Support Vector Classiﬁer . . . . 420

12.2.2 Mixture Example (Continued) . . . . . . . . . . 421

12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423

12.3.1 Computing the SVM for Classiﬁcation . . . . . . 423

12.3.2 The SVM as a Penalization Method . . . . . . . 426

12.3.3 Function Estimation and Reproducing Kernels . 428

12.3.4 SVMs and the Curse of Dimensionality . . . . . 431

12.3.5 A Path Algorithm for the SVM Classiﬁer . . . . 432

12.3.6 Support Vector Machines for Regression . . . . . 434

12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436

12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438

12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 438

12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440

12.5.1 Computing the FDA Estimates . . . . . . . . . . 444

12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446

12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449

12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455

13 Prototype Methods and Nearest-Neighbors

459

13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459

13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459

13.2.1 K-means Clustering . . . . . . . . . . . . . . . . 460

13.2.2 Learning Vector Quantization . . . . . . . . . . 462

13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463

13.3 k-Nearest-Neighbor Classiﬁers . . . . . . . . . . . . . . . 463

13.3.1 Example: A Comparative Study . . . . . . . . . 468

13.3.2 Example: k-Nearest-Neighbors

and Image Scene Classiﬁcation . . . . . . . . . . 470

13.3.3 Invariant Metrics and Tangent Distance . . . . . 471

13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475

13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478

13.4.2 Global Dimension Reduction

for Nearest-Neighbors . . . . . . . . . . . . . . . 479

13.5 Computational Considerations . . . . . . . . . . . . . . . 480

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481

xx Contents

14 Unsupervised Learning

485

14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485

14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487

14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488

14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489

14.2.3 Example: Market Basket Analysis . . . . . . . . 492

14.2.4 Unsupervised as Supervised Learning . . . . . . 495

14.2.5 Generalized Association Rules . . . . . . . . . . 497

14.2.6 Choice of Supervised Learning Method . . . . . 499

14.2.7 Example: Market Basket Analysis (Continued) . 499

14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501

14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503

14.3.2 Dissimilarities Based on Attributes . . . . . . . 503

14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505

14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507

14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507

14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . . 509

14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510

14.3.8 Example: Human Tumor Microarray Data . . . 512

14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514

14.3.10 K-medoids . . . . . . . . . . . . . . . . . . . . . 515

14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518

14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520

14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528

14.5 Principal Components, Curves and Surfaces . . . . . . . . 534

14.5.1 Principal Components . . . . . . . . . . . . . . . 534

14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541

14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544

14.5.4 Kernel Principal Components . . . . . . . . . . . 547

14.5.5 Sparse Principal Components . . . . . . . . . . . 550

14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553

14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554

14.7 Independent Component Analysis

and Exploratory Projection Pursuit . . . . . . . . . . . . 557

14.7.1 Latent Variables and Factor Analysis . . . . . . 558

14.7.2 Independent Component Analysis . . . . . . . . 560

14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565

14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565

14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570

14.9 Nonlinear Dimension Reduction

and Local Multidimensional Scaling . . . . . . . . . . . . 572

14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579

Contents xxi

15 Random Forests

587

15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587

15.2 Deﬁnition of Random Forests . . . . . . . . . . . . . . . . 587

15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592

15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592

15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593

15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595

15.3.4 Random Forests and Overﬁtting . . . . . . . . . 596

15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597

15.4.1 Variance and the De-Correlation Eﬀect . . . . . 597

15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600

15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603

16 Ensemble Learning

605

16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605

16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607

16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607

16.2.2 The “Bet on Sparsity” Principle . . . . . . . . . 610

16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613

16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616

16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617

16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622

Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624

17 Undirected Graphical Models

625

17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625

17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627

17.3 Undirected Graphical Models for Continuous Variables . 630

17.3.1 Estimation of the Parameters

when the Graph Structure is Known . . . . . . . 631

17.3.2 Estimation of the Graph Structure . . . . . . . . 635

17.4 Undirected Graphical Models for Discrete Variables . . . 638

17.4.1 Estimation of the Parameters

when the Graph Structure is Known . . . . . . . 639

17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641

17.4.3 Estimation of the Graph Structure . . . . . . . . 642

17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645

18 High-Dimensional Problems: p ≫ N

649

18.1 When p is Much Bigger than N . . . . . . . . . . . . . . 649

xxii Contents

18.2 Diagonal Linear Discriminant Analysis and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651
18.3 Linear Classiﬁers with Quadratic Regularization . . . . . 654 18.3.1 Regularized Discriminant Analysis . . . . . . . . 656 18.3.2 Logistic Regression with Quadratic Regularization . . . . . . . . . . 657 18.3.3 The Support Vector Classiﬁer . . . . . . . . . . 657 18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658 18.3.5 Computational Shortcuts When p ≫ N . . . . . 659
18.4 Linear Classiﬁers with L1 Regularization . . . . . . . . . 661 18.4.1 Application of Lasso to Protein Mass Spectroscopy . . . . . . . . . . 664 18.4.2 The Fused Lasso for Functional Data . . . . . . 666
18.5 Classiﬁcation When Features are Unavailable . . . . . . . 668 18.5.1 Example: String Kernels and Protein Classiﬁcation . . . . . . . . . . . . . 668 18.5.2 Classiﬁcation and Other Models Using Inner-Product Kernels and Pairwise Distances . 670 18.5.3 Example: Abstracts Classiﬁcation . . . . . . . . 672
18.6 High-Dimensional Regression: Supervised Principal Components . . . . . . . . . . . . . 674 18.6.1 Connection to Latent-Variable Modeling . . . . 678 18.6.2 Relationship with Partial Least Squares . . . . . 680 18.6.3 Pre-Conditioning for Feature Selection . . . . . 681
18.7 Feature Assessment and the Multiple-Testing Problem . . 683 18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687 18.7.2 Asymmetric Cutpoints and the SAM Procedure 690 18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692
18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694

References

699

Author Index

729

Index

737

1
Introduction

This is page 1 Printer: Opaque this

Statistical learning plays a key role in many areas of science, ﬁnance and industry. Here are some examples of learning problems:
• Predict whether a patient, hospitalized due to a heart attack, will have a second heart attack. The prediction is to be based on demographic, diet and clinical measurements for that patient.
• Predict the price of a stock in 6 months from now, on the basis of company performance measures and economic data.
• Identify the numbers in a handwritten ZIP code, from a digitized image.
• Estimate the amount of glucose in the blood of a diabetic person, from the infrared absorption spectrum of that person’s blood.
• Identify the risk factors for prostate cancer, based on clinical and demographic variables.
The science of learning plays a key role in the ﬁelds of statistics, data mining and artiﬁcial intelligence, intersecting with areas of engineering and other disciplines.
This book is about learning from data. In a typical scenario, we have an outcome measurement, usually quantitative (such as a stock price) or categorical (such as heart attack/no heart attack), that we wish to predict based on a set of features (such as diet and clinical measurements). We have a training set of data, in which we observe the outcome and feature

2 1. Introduction
TABLE 1.1. Average percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest diﬀerence between spam and email.
george you your hp free hpl ! our re edu remove spam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28 email 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01
measurements for a set of objects (such as people). Using this data we build a prediction model, or learner, which will enable us to predict the outcome for new unseen objects. A good learner is one that accurately predicts such an outcome.
The examples above describe what is called the supervised learning problem. It is called “supervised” because of the presence of the outcome variable to guide the learning process. In the unsupervised learning problem, we observe only the features and have no measurements of the outcome. Our task is rather to describe how the data are organized or clustered. We devote most of this book to supervised learning; the unsupervised problem is less developed in the literature, and is the focus of Chapter 14.
Here are some examples of real learning problems that are discussed in this book.
Example 1: Email Spam
The data for this example consists of information from 4601 email messages, in a study to try to predict whether the email was junk email, or “spam.” The objective was to design an automatic spam detector that could ﬁlter out spam before clogging the users’ mailboxes. For all 4601 email messages, the true outcome (email type) email or spam is available, along with the relative frequencies of 57 of the most commonly occurring words and punctuation marks in the email message. This is a supervised learning problem, with the outcome the class variable email/spam. It is also called a classiﬁcation problem.
Table 1.1 lists the words and characters showing the largest average diﬀerence between spam and email.
Our learning method has to decide which features to use and how: for example, we might use a rule such as
if (%george < 0.6) & (%you > 1.5) then spam else email.
Another form of a rule might be:
if (0.2 · %you − 0.3 · %george) > 0 then spam else email.

1. Introduction 3

012345

−1 1 2 3 4

2.5 3.5 4.5

40 50 60 70 80

−1 0 1 2

−1 1 2 3 4

40 50 60 70 80

0.0 0.4 0.8

6.0 7.0 8.0 9.0

lpsa

o oooooo

ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo o

o

oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o o

o
o o

o

o

oo oo

ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

ooooo

oooooooooooooooooooo

oo

o oo

o

ooooo ooo
o

oooooooooooooooooooooooooooooooooo

ooooooooooooooooo

oo oo

oo oo

oo

o

oo

oo

ooooooooooooo

oooooooooooooo ooooooooooooooooooooooo ooooooooooooooooooooooo

oo ooooooooooo

oooooooooooooooooo

o

o oooo

oo o oooooooooooooooooooooooooooooooooooooooo

oo o

oooo o o

o

o

o

oo o

oo

ooo

oo

oo o

oo o

oooo

o oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o oo

lcavol

o

oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o o

o o o

oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o oooooooooooooooo ooooo

o o

o

o
o ooooooo

oooooooooooooooooooooooooooooooooo

o

oo

o

ooooooooooooooooooooo

oooooooo ooooo

o o oooooooooo oooooo

ooooooooooooooooooooooo ooooooooooooooooooooo

o ooooooooooo oooooo

ooooooooooooooooo o

o

o o
o

oooooooooooooooooooooooooooooooooooooooooooooooooo

oo o

o oo oo

oo o o
o oo

oo o

oo

o oo

ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o o o

o

oo

o
o oooooooo
oo

oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o

lweight

oo

o

oo

o o o

ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o oooooooooooooo

o o o

o

o o ooooooo

ooooooooooooooooooooooooooooooooo oo

ooooooooooooooooooo

o

o

o

o

o

o

oo

o

o

ooooooooo

oooooooooooooo oo

o ooooooooooooooooooooo oooooooooooooooooooooo

oooo oooooooooo o

oooooooooooooooooo

o

oo
o oo

oooooooooooooooooooooooooooooooooooooooooooooo

oo o

o
o ooo

ooooo o

o
o o

o

o

o

o
o o

oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

oo oo oo

o

oo o

o ooo o oo o
ooo o

ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o

oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o o

age

oo oooooooooooooooooooo

o o o

ooooooooooo oooooooooooooooooooooooooooooooooooo

o

oo oooooooooooooooooooo ooo oo

ooo ooooooooo o o

oo oooooooooooooooo o ooo oo

oo ooooooooooooooooooooooo

o oo oooooooooooooo o oo o

o

o

o

oooooooooooo o ooo oo

ooooooooooooooooooo o oo

o

o
o
o o

o oo

ooooooooooooooooooooooooooooooooooooooo oo

ooooooooo o

o o

o ooooo oo

o oooo oo
o

o o
o

oooooooooooooooooooooooooooooooooooo

o

o

o o

oooooooooooooooooooooooooooooooo

o

ooo

ooooo o

o

ooo oooo

oo o

oo o

o

oo

o o

oooooooooooooooooooooooooooooooo

o

oooooo o o

o

o

oooooooooooooooooooooooooooooo

o oo
o

o oooooooooo

lbph

ooooooooooo ooo

o oo

o

oo

oo

oo oooooooooooooooooooooooooooo oooooo oooooooooooooooooooooo o o ooooooooooooooooooooo o o ooo o ooo ooooooooooooo oo

o

oooo o

ooo ooooo

oo oo ooo

o

o o

oo

oooooooo

o

ooo oooo

ooooooooo o

o ooo o

o o

o

oo

o o

o

oooo

o o

oooooooooooooooooooooo o

oo oo o oo

o

o

o o

oo

oo o

oo

oo

oo

ooo

ooo

o o ooo ooooo oooooooo o o

o oooooo o o o o o o o

oooooooooooo ooo

ooooo oooooooooo

oooooooooooo o o o ooooooooo ooo o o o o o ooooo

o

oooo oooooooooooo

o

o ooo ooo o o ooo o o

svi

oo oooooooooooooooooooooooooooo

ooooooo ooooooooooooooooooooooooooooo o oooooooooooooooooooooooooooooooooo o oo ooo oooooooooooooooooooo oo o oo ooooo oooooooooooooooooo

oooooooooooooooooooooo

o o o

oooooooooooooooooooooooooo

oooooooooooooooooooooo o oooooooooooooooooooooo

oooooooooooooooooooooo

o

ooooooooooooooooooooooo o o

o

o o oo ooooooooooooooo oo o

ooooooo

o oo
o o

o oooooooo

o oooo

o

oo

oo ooooooo

o

ooooooooooo

oo

oooo ooo

o

o

o

o o

oo

o

ooo

o o

o o

ooooooooo

oo ooooooooooooooooooooooooo

ooooooo oooooooooooooooo o oo o oo oooooooooooooooooooooooo oo ooo o oooooooooooooooo oo o o ooo ooooooooooooo o

oooo o

o o oo

oo o oo

oo o o o

oo o o

o

o

o

o

o

o

o ooooooooooooooooooooooooooo

oo ooooooooooooooooooooooooo o ooooooooooooooooooooooooooo oo o o o ooooooooooooooooooo o o ooooo o ooooooooooooo o

oo ooooooooooooooooo oo

oooooo ooooooo ooooooooo o

o ooooooooooooooo oooo oo ooo o oo oooooooooo

o o o o oooooooooo o

ooo

o oo

oo o

o oo o

o

oo

oo o oo

ooo

o

oo o

oo

o o

o ooooooo

o oo oo oo

o

o o

o

oo

o oo

o o

o ooooooo

ooo oo o

ooo oo o o

o o ooooo o

o

o

oo o

o oo

o oo

o oo

oo

oo

o

o oo

oooooooooooooooooooooooooooooooooooooooooooooooooo

o

o o

o oo oooooo

oooooooooooooooooooooooooooooooooooooooooooooooooo

o ooooooooooooooooooooooooooooooooooooooooooooooooooooo o

o oo

oo ooo

oooo

ooooooooooooooooooooooooooooooooooooooo

oo o

o

oo

ooooo

o o

o
oo oo

o

ooooooooooooooooooooooooooo

o ooooooo

012345

2.5 3.5 4.5

−1 0 1 2

o ooooooooo oooooo o o o o o oooooooo o o o o o o

oooooooo ooo

lcp

o

oooooooo

o

o oo o

o o o ooo

ooo o

o

ooo oo

o o o

o

o o o

o o o

ooooooooo

ooo oooo oo

o o

ooo o

o o

o

o

o oo o

o

oo

o oooooo o o o

oo

o oo o

o o ooo

o o o ooooooooooo oooooooooooo

gleason

o ooooooooo o o ooo

o oo o o

o

o o

oo o

ooo

ooo

o o

o o

o

o oo

o

ooo

o o

o

o oo ooo

o

oo

o

o

o

ooo ooo

o oooooo

oo o ooo oo oo oooo ooo

o ooo

ooo

o o

o

o

oooooooo

o

pgg45

−1 0 1 2 3

0 20 60 100

0.0 0.4 0.8

−1 0 1 2 3

6.0 7.0 8.0 9.0

0 20 60 100

FIGURE 1.1. Scatterplot matrix of the prostate cancer data. The ﬁrst row shows the response against each of the predictors in turn. Two of the predictors, svi and gleason, are categorical.

For this problem not all errors are equal; we want to avoid ﬁltering out good email, while letting spam get through is not desirable but less serious in its consequences. We discuss a number of diﬀerent methods for tackling this learning problem in the book.

Example 2: Prostate Cancer
The data for this example, displayed in Figure 1.11, come from a study by Stamey et al. (1989) that examined the correlation between the level of
1There was an error in these data in the ﬁrst edition of this book. Subject 32 had a value of 6.1 for lweight, which translates to a 449 gm prostate! The correct value is 44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.

4 1. Introduction
FIGURE 1.2. Examples of handwritten digits from U.S. postal envelopes.
prostate speciﬁc antigen (PSA) and a number of clinical measures, in 97 men who were about to receive a radical prostatectomy.
The goal is to predict the log of PSA (lpsa) from a number of measurements including log cancer volume (lcavol), log prostate weight lweight, age, log of benign prostatic hyperplasia amount lbph, seminal vesicle invasion svi, log of capsular penetration lcp, Gleason score gleason, and percent of Gleason scores 4 or 5 pgg45. Figure 1.1 is a scatterplot matrix of the variables. Some correlations with lpsa are evident, but a good predictive model is diﬃcult to construct by eye.
This is a supervised learning problem, known as a regression problem, because the outcome measurement is quantitative.
Example 3: Handwritten Digit Recognition
The data from this example come from the handwritten ZIP codes on envelopes from U.S. postal mail. Each image is a segment from a ﬁve digit ZIP code, isolating a single digit. The images are 16×16 eight-bit grayscale maps, with each pixel ranging in intensity from 0 to 255. Some sample images are shown in Figure 1.2.
The images have been normalized to have approximately the same size and orientation. The task is to predict, from the 16 × 16 matrix of pixel intensities, the identity of each image (0, 1, . . . , 9) quickly and accurately. If it is accurate enough, the resulting algorithm would be used as part of an automatic sorting procedure for envelopes. This is a classiﬁcation problem for which the error rate needs to be kept very low to avoid misdirection of

1. Introduction 5
mail. In order to achieve this low error rate, some objects can be assigned to a “don’t know” category, and sorted instead by hand.
Example 4: DNA Expression Microarrays
DNA stands for deoxyribonucleic acid, and is the basic material that makes up human chromosomes. DNA microarrays measure the expression of a gene in a cell by measuring the amount of mRNA (messenger ribonucleic acid) present for that gene. Microarrays are considered a breakthrough technology in biology, facilitating the quantitative study of thousands of genes simultaneously from a single sample of cells.
Here is how a DNA microarray works. The nucleotide sequences for a few thousand genes are printed on a glass slide. A target sample and a reference sample are labeled with red and green dyes, and each are hybridized with the DNA on the slide. Through ﬂuoroscopy, the log (red/green) intensities of RNA hybridizing at each site is measured. The result is a few thousand numbers, typically ranging from say −6 to 6, measuring the expression level of each gene in the target relative to the reference sample. Positive values indicate higher expression in the target versus the reference, and vice versa for negative values.
A gene expression dataset collects together the expression values from a series of DNA microarray experiments, with each column representing an experiment. There are therefore several thousand rows representing individual genes, and tens of columns representing samples: in the particular example of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns), although for clarity only a random sample of 100 rows are shown. The ﬁgure displays the data set as a heat map, ranging from green (negative) to red (positive). The samples are 64 cancer tumors from diﬀerent patients.
The challenge here is to understand how the genes and samples are organized. Typical questions include the following:
(a) which samples are most similar to each other, in terms of their expression proﬁles across genes?
(b) which genes are most similar to each other, in terms of their expression proﬁles across samples?
(c) do certain genes show very high (or low) expression for certain cancer samples?
We could view this task as a regression problem, with two categorical predictor variables—genes and samples—with the response variable being the level of expression. However, it is probably more useful to view it as unsupervised learning problem. For example, for question (a) above, we think of the samples as points in 6830–dimensional space, which we want to cluster together in some way.

6 1. Introduction

SIDW299104 SIDW380102 SID73161 GNAL H.sapiensmRNA SID325394 RASGTPASE SID207172 ESTs SIDW377402 HumanmRNA SIDW469884 ESTs SID471915 MYBPROTO ESTsChr.1 SID377451 DNAPOLYMER SID375812 SIDW31489 SID167117 SIDW470459 SIDW487261 Homosapiens SIDW376586 Chr MITOCHONDRIAL60 SID47116 ESTsChr.6 SIDW296310 SID488017 SID305167 ESTsChr.3 SID127504 SID289414 PTPRC SIDW298203 SIDW310141 SIDW376928 ESTsCh31 SID114241 SID377419 SID297117 SIDW201620 SIDW279664 SIDW510534 HLACLASSI SIDW203464 SID239012 SIDW205716 SIDW376776 HYPOTHETICAL WASWiskott SIDW321854 ESTsChr.15 SIDW376394 SID280066 ESTsChr.5 SIDW488221 SID46536 SIDW257915 ESTsChr.2 SIDW322806 SID200394 ESTsChr.15 SID284853 SID485148 SID297905 ESTs SIDW486740 SMALLNUC ESTs SIDW366311 SIDW357197 SID52979 ESTs SID43609 SIDW416621 ERLUMEN TUPLE1TUP1 SIDW428642 SID381079 SIDW298052 SIDW417270 SIDW362471 ESTsChr.15 SIDW321925 SID380265 SIDW308182 SID381508 SID377133 SIDW365099 ESTsChr.10 SIDW325120 SID360097 SID375990 SIDW128368 SID301902 SID31984 SID42354

BREAST RENAL
MELANOMA MELANOMA MCF7D-repro
COLON COLON K562B-repro COLON NSCLC LEUKEMIA RENAL MELANOMA BREAST
CNS CNS RENAL MCF7A-repro NSCLC K562A-repro COLON CNS NSCLC NSCLC LEUKEMIA CNS OVARIAN BREAST LEUKEMIA MELANOMA MELANOMA OVARIAN OVARIAN NSCLC RENAL BREAST MELANOMA OVARIAN OVARIAN NSCLC RENAL BREAST MELANOMA LEUKEMIA COLON BREAST LEUKEMIA COLON CNS MELANOMA NSCLC PROSTATE NSCLC RENAL RENAL NSCLC RENAL LEUKEMIA OVARIAN PROSTATE COLON BREAST RENAL UNKNOWN

FIGURE 1.3. DNA microarray data: expression matrix of 6830 genes (rows) and 64 samples (columns), for the human tumor data. Only a random sample of 100 rows are shown. The display is a heat map, ranging from bright green (negative, under expressed) to bright red (positive, over expressed). Missing values are gray. The rows and columns are displayed in a randomly chosen order.

1. Introduction 7
Who Should Read this Book
This book is designed for researchers and students in a broad variety of ﬁelds: statistics, artiﬁcial intelligence, engineering, ﬁnance and others. We expect that the reader will have had at least one elementary course in statistics, covering basic topics including linear regression.
We have not attempted to write a comprehensive catalog of learning methods, but rather to describe some of the most important techniques. Equally notable, we describe the underlying concepts and considerations by which a researcher can judge a learning method. We have tried to write this book in an intuitive fashion, emphasizing concepts rather than mathematical details.
As statisticians, our exposition will naturally reﬂect our backgrounds and areas of expertise. However in the past eight years we have been attending conferences in neural networks, data mining and machine learning, and our thinking has been heavily inﬂuenced by these exciting ﬁelds. This inﬂuence is evident in our current research, and in this book.
How This Book is Organized
Our view is that one must understand simple methods before trying to grasp more complex ones. Hence, after giving an overview of the supervising learning problem in Chapter 2, we discuss linear methods for regression and classiﬁcation in Chapters 3 and 4. In Chapter 5 we describe splines, wavelets and regularization/penalization methods for a single predictor, while Chapter 6 covers kernel methods and local regression. Both of these sets of methods are important building blocks for high-dimensional learning techniques. Model assessment and selection is the topic of Chapter 7, covering the concepts of bias and variance, overﬁtting and methods such as cross-validation for choosing models. Chapter 8 discusses model inference and averaging, including an overview of maximum likelihood, Bayesian inference and the bootstrap, the EM algorithm, Gibbs sampling and bagging, A related procedure called boosting is the focus of Chapter 10.
In Chapters 9–13 we describe a series of structured methods for supervised learning, with Chapters 9 and 11 covering regression and Chapters 12 and 13 focusing on classiﬁcation. Chapter 14 describes methods for unsupervised learning. Two recently proposed techniques, random forests and ensemble learning, are discussed in Chapters 15 and 16. We describe undirected graphical models in Chapter 17 and ﬁnally we study highdimensional problems in Chapter 18.
At the end of each chapter we discuss computational considerations important for data mining applications, including how the computations scale with the number of observations and predictors. Each chapter ends with Bibliographic Notes giving background references for the material.

8 1. Introduction
We recommend that Chapters 1–4 be ﬁrst read in sequence. Chapter 7 should also be considered mandatory, as it covers central concepts that pertain to all learning methods. With this in mind, the rest of the book can be read sequentially, or sampled, depending on the reader’s interest.

The symbol

indicates a technically diﬃcult section, one that can

be skipped without interrupting the ﬂow of the discussion.

Book Website
The website for this book is located at
http://www-stat.stanford.edu/ElemStatLearn
It contains a number of resources, including many of the datasets used in this book.

Note for Instructors
We have successively used the ﬁrst edition of this book as the basis for a two-quarter course, and with the additional materials in this second edition, it could even be used for a three-quarter sequence. Exercises are provided at the end of each chapter. It is important for students to have access to good software tools for these topics. We used the R and S-PLUS programming languages in our courses.

2
Overview of Supervised Learning

This is page 9 Printer: Opaque this

2.1 Introduction
The ﬁrst three examples described in Chapter 1 have several components in common. For each there is a set of variables that might be denoted as inputs, which are measured or preset. These have some inﬂuence on one or more outputs. For each example the goal is to use the inputs to predict the values of the outputs. This exercise is called supervised learning.
We have used the more modern language of machine learning. In the statistical literature the inputs are often called the predictors, a term we will use interchangeably with inputs, and more classically the independent variables. In the pattern recognition literature the term features is preferred, which we use as well. The outputs are called the responses, or classically the dependent variables.
2.2 Variable Types and Terminology
The outputs vary in nature among the examples. In the glucose prediction example, the output is a quantitative measurement, where some measurements are bigger than others, and measurements close in value are close in nature. In the famous Iris discrimination example due to R. A. Fisher, the output is qualitative (species of Iris) and assumes values in a ﬁnite set G = {Virginica, Setosa and Versicolor}. In the handwritten digit example the output is one of 10 diﬀerent digit classes: G = {0, 1, . . . , 9}. In both of

10 2. Overview of Supervised Learning
these there is no explicit ordering in the classes, and in fact often descriptive labels rather than numbers are used to denote the classes. Qualitative variables are also referred to as categorical or discrete variables as well as factors.
For both types of outputs it makes sense to think of using the inputs to predict the output. Given some speciﬁc atmospheric measurements today and yesterday, we want to predict the ozone level tomorrow. Given the grayscale values for the pixels of the digitized image of the handwritten digit, we want to predict its class label.
This distinction in output type has led to a naming convention for the prediction tasks: regression when we predict quantitative outputs, and classiﬁcation when we predict qualitative outputs. We will see that these two tasks have a lot in common, and in particular both can be viewed as a task in function approximation.
Inputs also vary in measurement type; we can have some of each of qualitative and quantitative input variables. These have also led to distinctions in the types of methods that are used for prediction: some methods are deﬁned most naturally for quantitative inputs, some most naturally for qualitative and some for both.
A third variable type is ordered categorical, such as small, medium and large, where there is an ordering between the values, but no metric notion is appropriate (the diﬀerence between medium and small need not be the same as that between large and medium). These are discussed further in Chapter 4.
Qualitative variables are typically represented numerically by codes. The easiest case is when there are only two classes or categories, such as “success” or “failure,” “survived” or “died.” These are often represented by a single binary digit or bit as 0 or 1, or else by −1 and 1. For reasons that will become apparent, such numeric codes are sometimes referred to as targets. When there are more than two categories, several alternatives are available. The most useful and commonly used coding is via dummy variables. Here a K-level qualitative variable is represented by a vector of K binary variables or bits, only one of which is “on” at a time. Although more compact coding schemes are possible, dummy variables are symmetric in the levels of the factor.
We will typically denote an input variable by the symbol X. If X is a vector, its components can be accessed by subscripts Xj. Quantitative outputs will be denoted by Y , and qualitative outputs by G (for group). We use uppercase letters such as X, Y or G when referring to the generic aspects of a variable. Observed values are written in lowercase; hence the ith observed value of X is written as xi (where xi is again a scalar or vector). Matrices are represented by bold uppercase letters; for example, a set of N input p-vectors xi, i = 1, . . . , N would be represented by the N ×p matrix X. In general, vectors will not be bold, except when they have N components; this convention distinguishes a p-vector of inputs xi for the

2.3 Least Squares and Nearest Neighbors 11
ith observation from the N -vector xj consisting of all the observations on variable Xj. Since all vectors are assumed to be column vectors, the ith row of X is xTi , the vector transpose of xi.
For the moment we can loosely state the learning task as follows: given the value of an input vector X, make a good prediction of the output Y, denoted by Yˆ (pronounced “y-hat”). If Y takes values in IR then so should Yˆ ; likewise for categorical outputs, Gˆ should take values in the same set G associated with G.
For a two-class G, one approach is to denote the binary coded target as Y , and then treat it as a quantitative output. The predictions Yˆ will typically lie in [0, 1], and we can assign to Gˆ the class label according to whether yˆ > 0.5. This approach generalizes to K-level qualitative outputs as well.
We need data to construct prediction rules, often a lot of it. We thus suppose we have available a set of measurements (xi, yi) or (xi, gi), i = 1, . . . , N , known as the training data, with which to construct our prediction rule.

2.3 Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors
In this section we develop two simple but powerful prediction methods: the linear model ﬁt by least squares and the k-nearest-neighbor prediction rule. The linear model makes huge assumptions about structure and yields stable but possibly inaccurate predictions. The method of k-nearest neighbors makes very mild structural assumptions: its predictions are often accurate but can be unstable.

2.3.1 Linear Models and Least Squares
The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs XT = (X1, X2, . . . , Xp), we predict the output Y via the model

p
Yˆ = βˆ0 + Xjβˆj.
j=1

(2.1)

The term βˆ0 is the intercept, also known as the bias in machine learning. Often it is convenient to include the constant variable 1 in X, include βˆ0 in the vector of coeﬃcients βˆ, and then write the linear model in vector form

as an inner product

Yˆ = XT βˆ,

(2.2)

12 2. Overview of Supervised Learning

where XT denotes vector or matrix transpose (X being a column vector). Here we are modeling a single output, so Yˆ is a scalar; in general Yˆ can be a K–vector, in which case β would be a p × K matrix of coeﬃcients. In the (p + 1)-dimensional input–output space, (X, Yˆ ) represents a hyperplane. If the constant is included in X, then the hyperplane includes the origin and is a subspace; if not, it is an aﬃne set cutting the Y -axis at the point (0, βˆ0). From now on we assume that the intercept is included in βˆ.
Viewed as a function over the p-dimensional input space, f (X) = XT β is linear, and the gradient f ′(X) = β is a vector in input space that points in the steepest uphill direction.
How do we ﬁt the linear model to a set of training data? There are many diﬀerent methods, but by far the most popular is the method of least squares. In this approach, we pick the coeﬃcients β to minimize the residual sum of squares

N
RSS(β) = (yi − xTi β)2.
i=1

(2.3)

RSS(β) is a quadratic function of the parameters, and hence its minimum always exists, but may not be unique. The solution is easiest to characterize in matrix notation. We can write

RSS(β) = (y − Xβ)T (y − Xβ),

(2.4)

where X is an N × p matrix with each row an input vector, and y is an

N -vector of the outputs in the training set. Diﬀerentiating w.r.t. β we get

the normal equations

XT (y − Xβ) = 0.

(2.5)

If XT X is nonsingular, then the unique solution is given by

βˆ = (XT X)−1XT y,

(2.6)

and the ﬁtted value at the ith input xi is yˆi = yˆ(xi) = xTi βˆ. At an arbitrary input x0 the prediction is yˆ(x0) = xT0 βˆ. The entire ﬁtted surface is characterized by the p parameters βˆ. Intuitively, it seems that we do not need a very large data set to ﬁt such a model.
Let’s look at an example of the linear model in a classiﬁcation context. Figure 2.1 shows a scatterplot of training data on a pair of inputs X1 and X2. The data are simulated, and for the present the simulation model is not important. The output class variable G has the values BLUE or ORANGE, and is represented as such in the scatterplot. There are 100 points in each of the two classes. The linear regression model was ﬁt to these data, with the response Y coded as 0 for BLUE and 1 for ORANGE. The ﬁtted values Yˆ are converted to a ﬁtted class variable Gˆ according to the rule

Gˆ =

ORANGE BLUE

if Yˆ > 0.5, if Yˆ ≤ 0.5.

(2.7)

2.3 Least Squares and Nearest Neighbors 13
Linear Regression of 0/1 Response
ooooooooo oo oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo o ooooo o ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ...................................................................................................
FIGURE 2.1. A classiﬁcation example in two dimensions. The classes are coded as a binary variable (BLUE = 0, ORANGE = 1), and then ﬁt by linear regression. The line is the decision boundary deﬁned by xT βˆ = 0.5. The orange shaded region denotes that part of input space classiﬁed as ORANGE, while the blue region is classiﬁed as BLUE.
The set of points in IR2 classiﬁed as ORANGE corresponds to {x : xT βˆ > 0.5}, indicated in Figure 2.1, and the two predicted classes are separated by the decision boundary {x : xT βˆ = 0.5}, which is linear in this case. We see that for these data there are several misclassiﬁcations on both sides of the decision boundary. Perhaps our linear model is too rigid— or are such errors unavoidable? Remember that these are errors on the training data itself, and we have not said where the constructed data came from. Consider the two possible scenarios:
Scenario 1: The training data in each class were generated from bivariate Gaussian distributions with uncorrelated components and diﬀerent means.
Scenario 2: The training data in each class came from a mixture of 10 lowvariance Gaussian distributions, with individual means themselves distributed as Gaussian.
A mixture of Gaussians is best described in terms of the generative model. One ﬁrst generates a discrete variable that determines which of

14 2. Overview of Supervised Learning
the component Gaussians to use, and then generates an observation from the chosen density. In the case of one Gaussian per class, we will see in Chapter 4 that a linear decision boundary is the best one can do, and that our estimate is almost optimal. The region of overlap is inevitable, and future data to be predicted will be plagued by this overlap as well.
In the case of mixtures of tightly clustered Gaussians the story is different. A linear decision boundary is unlikely to be optimal, and in fact is not. The optimal decision boundary is nonlinear and disjoint, and as such will be much more diﬃcult to obtain.
We now look at another classiﬁcation and regression procedure that is in some sense at the opposite end of the spectrum to the linear model, and far better suited to the second scenario.

2.3.2 Nearest-Neighbor Methods

Nearest-neighbor methods use those observations in the training set T closest in input space to x to form Yˆ . Speciﬁcally, the k-nearest neighbor ﬁt for Yˆ is deﬁned as follows:

Yˆ (x)

=

1 k

yi,

xi ∈Nk (x)

(2.8)

where Nk(x) is the neighborhood of x deﬁned by the k closest points xi in the training sample. Closeness implies a metric, which for the moment we assume is Euclidean distance. So, in words, we ﬁnd the k observations with xi closest to x in input space, and average their responses.
In Figure 2.2 we use the same training data as in Figure 2.1, and use 15-nearest-neighbor averaging of the binary coded response as the method of ﬁtting. Thus Yˆ is the proportion of ORANGE’s in the neighborhood, and so assigning class ORANGE to Gˆ if Yˆ > 0.5 amounts to a majority vote in the neighborhood. The colored regions indicate all those points in input space classiﬁed as BLUE or ORANGE by such a rule, in this case found by evaluating the procedure on a ﬁne grid in input space. We see that the decision boundaries that separate the BLUE from the ORANGE regions are far more irregular, and respond to local clusters where one class dominates.
Figure 2.3 shows the results for 1-nearest-neighbor classiﬁcation: Yˆ is assigned the value yℓ of the closest point xℓ to x in the training data. In this case the regions of classiﬁcation can be computed relatively easily, and correspond to a Voronoi tessellation of the training data. Each point xi has an associated tile bounding the region for which it is the closest input point. For all points x in the tile, Gˆ(x) = gi. The decision boundary is even more irregular than before.
The method of k-nearest-neighbor averaging is deﬁned in exactly the same way for regression of a quantitative output Y , although k = 1 would be an unlikely choice.

2.3 Least Squares and Nearest Neighbors 15
15-Nearest Neighbor Classifier
ooooooooo oo oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo o ooooo o ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ...................................................................................................
FIGURE 2.2. The same classiﬁcation example in two dimensions as in Figure 2.1. The classes are coded as a binary variable (BLUE = 0, ORANGE = 1) and then ﬁt by 15-nearest-neighbor averaging as in (2.8). The predicted class is hence chosen by majority vote amongst the 15-nearest neighbors.
In Figure 2.2 we see that far fewer training observations are misclassiﬁed than in Figure 2.1. This should not give us too much comfort, though, since in Figure 2.3 none of the training data are misclassiﬁed. A little thought suggests that for k-nearest-neighbor ﬁts, the error on the training data should be approximately an increasing function of k, and will always be 0 for k = 1. An independent test set would give us a more satisfactory means for comparing the diﬀerent methods.
It appears that k-nearest-neighbor ﬁts have a single parameter, the number of neighbors k, compared to the p parameters in least-squares ﬁts. Although this is the case, we will see that the eﬀective number of parameters of k-nearest neighbors is N/k and is generally bigger than p, and decreases with increasing k. To get an idea of why, note that if the neighborhoods were nonoverlapping, there would be N/k neighborhoods and we would ﬁt one parameter (a mean) in each neighborhood.
It is also clear that we cannot use sum-of-squared errors on the training set as a criterion for picking k, since we would always pick k = 1! It would seem that k-nearest-neighbor methods would be more appropriate for the mixture Scenario 2 described above, while for Gaussian data the decision boundaries of k-nearest neighbors would be unnecessarily noisy.

16 2. Overview of Supervised Learning

1−Nearest Neighbor Classifier

o
o oo oo o
o o

oo oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o o oo o

o

o

o

oooo oo

oo ooo

oo o

o ooo

o

o

o

FIGURE 2.3. The same classiﬁcation example in two dimensions as in Figure 2.1. The classes are coded as a binary variable (BLUE = 0, ORANGE = 1), and then predicted by 1-nearest-neighbor classiﬁcation.
2.3.3 From Least Squares to Nearest Neighbors
The linear decision boundary from least squares is very smooth, and apparently stable to ﬁt. It does appear to rely heavily on the assumption that a linear decision boundary is appropriate. In language we will develop shortly, it has low variance and potentially high bias.
On the other hand, the k-nearest-neighbor procedures do not appear to rely on any stringent assumptions about the underlying data, and can adapt to any situation. However, any particular subregion of the decision boundary depends on a handful of input points and their particular positions, and is thus wiggly and unstable—high variance and low bias.
Each method has its own situations for which it works best; in particular linear regression is more appropriate for Scenario 1 above, while nearest neighbors are more suitable for Scenario 2. The time has come to expose the oracle! The data in fact were simulated from a model somewhere between the two, but closer to Scenario 2. First we generated 10 means mk from a bivariate Gaussian distribution N ((1, 0)T , I) and labeled this class BLUE. Similarly, 10 more were drawn from N ((0, 1)T , I) and labeled class ORANGE. Then for each class we generated 100 observations as follows: for each observation, we picked an mk at random with probability 1/10, and

2.3 Least Squares and Nearest Neighbors 17

k − Number of Nearest Neighbors

151 101 69 45 31 21

11 7 5

3

1

Linear

0.30

0.25

0.20

Test Error

0.15

0.10

Train Test Bayes

23

5 8 12 18 29

67

200

Degrees of Freedom − N/k

FIGURE 2.4. Misclassiﬁcation curves for the simulation example used in Figures 2.1, 2.2 and 2.3. A single training sample of size 200 was used, and a test sample of size 10, 000. The orange curves are test and the blue are training error for k-nearest-neighbor classiﬁcation. The results for linear regression are the bigger orange and blue squares at three degrees of freedom. The purple line is the optimal Bayes error rate.

then generated a N (mk, I/5), thus leading to a mixture of Gaussian clusters for each class. Figure 2.4 shows the results of classifying 10,000 new observations generated from the model. We compare the results for least squares and those for k-nearest neighbors for a range of values of k.
A large subset of the most popular techniques in use today are variants of these two simple procedures. In fact 1-nearest-neighbor, the simplest of all, captures a large percentage of the market for low-dimensional problems. The following list describes some ways in which these simple procedures have been enhanced:
• Kernel methods use weights that decrease smoothly to zero with distance from the target point, rather than the eﬀective 0/1 weights used by k-nearest neighbors.
• In high-dimensional spaces the distance kernels are modiﬁed to emphasize some variable more than others.

18 2. Overview of Supervised Learning
• Local regression ﬁts linear models by locally weighted least squares, rather than ﬁtting constants locally.
• Linear models ﬁt to a basis expansion of the original inputs allow arbitrarily complex models.
• Projection pursuit and neural network models consist of sums of nonlinearly transformed linear models.

2.4 Statistical Decision Theory

In this section we develop a small amount of theory that provides a framework for developing models such as those discussed informally so far. We ﬁrst consider the case of a quantitative output, and place ourselves in the world of random variables and probability spaces. Let X ∈ IRp denote a real valued random input vector, and Y ∈ IR a real valued random output variable, with joint distribution Pr(X, Y ). We seek a function f (X) for predicting Y given values of the input X. This theory requires a loss function L(Y, f (X)) for penalizing errors in prediction, and by far the most common and convenient is squared error loss: L(Y, f (X)) = (Y − f (X))2. This leads us to a criterion for choosing f ,

EPE(f ) = E(Y − f (X))2 = [y − f (x)]2 Pr(dx, dy),

(2.9) (2.10)

the expected (squared) prediction error . By conditioning1 on X, we can

write EPE as

EPE(f ) = EX EY |X [Y − f (X)]2|X

(2.11)

and we see that it suﬃces to minimize EPE pointwise:

f (x) = argmincEY |X [Y − c]2|X = x .

(2.12)

The solution is

f (x) = E(Y |X = x),

(2.13)

the conditional expectation, also known as the regression function. Thus the best prediction of Y at any point X = x is the conditional mean, when best is measured by average squared error.
The nearest-neighbor methods attempt to directly implement this recipe using the training data. At each point x, we might ask for the average of all

1Conditioning here amounts to factoring the joint density Pr(X, Y ) = Pr(Y |X)Pr(X) where Pr(Y |X) = Pr(Y, X)/Pr(X), and splitting up the bivariate integral accordingly.

2.4 Statistical Decision Theory 19

those yis with input xi = x. Since there is typically at most one observation at any point x, we settle for

fˆ(x) = Ave(yi|xi ∈ Nk(x)),

(2.14)

where “Ave” denotes average, and Nk(x) is the neighborhood containing the k points in T closest to x. Two approximations are happening here:

• expectation is approximated by averaging over sample data;

• conditioning at a point is relaxed to conditioning on some region “close” to the target point.

For large training sample size N , the points in the neighborhood are likely

to be close to x, and as k gets large the average will get more stable.

In fact, under mild regularity conditions on the joint probability distri-

bution Pr(X, Y ), one can show that as N, k → ∞ such that k/N → 0,

fˆ(x) → E(Y |X = x). In light of this, why look further, since it seems

we have a universal approximator? We often do not have very large sam-

ples. If the linear or some more structured model is appropriate, then we

can usually get a more stable estimate than k-nearest neighbors, although

such knowledge has to be learned from the data as well. There are other

problems though, sometimes disastrous. In Section 2.5 we see that as the

dimension p gets large, so does the metric size of the k-nearest neighbor-

hood. So settling for nearest neighborhood as a surrogate for conditioning

will fail us miserably. The convergence above still holds, but the rate of

convergence decreases as the dimension increases.

How does linear regression ﬁt into this framework? The simplest explana-

tion is that one assumes that the regression function f (x) is approximately

linear in its arguments:

f (x) ≈ xT β.

(2.15)

This is a model-based approach—we specify a model for the regression function. Plugging this linear model for f (x) into EPE (2.9) and diﬀerentiating we can solve for β theoretically:

β = [E(XXT )]−1E(XY ).

(2.16)

Note we have not conditioned on X; rather we have used our knowledge of the functional relationship to pool over values of X. The least squares solution (2.6) amounts to replacing the expectation in (2.16) by averages over the training data.
So both k-nearest neighbors and least squares end up approximating conditional expectations by averages. But they diﬀer dramatically in terms of model assumptions:
• Least squares assumes f (x) is well approximated by a globally linear function.

20 2. Overview of Supervised Learning

• k-nearest neighbors assumes f (x) is well approximated by a locally constant function.

Although the latter seems more palatable, we have already seen that we may pay a price for this ﬂexibility.
Many of the more modern techniques described in this book are model based, although far more ﬂexible than the rigid linear model. For example, additive models assume that

p
f (X) = fj(Xj).
j=1

(2.17)

This retains the additivity of the linear model, but each coordinate function fj is arbitrary. It turns out that the optimal estimate for the additive model uses techniques such as k-nearest neighbors to approximate univariate conditional expectations simultaneously for each of the coordinate functions. Thus the problems of estimating a conditional expectation in high dimensions are swept away in this case by imposing some (often unrealistic) model assumptions, in this case additivity.
Are we happy with the criterion (2.11)? What happens if we replace the L2 loss function with the L1: E|Y − f (X)|? The solution in this case is the conditional median,

fˆ(x) = median(Y |X = x),

(2.18)

which is a diﬀerent measure of location, and its estimates are more robust

than those for the conditional mean. L1 criteria have discontinuities in their derivatives, which have hindered their widespread use. Other more

resistant loss functions will be mentioned in later chapters, but squared

error is analytically convenient and the most popular.

What do we do when the output is a categorical variable G? The same

paradigm works here, except we need a diﬀerent loss function for penalizing prediction errors. An estimate Gˆ will assume values in G, the set of possible

classes. Our loss function can be represented by a K × K matrix L, where

K = card(G). L will be zero on the diagonal and nonnegative elsewhere,

where L(k, ℓ) is the price paid for classifying an observation belonging to

class Gk as Gℓ. Most often we use the zero–one loss function, where all misclassiﬁcations are charged a single unit. The expected prediction error

is

EPE = E[L(G, Gˆ(X))],

(2.19)

where again the expectation is taken with respect to the joint distribution Pr(G, X). Again we condition, and can write EPE as

K
EPE = EX L[Gk, Gˆ(X)]Pr(Gk|X)
k=1

(2.20)

2.4 Statistical Decision Theory 21
Bayes Optimal Classifier
ooooooooo oo oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo o ooooo o ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ................................................................................................... ...................................................................................................
FIGURE 2.5. The optimal Bayes decision boundary for the simulation example of Figures 2.1, 2.2 and 2.3. Since the generating density is known for each class, this boundary can be calculated exactly (Exercise 2.2).

and again it suﬃces to minimize EPE pointwise:

K
Gˆ(x) = argming∈G L(Gk, g)Pr(Gk|X = x).
k=1
With the 0–1 loss function this simpliﬁes to

(2.21)

or simply

Gˆ(x) = argming∈G[1 − Pr(g|X = x)]

(2.22)

Gˆ(x)

=

Gk

if

Pr(Gk |X

=

x)

=

max Pr(g|X
g∈G

=

x).

(2.23)

This reasonable solution is known as the Bayes classiﬁer, and says that we classify to the most probable class, using the conditional (discrete) distribution Pr(G|X). Figure 2.5 shows the Bayes-optimal decision boundary for our simulation example. The error rate of the Bayes classiﬁer is called the Bayes rate.

22 2. Overview of Supervised Learning
Again we see that the k-nearest neighbor classiﬁer directly approximates this solution—a majority vote in a nearest neighborhood amounts to exactly this, except that conditional probability at a point is relaxed to conditional probability within a neighborhood of a point, and probabilities are estimated by training-sample proportions.
Suppose for a two-class problem we had taken the dummy-variable approach and coded G via a binary Y , followed by squared error loss estimation. Then fˆ(X) = E(Y |X) = Pr(G = G1|X) if G1 corresponded to Y = 1. Likewise for a K-class problem, E(Yk|X) = Pr(G = Gk|X). This shows that our dummy-variable regression procedure, followed by classiﬁcation to the largest ﬁtted value, is another way of representing the Bayes classiﬁer. Although this theory is exact, in practice problems can occur, depending on the regression model used. For example, when linear regression is used, fˆ(X) need not be positive, and we might be suspicious about using it as an estimate of a probability. We will discuss a variety of approaches to modeling Pr(G|X) in Chapter 4.
2.5 Local Methods in High Dimensions
We have examined two learning techniques for prediction so far: the stable but biased linear model and the less stable but apparently less biased class of k-nearest-neighbor estimates. It would seem that with a reasonably large set of training data, we could always approximate the theoretically optimal conditional expectation by k-nearest-neighbor averaging, since we should be able to ﬁnd a fairly large neighborhood of observations close to any x and average them. This approach and our intuition breaks down in high dimensions, and the phenomenon is commonly referred to as the curse of dimensionality (Bellman, 1961). There are many manifestations of this problem, and we will examine a few here.
Consider the nearest-neighbor procedure for inputs uniformly distributed in a p-dimensional unit hypercube, as in Figure 2.6. Suppose we send out a hypercubical neighborhood about a target point to capture a fraction r of the observations. Since this corresponds to a fraction r of the unit volume, the expected edge length will be ep(r) = r1/p. In ten dimensions e10(0.01) = 0.63 and e10(0.1) = 0.80, while the entire range for each input is only 1.0. So to capture 1% or 10% of the data to form a local average, we must cover 63% or 80% of the range of each input variable. Such neighborhoods are no longer “local.” Reducing r dramatically does not help much either, since the fewer observations we average, the higher is the variance of our ﬁt.
Another consequence of the sparse sampling in high dimensions is that all sample points are close to an edge of the sample. Consider N data points uniformly distributed in a p-dimensional unit ball centered at the origin. Suppose we consider a nearest-neighbor estimate at the origin. The median

Distance 0.0 0.2 0.4 0.6 0.8 1.0

Unit Cube 1

2.5 Local Methods in High Dimensions 23
p=10 p=3 p=2
p=1

0

1 Neighborhood

0.0

0.2

0.4

0.6

Fraction of Volume

FIGURE 2.6. The curse of dimensionality is well illustrated by a subcubical neighborhood for uniform data in a unit cube. The ﬁgure on the right shows the side-length of the subcube needed to capture a fraction r of the volume of the data, for diﬀerent dimensions p. In ten dimensions we need to cover 80% of the range of each coordinate to capture 10% of the data.

distance from the origin to the closest data point is given by the expression

d(p, N ) =

1

−

1 1/N 2

1/p

(2.24)

(Exercise 2.3). A more complicated expression exists for the mean distance to the closest point. For N = 500, p = 10 , d(p, N ) ≈ 0.52, more than halfway to the boundary. Hence most data points are closer to the boundary of the sample space than to any other data point. The reason that this presents a problem is that prediction is much more diﬃcult near the edges of the training sample. One must extrapolate from neighboring sample points rather than interpolate between them.
Another manifestation of the curse is that the sampling density is proportional to N 1/p, where p is the dimension of the input space and N is the sample size. Thus, if N1 = 100 represents a dense sample for a single input problem, then N10 = 10010 is the sample size required for the same sampling density with 10 inputs. Thus in high dimensions all feasible training samples sparsely populate the input space.
Let us construct another uniform example. Suppose we have 1000 training examples xi generated uniformly on [−1, 1]p. Assume that the true relationship between X and Y is

Y = f (X) = e−8||X||2 ,

without any measurement error. We use the 1-nearest-neighbor rule to predict y0 at the test-point x0 = 0. Denote the training set by T . We can

24 2. Overview of Supervised Learning
compute the expected prediction error at x0 for our procedure, averaging over all such samples of size 1000. Since the problem is deterministic, this is the mean squared error (MSE) for estimating f (0):

MSE(x0) = ET [f (x0) − yˆ0]2 = ET [yˆ0 − ET (yˆ0)]2 + [ET (yˆ0) − f (x0)]2 = VarT (yˆ0) + Bias2(yˆ0).

(2.25)

Figure 2.7 illustrates the setup. We have broken down the MSE into two components that will become familiar as we proceed: variance and squared bias. Such a decomposition is always possible and often useful, and is known as the bias–variance decomposition. Unless the nearest neighbor is at 0, yˆ0 will be smaller than f (0) in this example, and so the average estimate will be biased downward. The variance is due to the sampling variance of the 1-nearest neighbor. In low dimensions and with N = 1000, the nearest neighbor is very close to 0, and so both the bias and variance are small. As the dimension increases, the nearest neighbor tends to stray further from the target point, and both bias and variance are incurred. By p = 10, for more than 99% of the samples the nearest neighbor is a distance greater than 0.5 from the origin. Thus as p increases, the estimate tends to be 0 more often than not, and hence the MSE levels oﬀ at 1.0, as does the bias, and the variance starts dropping (an artifact of this example).
Although this is a highly contrived example, similar phenomena occur more generally. The complexity of functions of many variables can grow exponentially with the dimension, and if we wish to be able to estimate such functions with the same accuracy as function in low dimensions, then we need the size of our training set to grow exponentially as well. In this example, the function is a complex interaction of all p variables involved.
The dependence of the bias term on distance depends on the truth, and it need not always dominate with 1-nearest neighbor. For example, if the function always involves only a few dimensions as in Figure 2.8, then the variance can dominate instead.
Suppose, on the other hand, that we know that the relationship between Y and X is linear,

Y = XT β + ε,

(2.26)

where ε ∼ N (0, σ2) and we ﬁt the model by least squares to the train-

ing data. For an arbitrary test point x0, we have yˆ0 = xT0 βˆ, which can

be of

written as yˆ0 X(XT X)−1x0

= xT0 β . Since

+

N i=1

ℓi

(x0

)εi

,

under this model

where ℓi(x0) is the ith element the least squares estimates are

2.5 Local Methods in High Dimensions 25

f(X) 0.0 0.2 0.4 0.6 0.8 1.0

1-NN in One Dimension
•

-1.0

-0.5

0.0

0.5

1.0

X

X2

-1.0

-0.5

0.0

0.5

1.0

1-NN in One vs. Two Dimensions

•

•

••

••• • • ••

•• •

• ••• •

•• •

•

• •

•

-1.0

-0.5

0.0

0.5

1.0

X1

0.8

0.6

Average Distance to Nearest Neighbor

0.4

Distance to 1-NN vs. Dimension

• •

• • •
•
• • ••

2

4

6

8

10

Dimension

Mse

0.0

0.2

0.4

0.6

0.8

1.0

MSE vs. Dimension

• MSE • Variance • Sq. Bias

•• • •

•

••

• ••••••••••

2

4

6

8

10

Dimension

0.2

0.0

FIGURE 2.7. A simulation example, demonstrating the curse of dimensionality and its eﬀect on MSE, bias and variance. The input features are uniformly distributed in [−1, 1]p for p = 1, . . . , 10 The top left panel shows the target function (no noise) in IR: f (X) = e−8||X||2 , and demonstrates the error that 1-nearest neighbor makes in estimating f (0). The training point is indicated by the blue tick mark. The top right panel illustrates why the radius of the 1-nearest neighborhood increases with dimension p. The lower left panel shows the average radius of the 1-nearest neighborhoods. The lower-right panel shows the MSE, squared bias and variance curves as a function of dimension p.

MSE 0.0 0.05 0.10 0.15 0.20 0.25

f(X)

2

3

4

26 2. Overview of Supervised Learning
1-NN in One Dimension

MSE vs. Dimension

• MSE • Variance • Sq. Bias

•• • •

•• •

•

1

•

•

•

•

••

• •

•

•

•

•

•

•

0

-1.0

-0.5

0.0

0.5

1.0

2

4

6

8

10

X

Dimension

FIGURE 2.8. A simulation example with the same setup as in Figure 2.7. Here

the

function is constant in

all but

one

dimension: f (X)

=

1 2

(X1

+

1)3.

The

variance dominates.

unbiased, we ﬁnd that

EPE(x0) = Ey0|x0 ET (y0 − yˆ0)2

= Var(y0|x0) + ET [yˆ0 − ET yˆ0]2 + [ET yˆ0 − xT0 β]2

= Var(y0|x0) + VarT (yˆ0) + Bias2(yˆ0)

= σ2 + ET xT0 (XT X)−1x0σ2 + 02.

(2.27)

Here we have incurred an additional variance σ2 in the prediction error, since our target is not deterministic. There is no bias, and the variance depends on x0. If N is large and T were selected at random, and assuming E(X) = 0, then XT X → N Cov(X) and

Ex0 EPE(x0) ∼ Ex0 xT0 Cov(X)−1x0σ2/N + σ2 = trace[Cov(X)−1Cov(x0)]σ2/N + σ2
= σ2(p/N ) + σ2.

(2.28)

Here we see that the expected EPE increases linearly as a function of p, with slope σ2/N . If N is large and/or σ2 is small, this growth in variance is negligible (0 in the deterministic case). By imposing some heavy restrictions on the class of models being ﬁtted, we have avoided the curse of dimensionality. Some of the technical details in (2.27) and (2.28) are derived in Exercise 2.5.
Figure 2.9 compares 1-nearest neighbor vs. least squares in two situations, both of which have the form Y = f (X) + ε, X uniform as before, and ε ∼ N (0, 1). The sample size is N = 500. For the orange curve, f (x)

2.5 Local Methods in High Dimensions 27 Expected Prediction Error of 1NN vs. OLS
••••••••••

EPE Ratio 1.6 1.7 1.8 1.9 2.0 2.1

• Linear • Cubic

• •

• •••••••

2

4

6

8

10

Dimension

FIGURE 2.9. The curves show the expected prediction error (at x0 = 0) for

1-nearest neighbor relative to least squares for the model Y = f (X) + ε. For the

orange

curve,

f (x)

=

x1,

while

for

the

blue

curve

f (x)

=

1 2

(x1

+

1)3.

is linear in the ﬁrst coordinate, for the blue curve, cubic as in Figure 2.8. Shown is the relative EPE of 1-nearest neighbor to least squares, which appears to start at around 2 for the linear case. Least squares is unbiased in this case, and as discussed above the EPE is slightly above σ2 = 1. The EPE for 1-nearest neighbor is always above 2, since the variance of fˆ(x0) in this case is at least σ2, and the ratio increases with dimension as the nearest neighbor strays from the target point. For the cubic case, least squares is biased, which moderates the ratio. Clearly we could manufacture examples where the bias of least squares would dominate the variance, and the 1-nearest neighbor would come out the winner.
By relying on rigid assumptions, the linear model has no bias at all and negligible variance, while the error in 1-nearest neighbor is substantially larger. However, if the assumptions are wrong, all bets are oﬀ and the 1-nearest neighbor may dominate. We will see that there is a whole spectrum of models between the rigid linear models and the extremely ﬂexible 1-nearest-neighbor models, each with their own assumptions and biases, which have been proposed speciﬁcally to avoid the exponential growth in complexity of functions in high dimensions by drawing heavily on these assumptions.
Before we delve more deeply, let us elaborate a bit on the concept of statistical models and see how they ﬁt into the prediction framework.

28 2. Overview of Supervised Learning
2.6 Statistical Models, Supervised Learning and Function Approximation
Our goal is to ﬁnd a useful approximation fˆ(x) to the function f (x) that underlies the predictive relationship between the inputs and outputs. In the theoretical setting of Section 2.4, we saw that squared error loss lead us to the regression function f (x) = E(Y |X = x) for a quantitative response. The class of nearest-neighbor methods can be viewed as direct estimates of this conditional expectation, but we have seen that they can fail in at least two ways:
• if the dimension of the input space is high, the nearest neighbors need not be close to the target point, and can result in large errors;
• if special structure is known to exist, this can be used to reduce both the bias and the variance of the estimates.
We anticipate using other classes of models for f (x), in many cases specifically designed to overcome the dimensionality problems, and here we discuss a framework for incorporating them into the prediction problem.

2.6.1 A Statistical Model for the Joint Distribution Pr(X, Y )
Suppose in fact that our data arose from a statistical model

Y = f (X) + ε,

(2.29)

where the random error ε has E(ε) = 0 and is independent of X. Note that for this model, f (x) = E(Y |X = x), and in fact the conditional distribution Pr(Y |X) depends on X only through the conditional mean f (x).
The additive error model is a useful approximation to the truth. For most systems the input–output pairs (X, Y ) will not have a deterministic relationship Y = f (X). Generally there will be other unmeasured variables that also contribute to Y , including measurement error. The additive model assumes that we can capture all these departures from a deterministic relationship via the error ε.
For some problems a deterministic relationship does hold. Many of the classiﬁcation problems studied in machine learning are of this form, where the response surface can be thought of as a colored map deﬁned in IRp. The training data consist of colored examples from the map {xi, gi}, and the goal is to be able to color any point. Here the function is deterministic, and the randomness enters through the x location of the training points. For the moment we will not pursue such problems, but will see that they can be handled by techniques appropriate for the error-based models.
The assumption in (2.29) that the errors are independent and identically distributed is not strictly necessary, but seems to be at the back of our mind

2.6 Statistical Models, Supervised Learning and Function Approximation 29
when we average squared errors uniformly in our EPE criterion. With such a model it becomes natural to use least squares as a data criterion for model estimation as in (2.1). Simple modiﬁcations can be made to avoid the independence assumption; for example, we can have Var(Y |X = x) = σ(x), and now both the mean and variance depend on X. In general the conditional distribution Pr(Y |X) can depend on X in complicated ways, but the additive error model precludes these.
So far we have concentrated on the quantitative response. Additive error models are typically not used for qualitative outputs G; in this case the target function p(X) is the conditional density Pr(G|X), and this is modeled directly. For example, for two-class data, it is often reasonable to assume that the data arise from independent binary trials, with the probability of one particular outcome being p(X), and the other 1 − p(X). Thus if Y is the 0–1 coded version of G, then E(Y |X = x) = p(x), but the variance depends on x as well: Var(Y |X = x) = p(x)[1 − p(x)].
2.6.2 Supervised Learning
Before we launch into more statistically oriented jargon, we present the function-ﬁtting paradigm from a machine learning point of view. Suppose for simplicity that the errors are additive and that the model Y = f (X) + ε is a reasonable assumption. Supervised learning attempts to learn f by example through a teacher. One observes the system under study, both the inputs and outputs, and assembles a training set of observations T = (xi, yi), i = 1, . . . , N . The observed input values to the system xi are also fed into an artiﬁcial system, known as a learning algorithm (usually a computer program), which also produces outputs fˆ(xi) in response to the inputs. The learning algorithm has the property that it can modify its input/output relationship fˆ in response to diﬀerences yi − fˆ(xi) between the original and generated outputs. This process is known as learning by example. Upon completion of the learning process the hope is that the artiﬁcial and real outputs will be close enough to be useful for all sets of inputs likely to be encountered in practice.
2.6.3 Function Approximation
The learning paradigm of the previous section has been the motivation for research into the supervised learning problem in the ﬁelds of machine learning (with analogies to human reasoning) and neural networks (with biological analogies to the brain). The approach taken in applied mathematics and statistics has been from the perspective of function approximation and estimation. Here the data pairs {xi, yi} are viewed as points in a (p + 1)-dimensional Euclidean space. The function f (x) has domain equal to the p-dimensional input subspace, and is related to the data via a model

30 2. Overview of Supervised Learning

such as yi = f (xi) + εi. For convenience in this chapter we will assume the domain is IRp, a p-dimensional Euclidean space, although in general the inputs can be of mixed type. The goal is to obtain a useful approximation to f (x) for all x in some region of IRp, given the representations in T . Although somewhat less glamorous than the learning paradigm, treating supervised learning as a problem in function approximation encourages the geometrical concepts of Euclidean spaces and mathematical concepts of probabilistic inference to be applied to the problem. This is the approach taken in this book.
Many of the approximations we will encounter have associated a set of parameters θ that can be modiﬁed to suit the data at hand. For example, the linear model f (x) = xT β has θ = β. Another class of useful approximators can be expressed as linear basis expansions

K
fθ(x) = hk(x)θk,
k=1

(2.30)

where the hk are a suitable set of functions or transformations of the input vector x. Traditional examples are polynomial and trigonometric expansions, where for example hk might be x21, x1x22, cos(x1) and so on. We also encounter nonlinear expansions, such as the sigmoid transformation
common to neural network models,

hk (x)

=

1+

1 exp(−xT

βk

)

.

(2.31)

We can use least squares to estimate the parameters θ in fθ as we did for the linear model, by minimizing the residual sum-of-squares

N
RSS(θ) = (yi − fθ(xi))2
i=1

(2.32)

as a function of θ. This seems a reasonable criterion for an additive error model. In terms of function approximation, we imagine our parameterized function as a surface in p + 1 space, and what we observe are noisy realizations from it. This is easy to visualize when p = 2 and the vertical coordinate is the output y, as in Figure 2.10. The noise is in the output coordinate, so we ﬁnd the set of parameters such that the ﬁtted surface gets as close to the observed points as possible, where close is measured by the sum of squared vertical errors in RSS(θ).
For the linear model we get a simple closed form solution to the minimization problem. This is also true for the basis function methods, if the basis functions themselves do not have any hidden parameters. Otherwise the solution requires either iterative methods or numerical optimization.
While least squares is generally very convenient, it is not the only criterion used and in some cases would not make much sense. A more general

2.6 Statistical Models, Supervised Learning and Function Approximation 31

••

•

•

•

•• •

•• • ••

•• • •• • •
•• •

•••••••• •• •

• •

• ••••• • •
•• • •

•

•

FIGURE 2.10. Least squares ﬁtting of a function of two inputs. The parameters of fθ(x) are chosen so as to minimize the sum-of-squared vertical errors.

principle for estimation is maximum likelihood estimation. Suppose we have a random sample yi, i = 1, . . . , N from a density Prθ(y) indexed by some parameters θ. The log-probability of the observed sample is

N
L(θ) = log Prθ(yi).
i=1

(2.33)

The principle of maximum likelihood assumes that the most reasonable

values for θ are those for which the probability of the observed sample is

largest. Least squares for the additive error model Y = fθ(X) + ε, with ε ∼ N (0, σ2), is equivalent to maximum likelihood using the conditional

likelihood

Pr(Y |X, θ) = N (fθ(X), σ2).

(2.34)

So although the additional assumption of normality seems more restrictive, the results are the same. The log-likelihood of the data is

L(θ)

=

−

N 2

log(2π)

−

N

log σ

−

1 2σ2

N
(yi − fθ(xi))2,

i=1

(2.35)

and the only term involving θ is the last, which is RSS(θ) up to a scalar negative multiplier.
A more interesting example is the multinomial likelihood for the regression function Pr(G|X) for a qualitative output G. Suppose we have a model Pr(G = Gk|X = x) = pk,θ(x), k = 1, . . . , K for the conditional probability of each class given X, indexed by the parameter vector θ. Then the

32 2. Overview of Supervised Learning

log-likelihood (also referred to as the cross-entropy) is

N
L(θ) = log pgi,θ(xi),
i=1

(2.36)

and when maximized it delivers values of θ that best conform with the data in this likelihood sense.

2.7 Structured Regression Models
We have seen that although nearest-neighbor and other local methods focus directly on estimating the function at a point, they face problems in high dimensions. They may also be inappropriate even in low dimensions in cases where more structured approaches can make more eﬃcient use of the data. This section introduces classes of such structured approaches. Before we proceed, though, we discuss further the need for such classes.

2.7.1 Diﬃculty of the Problem
Consider the RSS criterion for an arbitrary function f ,

N
RSS(f ) = (yi − f (xi))2.
i=1

(2.37)

Minimizing (2.37) leads to inﬁnitely many solutions: any function fˆ passing through the training points (xi, yi) is a solution. Any particular solution chosen might be a poor predictor at test points diﬀerent from the training points. If there are multiple observation pairs xi, yiℓ, ℓ = 1, . . . , Ni at each value of xi, the risk is limited. In this case, the solutions pass through the average values of the yiℓ at each xi; see Exercise 2.6. The situation is similar to the one we have already visited in Section 2.4; indeed, (2.37) is the ﬁnite sample version of (2.11) on page 18. If the sample size N were suﬃciently large such that repeats were guaranteed and densely arranged, it would seem that these solutions might all tend to the limiting conditional expectation.
In order to obtain useful results for ﬁnite N , we must restrict the eligible solutions to (2.37) to a smaller set of functions. How to decide on the nature of the restrictions is based on considerations outside of the data. These restrictions are sometimes encoded via the parametric representation of fθ, or may be built into the learning method itself, either implicitly or explicitly. These restricted classes of solutions are the major topic of this book. One thing should be clear, though. Any restrictions imposed on f that lead to a unique solution to (2.37) do not really remove the ambiguity

2.8 Classes of Restricted Estimators 33
caused by the multiplicity of solutions. There are inﬁnitely many possible restrictions, each leading to a unique solution, so the ambiguity has simply been transferred to the choice of constraint.
In general the constraints imposed by most learning methods can be described as complexity restrictions of one kind or another. This usually means some kind of regular behavior in small neighborhoods of the input space. That is, for all input points x suﬃciently close to each other in some metric, fˆ exhibits some special structure such as nearly constant, linear or low-order polynomial behavior. The estimator is then obtained by averaging or polynomial ﬁtting in that neighborhood.
The strength of the constraint is dictated by the neighborhood size. The larger the size of the neighborhood, the stronger the constraint, and the more sensitive the solution is to the particular choice of constraint. For example, local constant ﬁts in inﬁnitesimally small neighborhoods is no constraint at all; local linear ﬁts in very large neighborhoods is almost a globally linear model, and is very restrictive.
The nature of the constraint depends on the metric used. Some methods, such as kernel and local regression and tree-based methods, directly specify the metric and size of the neighborhood. The nearest-neighbor methods discussed so far are based on the assumption that locally the function is constant; close to a target input x0, the function does not change much, and so close outputs can be averaged to produce fˆ(x0). Other methods such as splines, neural networks and basis-function methods implicitly deﬁne neighborhoods of local behavior. In Section 5.4.1 we discuss the concept of an equivalent kernel (see Figure 5.8 on page 157), which describes this local dependence for any method linear in the outputs. These equivalent kernels in many cases look just like the explicitly deﬁned weighting kernels discussed above—peaked at the target point and falling away smoothly away from it.
One fact should be clear by now. Any method that attempts to produce locally varying functions in small isotropic neighborhoods will run into problems in high dimensions—again the curse of dimensionality. And conversely, all methods that overcome the dimensionality problems have an associated—and often implicit or adaptive—metric for measuring neighborhoods, which basically does not allow the neighborhood to be simultaneously small in all directions.
2.8 Classes of Restricted Estimators
The variety of nonparametric regression techniques or learning methods fall into a number of diﬀerent classes depending on the nature of the restrictions imposed. These classes are not distinct, and indeed some methods fall in several classes. Here we give a brief summary, since detailed descriptions

34 2. Overview of Supervised Learning
are given in later chapters. Each of the classes has associated with it one or more parameters, sometimes appropriately called smoothing parameters, that control the eﬀective size of the local neighborhood. Here we describe three broad classes.

2.8.1 Roughness Penalty and Bayesian Methods
Here the class of functions is controlled by explicitly penalizing RSS(f ) with a roughness penalty

PRSS(f ; λ) = RSS(f ) + λJ(f ).

(2.38)

The user-selected functional J(f ) will be large for functions f that vary too rapidly over small regions of input space. For example, the popular cubic smoothing spline for one-dimensional inputs is the solution to the penalized least-squares criterion

N
PRSS(f ; λ) = (yi − f (xi))2 + λ [f ′′(x)]2dx.
i=1

(2.39)

The roughness penalty here controls large values of the second derivative

of f , and the amount of penalty is dictated by λ ≥ 0. For λ = 0 no penalty

is imposed, and any interpolating function will do, while for λ = ∞ only

functions linear in x are permitted.

Penalty functionals J can be constructed for functions in any dimension,

and special versions can be created to impose special structure. For ex-

ample, additive penalties J(f ) =

p j=1

J (fj)

are

used

in

conjunction

with

additive functions f (X) =

p j=1

fj

(Xj )

to

create

additive

models

with

smooth coordinate functions. Similarly, projection pursuit regression mod-

els have f (X) =

M m=1

gm(αm T X)

for

adaptively

chosen

directions

αm,

and

the functions gm can each have an associated roughness penalty.

Penalty function, or regularization methods, express our prior belief that

the type of functions we seek exhibit a certain type of smooth behavior, and

indeed can usually be cast in a Bayesian framework. The penalty J corre-

sponds to a log-prior, and PRSS(f ; λ) the log-posterior distribution, and

minimizing PRSS(f ; λ) amounts to ﬁnding the posterior mode. We discuss

roughness-penalty approaches in Chapter 5 and the Bayesian paradigm in

Chapter 8.

2.8.2 Kernel Methods and Local Regression
These methods can be thought of as explicitly providing estimates of the regression function or conditional expectation by specifying the nature of the local neighborhood, and of the class of regular functions ﬁtted locally. The local neighborhood is speciﬁed by a kernel function Kλ(x0, x) which assigns

2.8 Classes of Restricted Estimators 35

weights to points x in a region around x0 (see Figure 6.1 on page 192). For example, the Gaussian kernel has a weight function based on the Gaussian density function

Kλ(x0, x)

=

1 λ

exp

− ||x − x0||2 2λ

(2.40)

and assigns weights to points that die exponentially with their squared Euclidean distance from x0. The parameter λ corresponds to the variance of the Gaussian density, and controls the width of the neighborhood. The simplest form of kernel estimate is the Nadaraya–Watson weighted average

fˆ(x0) =

N i=1

Kλ(x0,

N i=1

Kλ

(x0

xi)yi , xi)

.

(2.41)

In general we can deﬁne a local regression estimate of f (x0) as fθˆ(x0), where θˆ minimizes

N
RSS(fθ, x0) = Kλ(x0, xi)(yi − fθ(xi))2,
i=1

(2.42)

and fθ is some parameterized function, such as a low-order polynomial. Some examples are:

• fθ(x) = θ0, the constant function; this results in the Nadaraya– Watson estimate in (2.41) above.

• fθ(x) = θ0 + θ1x gives the popular local linear regression model.
Nearest-neighbor methods can be thought of as kernel methods having a more data-dependent metric. Indeed, the metric for k-nearest neighbors is

Kk(x, x0) = I(||x − x0|| ≤ ||x(k) − x0||),

where x(k) is the training observation ranked kth in distance from x0, and I(S) is the indicator of the set S.
These methods of course need to be modiﬁed in high dimensions, to avoid the curse of dimensionality. Various adaptations are discussed in Chapter 6.

2.8.3 Basis Functions and Dictionary Methods

This class of methods includes the familiar linear and polynomial expansions, but more importantly a wide variety of more ﬂexible models. The model for f is a linear expansion of basis functions

M
fθ(x) = θmhm(x),
m=1

(2.43)

36 2. Overview of Supervised Learning

where each of the hm is a function of the input x, and the term linear here refers to the action of the parameters θ. This class covers a wide variety of methods. In some cases the sequence of basis functions is prescribed, such as a basis for polynomials in x of total degree M .
For one-dimensional x, polynomial splines of degree K can be represented by an appropriate sequence of M spline basis functions, determined in turn by M −K −1 knots. These produce functions that are piecewise polynomials of degree K between the knots, and joined up with continuity of degree K − 1 at the knots. As an example consider linear splines, or piecewise linear functions. One intuitively satisfying basis consists of the functions b1(x) = 1, b2(x) = x, and bm+2(x) = (x − tm)+, m = 1, . . . , M − 2, where tm is the mth knot, and z+ denotes positive part. Tensor products of spline bases can be used for inputs with dimensions larger than one (see Section 5.2, and the CART and MARS models in Chapter 9.) The parameter M controls the degree of the polynomial or the number of knots in the case of splines.
Radial basis functions are symmetric p-dimensional kernels located at particular centroids,

M
fθ(x) = Kλm (µm, x)θm;
m=1

(2.44)

for example, the Gaussian kernel Kλ(µ, x) = e−||x−µ||2/2λ is popular.

Radial basis functions have centroids µm and scales λm that have to

be determined. The spline basis functions have knots. In general we would

like the data to dictate them as well. Including these as parameters changes

the regression problem from a straightforward linear problem to a combi-

natorially hard nonlinear problem. In practice, shortcuts such as greedy

algorithms or two stage processes are used. Section 6.7 describes some such

approaches.

A single-layer feed-forward neural network model with linear output

weights can be thought of as an adaptive basis function method. The model

has the form

M

fθ(x) =

βmσ(αm T x + bm),

(2.45)

m=1

where σ(x) = 1/(1 + e−x) is known as the activation function. Here, as in the projection pursuit model, the directions αm and the bias terms bm have to be determined, and their estimation is the meat of the computation. Details are given in Chapter 11.
These adaptively chosen basis function methods are also known as dictionary methods, where one has available a possibly inﬁnite set or dictionary D of candidate basis functions from which to choose, and models are built up by employing some kind of search mechanism.

2.9 Model Selection and the Bias–Variance Tradeoﬀ 37
2.9 Model Selection and the Bias–Variance Tradeoﬀ

All the models described above and many others discussed in later chapters have a smoothing or complexity parameter that has to be determined:

• the multiplier of the penalty term;

• the width of the kernel;

• or the number of basis functions.

In the case of the smoothing spline, the parameter λ indexes models ranging from a straight line ﬁt to the interpolating model. Similarly a local degreem polynomial model ranges between a degree-m global polynomial when the window size is inﬁnitely large, to an interpolating ﬁt when the window size shrinks to zero. This means that we cannot use residual sum-of-squares on the training data to determine these parameters as well, since we would always pick those that gave interpolating ﬁts and hence zero residuals. Such a model is unlikely to predict future data well at all.
The k-nearest-neighbor regression ﬁt fˆk(x0) usefully illustrates the competing forces that aﬀect the predictive ability of such approximations. Suppose the data arise from a model Y = f (X) + ε, with E(ε) = 0 and Var(ε) = σ2. For simplicity here we assume that the values of xi in the sample are ﬁxed in advance (nonrandom). The expected prediction error at x0, also known as test or generalization error, can be decomposed:

EPEk(x0) = E[(Y − fˆk(x0))2|X = x0]

= σ2 + [Bias2(fˆk(x0)) + VarT (fˆk(x0))]

=

σ2 +

f (x0)

−

1 k

k

f (x(ℓ))

2
+

σ2 k

.

ℓ=1

(2.46) (2.47)

The subscripts in parentheses (ℓ) indicate the sequence of nearest neighbors to x0.
There are three terms in this expression. The ﬁrst term σ2 is the irreducible error—the variance of the new test target—and is beyond our control, even if we know the true f (x0).
The second and third terms are under our control, and make up the mean squared error of fˆk(x0) in estimating f (x0), which is broken down into a bias component and a variance component. The bias term is the squared diﬀerence between the true mean f (x0) and the expected value of the estimate—[ET (fˆk(x0)) − f (x0)]2—where the expectation averages the randomness in the training data. This term will most likely increase with k, if the true function is reasonably smooth. For small k the few closest neighbors will have values f (x(ℓ)) close to f (x0), so their average should

38 2. Overview of Supervised Learning
High Bias Low Variance

Low Bias High Variance

Prediction Error

Test Sample

Training Sample

Low

High

Model Complexity

FIGURE 2.11. Test and training error as a function of model complexity.

be close to f (x0). As k grows, the neighbors are further away, and then anything can happen.

The variance term is simply the variance of an average here, and de-

creases as the inverse of k. So as k varies, there is a bias–variance tradeoﬀ.

More generally, as the model complexity of our procedure is increased, the

variance tends to increase and the squared bias tends to decrease. The op-

posite behavior occurs as the model complexity is decreased. For k-nearest

neighbors, the model complexity is controlled by k.

Typically we would like to choose our model complexity to trade bias

oﬀ with variance in such a way as to minimize the test error. An obvious

estimate

of

test

error

is

the

training

error

1 N

i(yi − yˆi)2. Unfortunately

training error is not a good estimate of test error, as it does not properly

account for model complexity.

Figure 2.11 shows the typical behavior of the test and training error, as

model complexity is varied. The training error tends to decrease whenever

we increase the model complexity, that is, whenever we ﬁt the data harder.

However with too much ﬁtting, the model adapts itself too closely to the

training data, and will not generalize well (i.e., have large test error). In

that case the predictions fˆ(x0) will have large variance, as reﬂected in the

last term of expression (2.46). In contrast, if the model is not complex

enough, it will underﬁt and may have large bias, again resulting in poor

generalization. In Chapter 7 we discuss methods for estimating the test

error of a prediction method, and hence estimating the optimal amount of

model complexity for a given prediction method and training set.

Bibliographic Notes

Exercises 39

Some good general books on the learning problem are Duda et al. (2000), Bishop (1995),(Bishop, 2006), Ripley (1996), Cherkassky and Mulier (2007) and Vapnik (1996). Parts of this chapter are based on Friedman (1994b).

Exercises
Ex. 2.1 Suppose each of K-classes has an associated target tk, which is a vector of all zeros, except a one in the kth position. Show that classifying to the largest element of yˆ amounts to choosing the closest target, mink ||tk − yˆ||, if the elements of yˆ sum to one.
Ex. 2.2 Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5.
Ex. 2.3 Derive equation (2.24).
Ex. 2.4 The edge eﬀect problem discussed on page 23 is not peculiar to uniform sampling from bounded domains. Consider inputs drawn from a spherical multinormal distribution X ∼ N (0, Ip). The squared distance from any sample point to the origin has a χ2p distribution with mean p. Consider a prediction point x0 drawn from this distribution, and let a = x0/||x0|| be an associated unit vector. Let zi = aT xi be the projection of each of the training points on this direction.
Show that the zi are distributed N (0, 1) with expected squared distance from the origin 1, while the target point has expected squared distance p from the origin.
Hence for p = 10, a randomly drawn test point is about 3.1 standard deviations from the origin, while all the training points are on average one standard deviation along direction a. So most prediction points see themselves as lying on the edge of the training set.
Ex. 2.5
(a) Derive equation (2.27). The last line makes use of (3.8) through a conditioning argument.
(b) Derive equation (2.28), making use of the cyclic property of the trace operator [trace(AB) = trace(BA)], and its linearity (which allows us to interchange the order of trace and expectation).
Ex. 2.6 Consider a regression problem with inputs xi and outputs yi, and a parameterized model fθ(x) to be ﬁt by least squares. Show that if there are observations with tied or identical values of x, then the ﬁt can be obtained from a reduced weighted least squares problem.

40 2. Overview of Supervised Learning

Ex. 2.7 Suppose we have a sample of N pairs xi, yi drawn i.i.d. from the distribution characterized as follows:

xi ∼ h(x), the design density
yi = f (xi) + εi, f is the regression function εi ∼ (0, σ2) (mean zero, variance σ2)

We construct an estimator for f linear in the yi,
N
fˆ(x0) = ℓi(x0; X )yi,
i=1
where the weights ℓi(x0; X ) do not depend on the yi, but do depend on the entire training sequence of xi, denoted here by X .
(a) Show that linear regression and k-nearest-neighbor regression are members of this class of estimators. Describe explicitly the weights ℓi(x0; X ) in each of these cases.

(b) Decompose the conditional mean-squared error

EY|X (f (x0) − fˆ(x0))2

into a conditional squared bias and a conditional variance component. Like X , Y represents the entire training sequence of yi.
(c) Decompose the (unconditional) mean-squared error

EY,X (f (x0) − fˆ(x0))2

into a squared bias and a variance component.

(d) Establish a relationship between the squared biases and variances in the above two cases.

Ex. 2.8 Compare the classiﬁcation performance of linear regression and k– nearest neighbor classiﬁcation on the zipcode data. In particular, consider only the 2’s and 3’s, and k = 1, 3, 5, 7 and 15. Show both the training and test error for each choice. The zipcode data are available from the book website www-stat.stanford.edu/ElemStatLearn.

Ex. 2.9 Consider a linear regression model with p parameters, ﬁt by least

squares to a set of training data (x1, y1), . . . , (xN , yN ) drawn at random from a population. Let βˆ be the least squares estimate. Suppose we have

some test data (x˜1, y˜1), . . . , (x˜M , y˜M ) drawn at random from the same pop-

ulation

as

the

training

data.

If

Rtr (β )

=

1 N

N 1

(yi

−

βT xi)2

and

Rte(β)

=

1 M

M 1

(y˜i

−

βT x˜i)2,

prove

that

E[Rtr(βˆ)] ≤ E[Rte(βˆ)],

Exercises 41
where the expectations are over all that is random in each expression. [This exercise was brought to our attention by Ryan Tibshirani, from a homework assignment given by Andrew Ng.]

42 2. Overview of Supervised Learning

3
Linear Methods for Regression

This is page 43 Printer: Opaque this

3.1 Introduction
A linear regression model assumes that the regression function E(Y |X) is linear in the inputs X1, . . . , Xp. Linear models were largely developed in the precomputer age of statistics, but even in today’s computer era there are still good reasons to study and use them. They are simple and often provide an adequate and interpretable description of how the inputs aﬀect the output. For prediction purposes they can sometimes outperform fancier nonlinear models, especially in situations with small numbers of training cases, low signal-to-noise ratio or sparse data. Finally, linear methods can be applied to transformations of the inputs and this considerably expands their scope. These generalizations are sometimes called basis-function methods, and are discussed in Chapter 5.
In this chapter we describe linear methods for regression, while in the next chapter we discuss linear methods for classiﬁcation. On some topics we go into considerable detail, as it is our ﬁrm belief that an understanding of linear methods is essential for understanding nonlinear ones. In fact, many nonlinear techniques are direct generalizations of the linear methods discussed here.

44 3. Linear Methods for Regression

3.2 Linear Regression Models and Least Squares

As introduced in Chapter 2, we have an input vector XT = (X1, X2, . . . , Xp), and want to predict a real-valued output Y . The linear regression model

has the form
p

f (X) = β0 + Xjβj.

(3.1)

j=1

The linear model either assumes that the regression function E(Y |X) is linear, or that the linear model is a reasonable approximation. Here the βj’s are unknown parameters or coeﬃcients, and the variables Xj can come from diﬀerent sources:

• quantitative inputs;

• transformations of quantitative inputs, such as log, square-root or square;
• basis expansions, such as X2 = X12, X3 = X13, leading to a polynomial representation;

• numeric or “dummy” coding of the levels of qualitative inputs. For

example, if G is a ﬁve-level factor input, we might create Xj, j =

1, . . . , 5, such that Xj = I(G = j). Together this group of Xj repre-

sents the eﬀect of G by a set of level-dependent constants, since in

5 j=1

Xj βj ,

one

of

the

Xj s

is

one,

and

the

others

are

zero.

• interactions between variables, for example, X3 = X1 · X2.

No matter the source of the Xj, the model is linear in the parameters. Typically we have a set of training data (x1, y1) . . . (xN , yN ) from which
to estimate the parameters β. Each xi = (xi1, xi2, . . . , xip)T is a vector of feature measurements for the ith case. The most popular estimation method is least squares, in which we pick the coeﬃcients β = (β0, β1, . . . , βp)T to minimize the residual sum of squares

N

RSS(β) =

(yi − f (xi))2

i=1

N

p

2

=

yi − β0 − xij βj .

i=1

j=1

(3.2)

From a statistical point of view, this criterion is reasonable if the training
observations (xi, yi) represent independent random draws from their population. Even if the xi’s were not drawn randomly, the criterion is still valid if the yi’s are conditionally independent given the inputs xi. Figure 3.1 illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional

3.2 Linear Regression Models and Least Squares 45 Y

•

••

• •

• •

• •

• •
•••••

• ••

••

• •• • ••

• •

•

•• •

•••••

• •

•

•• •

•• • X2

•

•

• X1

FIGURE 3.1. Linear least squares ﬁtting with X ∈ IR2. We seek the linear function of X that minimizes the sum of squared residuals from Y .

space occupied by the pairs (X, Y ). Note that (3.2) makes no assumptions about the validity of model (3.1); it simply ﬁnds the best linear ﬁt to the data. Least squares ﬁtting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of ﬁt.
How do we minimize (3.2)? Denote by X the N × (p + 1) matrix with each row an input vector (with a 1 in the ﬁrst position), and similarly let y be the N -vector of outputs in the training set. Then we can write the residual sum-of-squares as

RSS(β) = (y − Xβ)T (y − Xβ).

(3.3)

This is a quadratic function in the p + 1 parameters. Diﬀerentiating with respect to β we obtain

∂RSS ∂β

=

−2XT (y

−

Xβ)

∂2RSS ∂β∂βT

= 2XT X.

(3.4)

Assuming (for the moment) that X has full column rank, and hence XT X is positive deﬁnite, we set the ﬁrst derivative to zero

XT (y − Xβ) = 0

(3.5)

to obtain the unique solution βˆ = (XT X)−1XT y.

(3.6)

46 3. Linear Methods for Regression y

x2

yˆ
x1
FIGURE 3.2. The N -dimensional geometry of least squares regression with two predictors. The outcome vector y is orthogonally projected onto the hyperplane spanned by the input vectors x1 and x2. The projection yˆ represents the vector of the least squares predictions

The predicted values at an input vector x0 are given by fˆ(x0) = (1 : x0)T βˆ; the ﬁtted values at the training inputs are

yˆ = Xβˆ = X(XT X)−1XT y,

(3.7)

where yˆi = fˆ(xi). The matrix H = X(XT X)−1XT appearing in equation (3.7) is sometimes called the “hat” matrix because it puts the hat on y.
Figure 3.2 shows a diﬀerent geometrical representation of the least squares estimate, this time in IRN . We denote the column vectors of X by x0, x1, . . . , xp, with x0 ≡ 1. For much of what follows, this ﬁrst column is treated like any other. These vectors span a subspace of IRN , also referred to as the column space of X. We minimize RSS(β) = y − Xβ 2 by choosing βˆ so that the residual vector y − yˆ is orthogonal to this subspace. This orthogonality is expressed in (3.5), and the resulting estimate yˆ is hence the orthogonal projection of y onto this subspace. The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix.
It might happen that the columns of X are not linearly independent, so that X is not of full rank. This would occur, for example, if two of the inputs were perfectly correlated, (e.g., x2 = 3x1). Then XT X is singular and the least squares coeﬃcients βˆ are not uniquely deﬁned. However, the ﬁtted values yˆ = Xβˆ are still the projection of y onto the column space of X; there is just more than one way to express that projection in terms of the column vectors of X. The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion. There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X. Most regression software packages detect these redundancies and automatically implement

3.2 Linear Regression Models and Least Squares 47

some strategy for removing them. Rank deﬁciencies can also occur in signal and image analysis, where the number of inputs p can exceed the number of training cases N . In this case, the features are typically reduced by ﬁltering or else the ﬁtting is controlled by regularization (Section 5.2.3 and Chapter 18).
Up to now we have made minimal assumptions about the true distribution of the data. In order to pin down the sampling properties of βˆ, we now assume that the observations yi are uncorrelated and have constant variance σ2, and that the xi are ﬁxed (non random). The variance–covariance matrix of the least squares parameter estimates is easily derived from (3.6) and is given by

Var(βˆ) = (XT X)−1σ2.

(3.8)

Typically one estimates the variance σ2 by

σˆ2

=

N

1 −p

−

1

N
(yi − yˆi)2.

i=1

The N − p − 1 rather than N in the denominator makes σˆ2 an unbiased estimate of σ2: E(σˆ2) = σ2.
To draw inferences about the parameters and the model, additional assumptions are needed. We now assume that (3.1) is the correct model for the mean; that is, the conditional expectation of Y is linear in X1, . . . , Xp. We also assume that the deviations of Y around its expectation are additive and Gaussian. Hence

Y = E(Y |X1, . . . , Xp) + ε
p
= β0 + Xjβj + ε,
j=1

(3.9)

where the error ε is a Gaussian random variable with expectation zero and variance σ2, written ε ∼ N (0, σ2).
Under (3.9), it is easy to show that

βˆ ∼ N (β, (XT X)−1σ2).

(3.10)

This is a multivariate normal distribution with mean vector and variance– covariance matrix as shown. Also

(N − p − 1)σˆ2 ∼ σ2χ2N−p−1,

(3.11)

a chi-squared distribution with N − p − 1 degrees of freedom. In addition βˆ and σˆ2 are statistically independent. We use these distributional properties
to form tests of hypothesis and conﬁdence intervals for the parameters βj.

48 3. Linear Methods for Regression

Tail Probabilities 0.01 0.02 0.03 0.04 0.05 0.06

t30
t100 normal

2.0

2.2

2.4

2.6

2.8

3.0

Z

FIGURE 3.3. The tail probabilities Pr(|Z| > z) for three distributions, t30, t100 and standard normal. Shown are the appropriate quantiles for testing signiﬁcance at the p = 0.05 and 0.01 levels. The diﬀerence between t and the standard normal becomes negligible for N bigger than about 100.

To test the hypothesis that a particular coeﬃcient βj = 0, we form the standardized coeﬃcient or Z-score

zj = σˆ√βˆjvj ,

(3.12)

where vj is the jth diagonal element of (XT X)−1. Under the null hypothesis that βj = 0, zj is distributed as tN−p−1 (a t distribution with N − p − 1 degrees of freedom), and hence a large (absolute) value of zj will lead to rejection of this null hypothesis. If σˆ is replaced by a known value σ, then zj would have a standard normal distribution. The diﬀerence between the tail quantiles of a t-distribution and a standard normal become negligible as the sample size increases, and so we typically use the normal quantiles (see Figure 3.3).
Often we need to test for the signiﬁcance of groups of coeﬃcients simultaneously. For example, to test if a categorical variable with k levels can be excluded from a model, we need to test whether the coeﬃcients of the dummy variables used to represent the levels can all be set to zero. Here we use the F statistic,

F

=

(RSS0 − RSS1)/(p1 RSS1/(N − p1 −

− p0) 1)

,

(3.13)

where RSS1 is the residual sum-of-squares for the least squares ﬁt of the bigger model with p1 +1 parameters, and RSS0 the same for the nested smaller model with p0 + 1 parameters, having p1 − p0 parameters constrained to be

3.2 Linear Regression Models and Least Squares 49

zero. The F statistic measures the change in residual sum-of-squares per
additional parameter in the bigger model, and it is normalized by an estimate of σ2. Under the Gaussian assumptions, and the null hypothesis that
the smaller model is correct, the F statistic will have a Fp1−p0,N−p1−1 distribution. It can be shown (Exercise 3.1) that the zj in (3.12) are equivalent to the F statistic for dropping the single coeﬃcient βj from the model. For large N , the quantiles of Fp1−p0,N−p1−1 approach those of χ2p1−p0 /(p1 −p0).
Similarly, we can isolate βj in (3.10) to obtain a 1−2α conﬁdence interval for βj:

(βˆj

−

z

(1−α)

1
vj2

σˆ,

βˆj

+

z

(1−α)

1
vj2

σˆ).

(3.14)

Here z(1−α) is the 1 − α percentile of the normal distribution:

z(1−0.025) = 1.96, z(1−.05) = 1.645, etc.

Hence the standard practice of reporting βˆ ± 2 · se(βˆ) amounts to an approximate 95% conﬁdence interval. Even if the Gaussian error assumption does not hold, this interval will be approximately correct, with its coverage approaching 1 − 2α as the sample size N → ∞.
In a similar fashion we can obtain an approximate conﬁdence set for the entire parameter vector β, namely

Cβ = {β|(βˆ − β)T XT X(βˆ − β) ≤ σˆ2χ2p+1(1−α)},

(3.15)

where χ2ℓ (1−α) is the 1 − α percentile of the chi-squared distribution on ℓ degrees of freedom: for example, χ25(1−0.05) = 11.1, χ25(1−0.1) = 9.2. This conﬁdence set for β generates a corresponding conﬁdence set for the true function f (x) = xT β, namely {xT β|β ∈ Cβ} (Exercise 3.2; see also Fig-
ure 5.4 in Section 5.2.2 for examples of conﬁdence bands for functions).

3.2.1 Example: Prostate Cancer
The data for this example come from a study by Stamey et al. (1989). They examined the correlation between the level of prostate-speciﬁc antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. The variables are log cancer volume (lcavol), log prostate weight (lweight), age, log of the amount of benign prostatic hyperplasia (lbph), seminal vesicle invasion (svi), log of capsular penetration (lcp), Gleason score (gleason), and percent of Gleason scores 4 or 5 (pgg45). The correlation matrix of the predictors given in Table 3.1 shows many strong correlations. Figure 1.1 (page 3) of Chapter 1 is a scatterplot matrix showing every pairwise plot between the variables. We see that svi is a binary variable, and gleason is an ordered categorical variable. We see, for

50 3. Linear Methods for Regression

TABLE 3.1. Correlations of predictors in the prostate cancer data.

lweight age
lbph svi lcp
gleason pgg45

lcavol
0.300 0.286 0.063 0.593 0.692 0.426 0.483

lweight
0.317 0.437 0.181 0.157 0.024 0.074

age
0.287 0.129 0.173 0.366 0.276

lbph
−0.139 −0.089
0.033 −0.030

svi
0.671 0.307 0.481

lcp
0.476 0.663

gleason 0.757

TABLE 3.2. Linear model ﬁt to the prostate cancer data. The Z score is the coeﬃcient divided by its standard error (3.12). Roughly a Z score larger than two in absolute value is signiﬁcantly nonzero at the p = 0.05 level.

Term Intercept
lcavol lweight
age lbph
svi lcp gleason pgg45

Coeﬃcient
2.46 0.68 0.26 −0.14 0.21 0.31 −0.29 −0.02 0.27

Std. Error 0.09 0.13 0.10 0.10 0.10 0.12 0.15 0.15 0.15

Z Score
27.60 5.37 2.75 −1.40 2.06 2.47 −1.87 −0.15 1.74

example, that both lcavol and lcp show a strong relationship with the response lpsa, and with each other. We need to ﬁt the eﬀects jointly to untangle the relationships between the predictors and the response.
We ﬁt a linear model to the log of prostate-speciﬁc antigen, lpsa, after ﬁrst standardizing the predictors to have unit variance. We randomly split the dataset into a training set of size 67 and a test set of size 30. We applied least squares estimation to the training set, producing the estimates, standard errors and Z-scores shown in Table 3.2. The Z-scores are deﬁned in (3.12), and measure the eﬀect of dropping that variable from the model. A Z-score greater than 2 in absolute value is approximately signiﬁcant at the 5% level. (For our example, we have nine parameters, and the 0.025 tail quantiles of the t67−9 distribution are ±2.002!) The predictor lcavol shows the strongest eﬀect, with lweight and svi also strong. Notice that lcp is not signiﬁcant, once lcavol is in the model (when used in a model without lcavol, lcp is strongly signiﬁcant). We can also test for the exclusion of a number of terms at once, using the F -statistic (3.13). For example, we consider dropping all the non-signiﬁcant terms in Table 3.2, namely age,

3.2 Linear Regression Models and Least Squares 51

lcp, gleason, and pgg45. We get

F

=

(32.81 − 29.43)/(9 − 29.43/(67 − 9)

5)

=

1.67,

(3.16)

which has a p-value of 0.17 (Pr(F4,58 > 1.67) = 0.17), and hence is not signiﬁcant.
The mean prediction error on the test data is 0.521. In contrast, prediction using the mean training value of lpsa has a test error of 1.057, which is called the “base error rate.” Hence the linear model reduces the base error rate by about 50%. We will return to this example later to compare various selection and shrinkage methods.

3.2.2 The Gauss–Markov Theorem

One of the most famous results in statistics asserts that the least squares estimates of the parameters β have the smallest variance among all linear unbiased estimates. We will make this precise here, and also make clear that the restriction to unbiased estimates is not necessarily a wise one. This observation will lead us to consider biased estimates such as ridge regression later in the chapter. We focus on estimation of any linear combination of the parameters θ = aT β; for example, predictions f (x0) = xT0 β are of this form. The least squares estimate of aT β is

θˆ = aT βˆ = aT (XT X)−1XT y.

(3.17)

Considering X to be ﬁxed, this is a linear function cT0 y of the response vector y. If we assume that the linear model is correct, aT βˆ is unbiased since

E(aT βˆ) = E(aT (XT X)−1XT y) = aT (XT X)−1XT Xβ = aT β.

(3.18)

The Gauss–Markov theorem states that if we have any other linear estimator θ˜ = cT y that is unbiased for aT β, that is, E(cT y) = aT β, then

Var(aT βˆ) ≤ Var(cT y).

(3.19)

The proof (Exercise 3.3) uses the triangle inequality. For simplicity we have stated the result in terms of estimation of a single parameter aT β, but with
a few more deﬁnitions one can state it in terms of the entire parameter
vector β (Exercise 3.3). Consider the mean squared error of an estimator θ˜ in estimating θ:

MSE(θ˜) = E(θ˜ − θ)2 = Var(θ˜) + [E(θ˜) − θ]2.

(3.20)

52 3. Linear Methods for Regression

The ﬁrst term is the variance, while the second term is the squared bias. The Gauss-Markov theorem implies that the least squares estimator has the smallest mean squared error of all linear estimators with no bias. However, there may well exist a biased estimator with smaller mean squared error. Such an estimator would trade a little bias for a larger reduction in variance. Biased estimates are commonly used. Any method that shrinks or sets to zero some of the least squares coeﬃcients may result in a biased estimate. We discuss many examples, including variable subset selection and ridge regression, later in this chapter. From a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance. We go into these issues in more detail in Chapter 7.
Mean squared error is intimately related to prediction accuracy, as discussed in Chapter 2. Consider the prediction of the new response at input x0,

Y0 = f (x0) + ε0.
Then the expected prediction error of an estimate f˜(x0) = xT0 β˜ is E(Y0 − f˜(x0))2 = σ2 + E(xT0 β˜ − f (x0))2 = σ2 + MSE(f˜(x0)).

(3.21) (3.22)

Therefore, expected prediction error and mean squared error diﬀer only by the constant σ2, representing the variance of the new observation y0.

3.2.3 Multiple Regression from Simple Univariate Regression

The linear model (3.1) with p > 1 inputs is called the multiple linear regression model. The least squares estimates (3.6) for this model are best understood in terms of the estimates for the univariate (p = 1) linear model, as we indicate in this section.
Suppose ﬁrst that we have a univariate model with no intercept, that is,

Y = Xβ + ε.

(3.23)

The least squares estimate and residuals are

βˆ =

N 1

xiyi

N 1

x2i

,

ri = yi − xiβˆ.

(3.24)

In convenient vector notation, we let y = (y1, . . . , yN )T , x = (x1, . . . , xN )T and deﬁne

N

x, y =

xiyi,

i=1

= xT y,

(3.25)

3.2 Linear Regression Models and Least Squares 53

the inner product between x and y1. Then we can write

βˆ =

x, y x, x

,

r = y − xβˆ.

(3.26)

As we will see, this simple univariate regression provides the building block for multiple linear regression. Suppose next that the inputs x1, x2, . . . , xp (the columns of the data matrix X) are orthogonal; that is xj, xk = 0 for all j = k. Then it is easy to check that the multiple least squares estimates βˆj are equal to xj, y / xj, xj —the univariate estimates. In other words, when the inputs are orthogonal, they have no eﬀect on each other’s parameter estimates in the model.
Orthogonal inputs occur most often with balanced, designed experiments (where orthogonality is enforced), but almost never with observational data. Hence we will have to orthogonalize them in order to carry this idea further. Suppose next that we have an intercept and a single input x. Then the least squares coeﬃcient of x has the form

βˆ1 =

x − x¯1, y x − x¯1, x − x¯1

,

(3.27)

where x¯ = i xi/N , and 1 = x0, the vector of N ones. We can view the estimate (3.27) as the result of two applications of the simple regression (3.26). The steps are:
1. regress x on 1 to produce the residual z = x − x¯1;
2. regress y on the residual z to give the coeﬃcient βˆ1.
In this procedure, “regress b on a” means a simple univariate regression of b on a with no intercept, producing coeﬃcient γˆ = a, b / a, a and residual vector b − γˆa. We say that b is adjusted for a, or is “orthogonalized” with respect to a.
Step 1 orthogonalizes x with respect to x0 = 1. Step 2 is just a simple univariate regression, using the orthogonal predictors 1 and z. Figure 3.4 shows this process for two general inputs x1 and x2. The orthogonalization does not change the subspace spanned by x1 and x2, it simply produces an orthogonal basis for representing it.
This recipe generalizes to the case of p inputs, as shown in Algorithm 3.1. Note that the inputs z0, . . . , zj−1 in step 2 are orthogonal, hence the simple regression coeﬃcients computed there are in fact also the multiple regression coeﬃcients.

1The inner-product notation is suggestive of generalizations of linear regression to diﬀerent metric spaces, as well as to probability spaces.

54 3. Linear Methods for Regression y

x2 z

yˆ
x1
FIGURE 3.4. Least squares regression by orthogonalization of the inputs. The vector x2 is regressed on the vector x1, leaving the residual vector z. The regression of y on z gives the multiple regression coeﬃcient of x2. Adding together the projections of y on each of x1 and z gives the least squares ﬁt yˆ.

Algorithm 3.1 Regression by Successive Orthogonalization. 1. Initialize z0 = x0 = 1. 2. For j = 1, 2, . . . , p

Regress xj on z0, z1, . . . , , zj−1 to produce coeﬃcients γˆℓj =

zℓ, xj / zℓ, zℓ , ℓ = 0, . . . , j − 1 and residual vector zj =

xj −

j−1 k=0

γˆkj

zk .

3. Regress y on the residual zp to give the estimate βˆp.

The result of this algorithm is

βˆp =

zp, y zp, zp

.

(3.28)

Re-arranging the residual in step 2, we can see that each of the xj is a linear combination of the zk, k ≤ j. Since the zj are all orthogonal, they form a basis for the column space of X, and hence the least squares projection
onto this subspace is yˆ. Since zp alone involves xp (with coeﬃcient 1), we see that the coeﬃcient (3.28) is indeed the multiple regression coeﬃcient of
y on xp. This key result exposes the eﬀect of correlated inputs in multiple regression. Note also that by rearranging the xj, any one of them could be in the last position, and a similar results holds. Hence stated more
generally, we have shown that the jth multiple regression coeﬃcient is the
univariate regression coeﬃcient of y on xj·012...(j−1)(j+1)...,p, the residual after regressing xj on x0, x1, . . . , xj−1, xj+1, . . . , xp:

3.2 Linear Regression Models and Least Squares 55

The multiple regression coeﬃcient βˆj represents the additional contribution of xj on y, after xj has been adjusted for x0, x1, . . . , xj−1, xj+1, . . . , xp.
If xp is highly correlated with some of the other xk’s, the residual vector zp will be close to zero, and from (3.28) the coeﬃcient βˆp will be very unstable. This will be true for all the variables in the correlated set. In such situations, we might have all the Z-scores (as in Table 3.2) be small— any one of the set can be deleted—yet we cannot delete them all. From (3.28) we also obtain an alternate formula for the variance estimates (3.8),

Var(βˆp) =

σ2 zp, zp

=

σ2 zp

2

.

(3.29)

In other words, the precision with which we can estimate βˆp depends on the length of the residual vector zp; this represents how much of xp is unexplained by the other xk’s.
Algorithm 3.1 is known as the Gram–Schmidt procedure for multiple
regression, and is also a useful numerical strategy for computing the estimates. We can obtain from it not just βˆp, but also the entire multiple least squares ﬁt, as shown in Exercise 3.4.
We can represent step 2 of Algorithm 3.1 in matrix form:

X = ZΓ,

(3.30)

where Z has as columns the zj (in order), and Γ is the upper triangular matrix with entries γˆkj. Introducing the diagonal matrix D with jth diagonal entry Djj = zj , we get

X = ZD−1DΓ = QR,

(3.31)

the so-called QR decomposition of X. Here Q is an N × (p + 1) orthogonal matrix, QT Q = I, and R is a (p + 1) × (p + 1) upper triangular matrix.
The QR decomposition represents a convenient orthogonal basis for the column space of X. It is easy to see, for example, that the least squares solution is given by

βˆ = R−1QT y, yˆ = QQT y.

(3.32) (3.33)

Equation (3.32) is easy to solve because R is upper triangular (Exercise 3.4).

56 3. Linear Methods for Regression

3.2.4 Multiple Outputs

Suppose we have multiple outputs Y1, Y2, . . . , YK that we wish to predict from our inputs X0, X1, X2, . . . , Xp. We assume a linear model for each output

p
Yk = β0k + Xj βjk + εk
j=1
= fk(X) + εk.

(3.34) (3.35)

With N training cases we can write the model in matrix notation

Y = XB + E.

(3.36)

Here Y is the N ×K response matrix, with ik entry yik, X is the N ×(p+1) input matrix, B is the (p + 1) × K matrix of parameters and E is the N × K matrix of errors. A straightforward generalization of the univariate
loss function (3.2) is

KN

RSS(B) =

(yik − fk(xi))2

k=1 i=1

= tr[(Y − XB)T (Y − XB)].

(3.37) (3.38)

The least squares estimates have exactly the same form as before

Bˆ = (XT X)−1XT Y.

(3.39)

Hence the coeﬃcients for the kth outcome are just the least squares estimates in the regression of yk on x0, x1, . . . , xp. Multiple outputs do not aﬀect one another’s least squares estimates.
If the errors ε = (ε1, . . . , εK ) in (3.34) are correlated, then it might seem appropriate to modify (3.37) in favor of a multivariate version. Speciﬁcally, suppose Cov(ε) = Σ, then the multivariate weighted criterion

N
RSS(B; Σ) = (yi − f (xi))T Σ−1(yi − f (xi))
i=1

(3.40)

arises naturally from multivariate Gaussian theory. Here f (x) is the vector function (f1(x), . . . , fK (x))T , and yi the vector of K responses for observation i. However, it can be shown that again the solution is given by
(3.39); K separate regressions that ignore the correlations (Exercise 3.11).
If the Σi vary among observations, then this is no longer the case, and the solution for B no longer decouples.
In Section 3.7 we pursue the multiple outcome problem, and consider
situations where it does pay to combine the regressions.

3.3 Subset Selection

3.3 Subset Selection 57

There are two reasons why we are often not satisﬁed with the least squares estimates (3.6).
• The ﬁrst is prediction accuracy: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coeﬃcients to zero. By doing so we sacriﬁce a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy.
• The second reason is interpretation. With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest eﬀects. In order to get the “big picture,” we are willing to sacriﬁce some of the small details.
In this section we describe a number of approaches to variable subset selection with linear regression. In later sections we discuss shrinkage and hybrid approaches for controlling variance, as well as other dimension-reduction strategies. These all fall under the general heading model selection. Model selection is not restricted to linear models; Chapter 7 covers this topic in some detail.
With subset selection we retain only a subset of the variables, and eliminate the rest from the model. Least squares regression is used to estimate the coeﬃcients of the inputs that are retained. There are a number of different strategies for choosing the subset.

3.3.1 Best-Subset Selection
Best subset regression ﬁnds for each k ∈ {0, 1, 2, . . . , p} the subset of size k that gives smallest residual sum of squares (3.2). An eﬃcient algorithm— the leaps and bounds procedure (Furnival and Wilson, 1974)—makes this feasible for p as large as 30 or 40. Figure 3.5 shows all the subset models for the prostate cancer example. The lower boundary represents the models that are eligible for selection by the best-subsets approach. Note that the best subset of size 2, for example, need not include the variable that was in the best subset of size 1 (for this example all the subsets are nested). The best-subset curve (red lower boundary in Figure 3.5) is necessarily decreasing, so cannot be used to select the subset size k. The question of how to choose k involves the tradeoﬀ between bias and variance, along with the more subjective desire for parsimony. There are a number of criteria that one may use; typically we choose the smallest model that minimizes an estimate of the expected prediction error.
Many of the other approaches that we discuss in this chapter are similar, in that they use the training data to produce a sequence of models varying in complexity and indexed by a single parameter. In the next section we use

58 3. Linear Methods for Regression

100

80

60

•

•• •

••

•• •
•

••••• •• •••
••••

•••• ••
•••••••••••••

• ••
••••••••••••••••

• •••
•••••••••

• ••
••••••

•
•••

•

40

Residual Sum−of−Squares

20

0

0

1

2

3

4

5

6

7

8

Subset Size k

FIGURE 3.5. All possible subset models for the prostate cancer example. At each subset size is shown the residual sum-of-squares for each model of that size.

cross-validation to estimate prediction error and select k; the AIC criterion is a popular alternative. We defer more detailed discussion of these and other approaches to Chapter 7.

3.3.2 Forward- and Backward-Stepwise Selection
Rather than search through all possible subsets (which becomes infeasible for p much larger than 40), we can seek a good path through them. Forwardstepwise selection starts with the intercept, and then sequentially adds into the model the predictor that most improves the ﬁt. With many candidate predictors, this might seem like a lot of computation; however, clever updating algorithms can exploit the QR decomposition for the current ﬁt to rapidly establish the next candidate (Exercise 3.9). Like best-subset regression, forward stepwise produces a sequence of models indexed by k, the subset size, which must be determined.
Forward-stepwise selection is a greedy algorithm, producing a nested sequence of models. In this sense it might seem sub-optimal compared to best-subset selection. However, there are several reasons why it might be preferred:

3.3 Subset Selection 59
• Computational; for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence (even when p ≫ N ).
• Statistical; a price is paid in variance for selecting the best subset of each size; forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias.
Best Subset Forward Stepwise Backward Stepwise Forward Stagewise

E||βˆ(k) − β||2
0.65 0.70 0.75 0.80 0.85 0.90 0.95

0

5

10

15

20

25

30

Subset Size k

FIGURE 3.6. Comparison of four subset-selection techniques on a simulated linear regression problem Y = XT β + ε. There are N = 300 observations on p = 31 standard Gaussian variables, with pairwise correlations all equal to 0.85. For 10 of the variables, the coeﬃcients are drawn at random from a N (0, 0.4) distribution; the rest are zero. The noise ε ∼ N (0, 6.25), resulting in a signal-to-noise ratio of 0.64. Results are averaged over 50 simulations. Shown is the mean-squared error of the estimated coeﬃcient βˆ(k) at each step from the true β.
Backward-stepwise selection starts with the full model, and sequentially deletes the predictor that has the least impact on the ﬁt. The candidate for dropping is the variable with the smallest Z-score (Exercise 3.10). Backward selection can only be used when N > p, while forward stepwise can always be used.
Figure 3.6 shows the results of a small simulation study to compare best-subset regression with the simpler alternatives forward and backward selection. Their performance is very similar, as is often the case. Included in the ﬁgure is forward stagewise regression (next section), which takes longer to reach minimum error.

60 3. Linear Methods for Regression
On the prostate cancer example, best-subset, forward and backward selection all gave exactly the same sequence of terms.
Some software packages implement hybrid stepwise-selection strategies that consider both forward and backward moves at each step, and select the “best” of the two. For example in the R package the step function uses the AIC criterion for weighing the choices, which takes proper account of the number of parameters ﬁt; at each step an add or drop will be performed that minimizes the AIC score.
Other more traditional packages base the selection on F -statistics, adding “signiﬁcant” terms, and dropping “non-signiﬁcant” terms. These are out of fashion, since they do not take proper account of the multiple testing issues. It is also tempting after a model search to print out a summary of the chosen model, such as in Table 3.2; however, the standard errors are not valid, since they do not account for the search process. The bootstrap (Section 8.2) can be useful in such settings.
Finally, we note that often variables come in groups (such as the dummy variables that code a multi-level categorical predictor). Smart stepwise procedures (such as step in R) will add or drop whole groups at a time, taking proper account of their degrees-of-freedom.
3.3.3 Forward-Stagewise Regression
Forward-stagewise regression (FS) is even more constrained than forwardstepwise regression. It starts like forward-stepwise regression, with an intercept equal to y¯, and centered predictors with coeﬃcients initially all 0. At each step the algorithm identiﬁes the variable most correlated with the current residual. It then computes the simple linear regression coeﬃcient of the residual on this chosen variable, and then adds it to the current coeﬃcient for that variable. This is continued till none of the variables have correlation with the residuals—i.e. the least-squares ﬁt when N > p.
Unlike forward-stepwise regression, none of the other variables are adjusted when a term is added to the model. As a consequence, forward stagewise can take many more than p steps to reach the least squares ﬁt, and historically has been dismissed as being ineﬃcient. It turns out that this “slow ﬁtting” can pay dividends in high-dimensional problems. We see in Section 3.8.1 that both forward stagewise and a variant which is slowed down even further are quite competitive, especially in very highdimensional problems.
Forward-stagewise regression is included in Figure 3.6. In this example it takes over 1000 steps to get all the correlations below 10−4. For subset size k, we plotted the error for the last step for which there where k nonzero coeﬃcients. Although it catches up with the best ﬁt, it takes longer to do so.

3.4 Shrinkage Methods 61
3.3.4 Prostate Cancer Data Example (Continued)
Table 3.3 shows the coeﬃcients from a number of diﬀerent selection and shrinkage methods. They are best-subset selection using an all-subsets search, ridge regression, the lasso, principal components regression and partial least squares. Each method has a complexity parameter, and this was chosen to minimize an estimate of prediction error based on tenfold cross-validation; full details are given in Section 7.10. Brieﬂy, cross-validation works by dividing the training data randomly into ten equal parts. The learning method is ﬁt—for a range of values of the complexity parameter—to nine-tenths of the data, and the prediction error is computed on the remaining one-tenth. This is done in turn for each one-tenth of the data, and the ten prediction error estimates are averaged. From this we obtain an estimated prediction error curve as a function of the complexity parameter.
Note that we have already divided these data into a training set of size 67 and a test set of size 30. Cross-validation is applied to the training set, since selecting the shrinkage parameter is part of the training process. The test set is there to judge the performance of the selected model.
The estimated prediction error curves are shown in Figure 3.7. Many of the curves are very ﬂat over large ranges near their minimum. Included are estimated standard error bands for each estimated error rate, based on the ten error estimates computed by cross-validation. We have used the “one-standard-error” rule—we pick the most parsimonious model within one standard error of the minimum (Section 7.10, page 244). Such a rule acknowledges the fact that the tradeoﬀ curve is estimated with error, and hence takes a conservative approach.
Best-subset selection chose to use the two predictors lcvol and lweight. The last two lines of the table give the average prediction error (and its estimated standard error) over the test set.
3.4 Shrinkage Methods
By retaining a subset of the predictors and discarding the rest, subset selection produces a model that is interpretable and has possibly lower prediction error than the full model. However, because it is a discrete process— variables are either retained or discarded—it often exhibits high variance, and so doesn’t reduce the prediction error of the full model. Shrinkage methods are more continuous, and don’t suﬀer as much from high variability.
3.4.1 Ridge Regression
Ridge regression shrinks the regression coeﬃcients by imposing a penalty on their size. The ridge coeﬃcients minimize a penalized residual sum of

62 3. Linear Methods for Regression
All Subsets

Ridge Regression

CV Error 0.6 0.8 1.0 1.2 1.4 1.6 1.8

CV Error 0.6 0.8 1.0 1.2 1.4 1.6 1.8

•

••••••••

0

2

4

6

8

Subset Size

Lasso

•

• • • • • • ••

0

2

4

6

8

Degrees of Freedom

Principal Components Regression

CV Error 0.6 0.8 1.0 1.2 1.4 1.6 1.8

CV Error 0.6 0.8 1.0 1.2 1.4 1.6 1.8

•

•
• ••••••

0.0

0.2

0.4

0.6

0.8

1.0

Shrinkage Factor s

Partial Least Squares

•

••••••••

0

2

4

6

8

Number of Directions

•

CV Error 0.6 0.8 1.0 1.2 1.4 1.6 1.8

••••••••

0

2

4

6

8

Number of Directions

FIGURE 3.7. Estimated prediction error curves and their standard errors for the various selection and shrinkage methods. Each curve is plotted as a function of the corresponding complexity parameter for that method. The horizontal axis has been chosen so that the model complexity increases as we move from left to right. The estimates of prediction error and their standard errors were obtained by tenfold cross-validation; full details are given in Section 7.10. The least complex model within one standard error of the best is chosen, indicated by the purple vertical broken lines.

3.4 Shrinkage Methods 63

TABLE 3.3. Estimated coeﬃcients and test error results, for diﬀerent subset and shrinkage methods applied to the prostate data. The blank entries correspond to variables omitted.

Term Intercept
lcavol lweight
age lbph
svi lcp gleason pgg45 Test Error Std Error

LS 2.465 0.680 0.263 −0.141 0.210 0.305 −0.288 −0.021 0.267 0.521 0.179

Best Subset 2.477 0.740 0.316
0.492 0.143

Ridge 2.452 0.420 0.238 −0.046 0.162 0.227 0.000 0.040 0.133 0.492 0.165

Lasso 2.468 0.533 0.169
0.002 0.094
0.479 0.164

PCR 2.497 0.543 0.289 −0.152 0.214 0.315 −0.051 0.232 −0.056 0.449 0.105

PLS 2.452 0.419 0.344 −0.026 0.220 0.243 0.079 0.011 0.084 0.528 0.152

squares,

N

p

p

βˆridge = argmin

yi − β0 − xij βj 2 + λ βj2 .

β

i=1

j=1

j=1

(3.41)

Here λ ≥ 0 is a complexity parameter that controls the amount of shrinkage: the larger the value of λ, the greater the amount of shrinkage. The coeﬃcients are shrunk toward zero (and each other). The idea of penalizing by the sum-of-squares of the parameters is also used in neural networks, where it is known as weight decay (Chapter 11).
An equivalent way to write the ridge problem is

N
βˆridge = argmin

p

2

yi − β0 − xij βj ,

β i=1

j=1

p

subject to βj2 ≤ t,

j=1

(3.42)

which makes explicit the size constraint on the parameters. There is a oneto-one correspondence between the parameters λ in (3.41) and t in (3.42). When there are many correlated variables in a linear regression model, their coeﬃcients can become poorly determined and exhibit high variance. A wildly large positive coeﬃcient on one variable can be canceled by a similarly large negative coeﬃcient on its correlated cousin. By imposing a size constraint on the coeﬃcients, as in (3.42), this problem is alleviated.
The ridge solutions are not equivariant under scaling of the inputs, and so one normally standardizes the inputs before solving (3.41). In addition,

64 3. Linear Methods for Regression

notice that the intercept β0 has been left out of the penalty term. Penal-

ization of the intercept would make the procedure depend on the origin

chosen for Y ; that is, adding a constant c to each of the targets yi would not simply result in a shift of the predictions by the same amount c. It

can be shown (Exercise 3.5) that the solution to (3.41) can be separated

into two parts, after reparametrization using centered inputs: each xij gets

replaced

by

xij

−

x¯j .

We

estimate

β0

by

y¯

=

1 N

N 1

yi.

The

remaining

co-

eﬃcients get estimated by a ridge regression without intercept, using the

centered xij. Henceforth we assume that this centering has been done, so

that the input matrix X has p (rather than p + 1) columns.

Writing the criterion in (3.41) in matrix form,

RSS(λ) = (y − Xβ)T (y − Xβ) + λβT β,

(3.43)

the ridge regression solutions are easily seen to be βˆridge = (XT X + λI)−1XT y,

(3.44)

where I is the p×p identity matrix. Notice that with the choice of quadratic penalty βT β, the ridge regression solution is again a linear function of y. The solution adds a positive constant to the diagonal of XT X before inversion. This makes the problem nonsingular, even if XT X is not of full rank, and was the main motivation for ridge regression when it was ﬁrst introduced in statistics (Hoerl and Kennard, 1970). Traditional descriptions of ridge regression start with deﬁnition (3.44). We choose to motivate it via (3.41) and (3.42), as these provide insight into how it works.
Figure 3.8 shows the ridge coeﬃcient estimates for the prostate cancer example, plotted as functions of df(λ), the eﬀective degrees of freedom implied by the penalty λ (deﬁned in (3.50) on page 68). In the case of orthonormal inputs, the ridge estimates are just a scaled version of the least squares estimates, that is, βˆridge = βˆ/(1 + λ).
Ridge regression can also be derived as the mean or mode of a posterior distribution, with a suitably chosen prior distribution. In detail, suppose yi ∼ N (β0 + xTi β, σ2), and the parameters βj are each distributed as N (0, τ 2), independently of one another. Then the (negative) log-posterior density of β, with τ 2 and σ2 assumed known, is equal to the expression in curly braces in (3.41), with λ = σ2/τ 2 (Exercise 3.6). Thus the ridge estimate is the mode of the posterior distribution; since the distribution is Gaussian, it is also the posterior mean.
The singular value decomposition (SVD) of the centered input matrix X gives us some additional insight into the nature of ridge regression. This decomposition is extremely useful in the analysis of many statistical methods. The SVD of the N × p matrix X has the form

X = UDVT .

(3.45)

3.4 Shrinkage Methods 65

0.6

0.4

0.2

Coefficients

• lcavol

• ••• • •

•

•

•

•

•

•

•

•

•

•

•

• ••••

• ••••

• • ••••

•
• ••• •

•
• •• • •

•
• • •

••
•• • •

••
••• • •

•
•• •• • • •

•
•• • • •• •

•• • • •• •

•• • •
•
•

•• • •
•• •

••
• •
• • •

•• • •
• • •

•• •• •• •• • • • •

• •

• •

••

••

•

•

••

• •

• • • • • •••

•

•

••

•

• •

•

• ••

•••

svi lweight pgg45

• lbph

• gleason

•

• age

• ••

• lcp

0.0

−0.2

0

2

4

6

8

df (λ)

FIGURE 3.8. Proﬁles of ridge coeﬃcients for the prostate cancer example, as the tuning parameter λ is varied. Coeﬃcients are plotted versus df(λ), the eﬀective degrees of freedom. A vertical line is drawn at df = 5.0, the value chosen by cross-validation.

66 3. Linear Methods for Regression

Here U and V are N × p and p × p orthogonal matrices, with the columns of U spanning the column space of X, and the columns of V spanning the row space. D is a p × p diagonal matrix, with diagonal entries d1 ≥ d2 ≥ · · · ≥ dp ≥ 0 called the singular values of X. If one or more values dj = 0, X is singular.
Using the singular value decomposition we can write the least squares ﬁtted vector as

Xβˆls = X(XT X)−1XT y = UUT y,

(3.46)

after some simpliﬁcation. Note that UT y are the coordinates of y with respect to the orthonormal basis U. Note also the similarity with (3.33); Q and U are generally diﬀerent orthogonal bases for the column space of X (Exercise 3.8).
Now the ridge solutions are

Xβˆridge = X(XT X + λI)−1XT y

= U D(D2 + λI)−1D UT y

=

p j=1

uj

d2j d2j +

λ

uTj

y,

(3.47)

where the uj are the columns of U. Note that since λ ≥ 0, we have d2j /(d2j + λ) ≤ 1. Like linear regression, ridge regression computes the coordinates of
y with respect to the orthonormal basis U. It then shrinks these coordinates by the factors d2j /(d2j + λ). This means that a greater amount of shrinkage is applied to the coordinates of basis vectors with smaller d2j .
What does a small value of d2j mean? The SVD of the centered matrix X is another way of expressing the principal components of the variables in X. The sample covariance matrix is given by S = XT X/N , and from
(3.45) we have

XT X = VD2VT ,

(3.48)

which is the eigen decomposition of XT X (and of S, up to a factor N ). The eigenvectors vj (columns of V) are also called the principal components (or Karhunen–Loeve) directions of X. The ﬁrst principal component direction v1 has the property that z1 = Xv1 has the largest sample variance amongst all normalized linear combinations of the columns of X. This sample variance is easily seen to be

Var(z1)

=

Var(Xv1)

=

d21 N

,

(3.49)

and in fact z1 = Xv1 = u1d1. The derived variable z1 is called the ﬁrst principal component of X, and hence u1 is the normalized ﬁrst principal

3.4 Shrinkage Methods 67

4

2

X2

0

Largest Principal o Component

o

o

o o oo o

o

o oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooSoooooomooooooaolooooleosoot Principal

o

Component

o

o

-2

-4

-4

-2

0

2

4

X1

FIGURE 3.9. Principal components of some input data points. The largest principal component is the direction that maximizes the variance of the projected data, and the smallest principal component minimizes that variance. Ridge regression projects y onto these components, and then shrinks the coeﬃcients of the low– variance components more than the high-variance components.

component. Subsequent principal components zj have maximum variance d2j /N , subject to being orthogonal to the earlier ones. Conversely the last principal component has minimum variance. Hence the small singular values dj correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most.
Figure 3.9 illustrates the principal components of some data points in two dimensions. If we consider ﬁtting a linear surface over this domain (the Y -axis is sticking out of the page), the conﬁguration of the data allow us to determine its gradient more accurately in the long direction than the short. Ridge regression protects against the potentially high variance of gradients estimated in the short directions. The implicit assumption is that the response will tend to vary most in the directions of high variance of the inputs. This is often a reasonable assumption, since predictors are often chosen for study because they vary with the response variable, but need not hold in general.

68 3. Linear Methods for Regression

In Figure 3.7 we have plotted the estimated prediction error versus the quantity

df(λ) = tr[X(XT X + λI)−1XT ],

= tr(Hλ)

=

p j=1

d2j d2j +

λ.

(3.50)

This monotone decreasing function of λ is the eﬀective degrees of freedom of the ridge regression ﬁt. Usually in a linear-regression ﬁt with p variables, the degrees-of-freedom of the ﬁt is p, the number of free parameters. The idea is that although all p coeﬃcients in a ridge ﬁt will be non-zero, they are ﬁt in a restricted fashion controlled by λ. Note that df(λ) = p when λ = 0 (no regularization) and df(λ) → 0 as λ → ∞. Of course there is always an additional one degree of freedom for the intercept, which was removed apriori. This deﬁnition is motivated in more detail in Section 3.4.4 and Sections 7.4–7.6. In Figure 3.7 the minimum occurs at df(λ) = 5.0. Table 3.3 shows that ridge regression reduces the test error of the full least squares estimates by a small amount.

3.4.2 The Lasso

The lasso is a shrinkage method like ridge, with subtle but important differences. The lasso estimate is deﬁned by

βˆlasso

=

N
argmin

p

2

yi − β0 − xij βj

β i=1

j=1

p

subject to |βj| ≤ t.

j=1

(3.51)

Just as in ridge regression, we can re-parametrize the constant β0 by standardizing the predictors; the solution for βˆ0 is y¯, and thereafter we ﬁt a model without an intercept (Exercise 3.5). In the signal processing literature, the lasso is also known as basis pursuit (Chen et al., 1998).
We can also write the lasso problem in the equivalent Lagrangian form

βˆlasso = argmin
β

1N 2

p

p

yi − β0 − xij βj 2 + λ |βj |

i=1

j=1

j=1

.

(3.52)

Notice the similarity to the ridge regression problem (3.42) or (3.41): the

L2 ridge penalty

p 1

βj2

is

replaced

by

the

L1

lasso

penalty

p 1

|βj |.

This

latter constraint makes the solutions nonlinear in the yi, and there is no

closed form expression as in ridge regression. Computing the lasso solution

3.4 Shrinkage Methods 69

is a quadratic programming problem, although we see in Section 3.4.4 that

eﬃcient algorithms are available for computing the entire path of solutions

as λ is varied, with the same computational cost as for ridge regression.

Because of the nature of the constraint, making t suﬃciently small will

cause some of the coeﬃcients to be exactly zero. Thus the lasso does a kind

of continuous subset selection. If t is chosen larger than t0 =

p 1

|βˆj |

(where

βˆj = βˆjls, the least squares estimates), then the lasso estimates are the βˆj’s.

On the other hand, for t = t0/2 say, then the least squares coeﬃcients are

shrunk by about 50% on average. However, the nature of the shrinkage

is not obvious, and we investigate it further in Section 3.4.4 below. Like

the subset size in variable subset selection, or the penalty parameter in

ridge regression, t should be adaptively chosen to minimize an estimate of

expected prediction error.

In Figure 3.7, for ease of interpretation, we have plotted the lasso pre-

diction error estimates versus the standardized parameter s = t/

p 1

|βˆj

|.

A value sˆ ≈ 0.36 was chosen by 10-fold cross-validation; this caused four

coeﬃcients to be set to zero (ﬁfth column of Table 3.3). The resulting

model has the second lowest test error, slightly lower than the full least

squares model, but the standard errors of the test error estimates (last line

of Table 3.3) are fairly large.

Figure 3.10 shows the lasso coeﬃcients as the standardized tuning pa-

rameter s = t/

p 1

|βˆj

|

is

varied.

At

s

=

1.0

these

are

the

least

squares

estimates; they decrease to 0 as s → 0. This decrease is not always strictly

monotonic, although it is in this example. A vertical line is drawn at

s = 0.36, the value chosen by cross-validation.

3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso
In this section we discuss and compare the three approaches discussed so far for restricting the linear regression model: subset selection, ridge regression and the lasso.
In the case of an orthonormal input matrix X the three procedures have explicit solutions. Each method applies a simple transformation to the least squares estimate βˆj, as detailed in Table 3.4.
Ridge regression does a proportional shrinkage. Lasso translates each coeﬃcient by a constant factor λ, truncating at zero. This is called “soft thresholding,” and is used in the context of wavelet-based smoothing in Section 5.9. Best-subset selection drops all variables with coeﬃcients smaller than the M th largest; this is a form of “hard-thresholding.”
Back to the nonorthogonal case; some pictures help understand their relationship. Figure 3.11 depicts the lasso (left) and ridge regression (right) when there are only two parameters. The residual sum of squares has elliptical contours, centered at the full least squares estimate. The constraint

70 3. Linear Methods for Regression

lcavol

0.6

0.4

0.2

Coefficients

svi lweight pgg45 lbph

0.0

gleason age

−0.2

lcp

0.0

0.2

0.4

0.6

0.8

1.0

Shrinkage Factor s

FIGURE 3.10. Proﬁles of lasso coeﬃcients, as the tuning parameter t is varied.

Coeﬃcients are plotted versus s = t/

p 1

|βˆj |.

A

vertical

line

is

drawn

at

s

=

0.36,

the value chosen by cross-validation. Compare Figure 3.8 on page 65; the lasso

proﬁles hit zero, while those for ridge do not. The proﬁles are piece-wise linear,

and so are computed only at the points displayed; see Section 3.4.4 for details.

3.4 Shrinkage Methods 71

TABLE 3.4. Estimators of βj in the case of orthonormal columns of X. M and λ
are constants chosen by the corresponding techniques; sign denotes the sign of its
argument (±1), and x+ denotes “positive part” of x. Below the table, estimators are shown by broken red lines. The 45◦ line in gray shows the unrestricted estimate
for reference.

Estimator

Formula

Best subset (size M ) Ridge Lasso

βˆj · I(|βˆj| ≥ |βˆ(M)|) βˆj/(1 + λ) sign(βˆj)(|βˆj| − λ)+

Best Subset

Ridge

Lasso

λ

|βˆ(M ) | (0,0)

(0,0)

(0,0)

β2

.β^

β2

.β^

β1

β1

FIGURE 3.11. Estimation picture for the lasso (left) and ridge regression
(right). Shown are contours of the error and constraint functions. The solid blue areas are the constraint regions |β1| + |β2| ≤ t and β12 + β22 ≤ t2, respectively, while the red ellipses are the contours of the least squares error function.

72 3. Linear Methods for Regression

region for ridge regression is the disk β12 + β22 ≤ t, while that for lasso is the diamond |β1| + |β2| ≤ t. Both methods ﬁnd the ﬁrst point where the elliptical contours hit the constraint region. Unlike the disk, the diamond
has corners; if the solution occurs at a corner, then it has one parameter
βj equal to zero. When p > 2, the diamond becomes a rhomboid, and has many corners, ﬂat edges and faces; there are many more opportunities for
the estimated parameters to be zero.
We can generalize ridge regression and the lasso, and view them as Bayes
estimates. Consider the criterion

N

p

p

β˜ = argmin

yi − β0 − xij βj 2 + λ |βj |q

β

i=1

j=1

j=1

(3.53)

for q ≥ 0. The contours of constant value of j |βj|q are shown in Figure 3.12, for the case of two inputs.
Thinking of |βj|q as the log-prior density for βj, these are also the equicontours of the prior distribution of the parameters. The value q = 0 corresponds to variable subset selection, as the penalty simply counts the number of nonzero parameters; q = 1 corresponds to the lasso, while q = 2 to ridge regression. Notice that for q ≤ 1, the prior is not uniform in direction, but concentrates more mass in the coordinate directions. The prior corresponding to the q = 1 case is an independent double exponential (or Laplace) distribution for each input, with density (1/2τ ) exp(−|β|/τ ) and τ = 1/λ. The case q = 1 (lasso) is the smallest q such that the constraint region is convex; non-convex constraint regions make the optimization problem more diﬃcult.
In this view, the lasso, ridge regression and best subset selection are Bayes estimates with diﬀerent priors. Note, however, that they are derived as posterior modes, that is, maximizers of the posterior. It is more common to use the mean of the posterior as the Bayes estimate. Ridge regression is also the posterior mean, but the lasso and best subset selection are not.
Looking again at the criterion (3.53), we might try using other values of q besides 0, 1, or 2. Although one might consider estimating q from the data, our experience is that it is not worth the eﬀort for the extra variance incurred. Values of q ∈ (1, 2) suggest a compromise between the lasso and ridge regression. Although this is the case, with q > 1, |βj|q is diﬀerentiable at 0, and so does not share the ability of lasso (q = 1) for

q=4

q=2

q=1

q = 0.5

q = 0.1

FIGURE 3.12. Contours of constant value of j |βj|q for given values of q.

q = 1.2

3.4 Shrinkage Methods 73
α = 0.2

Lq

Elastic Net

FIGURE 3.13. Contours of constant value of j |βj|q for q = 1.2 (left plot), and the elastic-net penalty j(αβj2+(1−α)|βj|) for α = 0.2 (right plot). Although visually very similar, the elastic-net has sharp (non-diﬀerentiable) corners, while
the q = 1.2 penalty does not.

setting coeﬃcients exactly to zero. Partly for this reason as well as for

computational tractability, Zou and Hastie (2005) introduced the elastic-

net penalty

p

λ αβj2 + (1 − α)|βj| ,

(3.54)

j=1

a diﬀerent compromise between ridge and lasso. Figure 3.13 compares the Lq penalty with q = 1.2 and the elastic-net penalty with α = 0.2; it is hard to detect the diﬀerence by eye. The elastic-net selects variables like the lasso, and shrinks together the coeﬃcients of correlated predictors like ridge. It also has considerable computational advantages over the Lq penalties. We discuss the elastic-net further in Section 18.4.

3.4.4 Least Angle Regression
Least angle regression (LAR) is a relative newcomer (Efron et al., 2004), and can be viewed as a kind of “democratic” version of forward stepwise regression (Section 3.3.2). As we will see, LAR is intimately connected with the lasso, and in fact provides an extremely eﬃcient algorithm for computing the entire lasso path as in Figure 3.10.
Forward stepwise regression builds a model sequentially, adding one variable at a time. At each step, it identiﬁes the best variable to include in the active set, and then updates the least squares ﬁt to include all the active variables.
Least angle regression uses a similar strategy, but only enters “as much” of a predictor as it deserves. At the ﬁrst step it identiﬁes the variable most correlated with the response. Rather than ﬁt this variable completely, LAR moves the coeﬃcient of this variable continuously toward its leastsquares value (causing its correlation with the evolving residual to decrease in absolute value). As soon as another variable “catches up” in terms of correlation with the residual, the process is paused. The second variable then joins the active set, and their coeﬃcients are moved together in a way that keeps their correlations tied and decreasing. This process is continued

74 3. Linear Methods for Regression
until all the variables are in the model, and ends at the full least-squares ﬁt. Algorithm 3.2 provides the details. The termination condition in step 5 requires some explanation. If p > N − 1, the LAR algorithm reaches a zero residual solution after N − 1 steps (the −1 is because we have centered the data).
Algorithm 3.2 Least Angle Regression.
1. Standardize the predictors to have mean zero and unit norm. Start with the residual r = y − y¯, β1, β2, . . . , βp = 0.
2. Find the predictor xj most correlated with r.
3. Move βj from 0 towards its least-squares coeﬃcient xj, r , until some other competitor xk has as much correlation with the current residual as does xj.
4. Move βj and βk in the direction deﬁned by their joint least squares coeﬃcient of the current residual on (xj, xk), until some other competitor xl has as much correlation with the current residual.
5. Continue in this way until all p predictors have been entered. After min(N − 1, p) steps, we arrive at the full least-squares solution.

Suppose Ak is the active set of variables at the beginning of the kth step, and let βAk be the coeﬃcient vector for these variables at this step; there will be k − 1 nonzero values, and the one just entered will be zero. If
rk = y − XAk βAk is the current residual, then the direction for this step is

δk = (XTAk XAk )−1XTAk rk.

(3.55)

The coeﬃcient proﬁle then evolves as βAk (α) = βAk + α · δk. Exercise 3.23 veriﬁes that the directions chosen in this fashion do what is claimed: keep

the correlations tied and decreasing. If the ﬁt vector at the beginning of this step is ˆfk, then it evolves as ˆfk(α) = ˆfk + α · uk, where uk = XAk δk is the new ﬁt direction. The name “least angle” arises from a geometrical

interpretation of this process; uk makes the smallest (and equal) angle with each of the predictors in Ak (Exercise 3.24). Figure 3.14 shows the absolute correlations decreasing and joining ranks with each step of the

LAR algorithm, using simulated data.

By construction the coeﬃcients in LAR change in a piecewise linear fash-

ion. Figure 3.15 [left panel] shows the LAR coeﬃcient proﬁle evolving as a function of their L1 arc length 2. Note that we do not need to take small

2The L1 arc-length of a diﬀerentiable curve β(s) for s ∈ [0, S] is given by TV(β, S) =

S 0

||β˙ (s)||1 ds,

where

β˙ (s)

=

∂β(s)/∂s.

For

the

piecewise-linear

LAR

coeﬃcient

proﬁle,

this amounts to summing the L1 norms of the changes in coeﬃcients from step to step.

v2 v6

3.4 Shrinkage Methods 75

v4

v5

v3

v1

0.4

0.3

0.2

Absolute Correlations

0.1

0.0

0

5

10

15

L1 Arc Length

FIGURE 3.14. Progression of the absolute correlations during each step of the LAR procedure, using a simulated data set with six predictors. The labels at the top of the plot indicate which variables enter the active set at each step. The step length are measured in units of L1 arc length.

Least Angle Regression

Lasso

Coeﬃcients
−1.5 −1.0 −0.5 0.0 0.5
Coeﬃcients
−1.5 −1.0 −0.5 0.0 0.5

0

5

10

15

L1 Arc Length

0

5

10

15

L1 Arc Length

FIGURE 3.15. Left panel shows the LAR coeﬃcient proﬁles on the simulated data, as a function of the L1 arc length. The right panel shows the Lasso proﬁle. They are identical until the dark-blue coeﬃcient crosses zero at an arc length of about 18.

76 3. Linear Methods for Regression
steps and recheck the correlations in step 3; using knowledge of the covariance of the predictors and the piecewise linearity of the algorithm, we can work out the exact step length at the beginning of each step (Exercise 3.25).
The right panel of Figure 3.15 shows the lasso coeﬃcient proﬁles on the same data. They are almost identical to those in the left panel, and diﬀer for the ﬁrst time when the blue coeﬃcient passes back through zero. For the prostate data, the LAR coeﬃcient proﬁle turns out to be identical to the lasso proﬁle in Figure 3.10, which never crosses zero. These observations lead to a simple modiﬁcation of the LAR algorithm that gives the entire lasso path, which is also piecewise-linear.
Algorithm 3.2a Least Angle Regression: Lasso Modiﬁcation.
4a. If a non-zero coeﬃcient hits zero, drop its variable from the active set of variables and recompute the current joint least squares direction.

The LAR(lasso) algorithm is extremely eﬃcient, requiring the same order of computation as that of a single least squares ﬁt using the p predictors. Least angle regression always takes p steps to get to the full least squares estimates. The lasso path can have more than p steps, although the two are often quite similar. Algorithm 3.2 with the lasso modiﬁcation 3.2a is an eﬃcient way of computing the solution to any lasso problem, especially when p ≫ N . Osborne et al. (2000a) also discovered a piecewise-linear path for computing the lasso, which they called a homotopy algorithm.
We now give a heuristic argument for why these procedures are so similar. Although the LAR algorithm is stated in terms of correlations, if the input features are standardized, it is equivalent and easier to work with innerproducts. Suppose A is the active set of variables at some stage in the algorithm, tied in their absolute inner-product with the current residuals y − Xβ. We can express this as

xTj (y − Xβ) = γ · sj, ∀j ∈ A

(3.56)

where sj ∈ {−1, 1} indicates the sign of the inner-product, and γ is the common value. Also |xTk (y − Xβ)| ≤ γ ∀k ∈ A. Now consider the lasso criterion (3.52), which we write in vector form

R(β)

=

1 2

||y

−

Xβ||22

+

λ||β||1.

(3.57)

Let B be the active set of variables in the solution for a given value of λ.

For these variables R(β) is diﬀerentiable, and the stationarity conditions

give

xTj (y − Xβ) = λ · sign(βj), ∀j ∈ B

(3.58)

Comparing (3.58) with (3.56), we see that they are identical only if the sign of βj matches the sign of the inner product. That is why the LAR

3.4 Shrinkage Methods 77

algorithm and lasso start to diﬀer when an active coeﬃcient passes through zero; condition (3.58) is violated for that variable, and it is kicked out of the active set B. Exercise 3.23 shows that these equations imply a piecewiselinear coeﬃcient proﬁle as λ decreases. The stationarity conditions for the non-active variables require that

|xTk (y − Xβ)| ≤ λ, ∀k ∈ B,

(3.59)

which again agrees with the LAR algorithm. Figure 3.16 compares LAR and lasso to forward stepwise and stagewise
regression. The setup is the same as in Figure 3.6 on page 59, except here N = 100 here rather than 300, so the problem is more diﬃcult. We see that the more aggressive forward stepwise starts to overﬁt quite early (well before the 10 true variables can enter the model), and ultimately performs worse than the slower forward stagewise regression. The behavior of LAR and lasso is similar to that of forward stagewise regression. Incremental forward stagewise is similar to LAR and lasso, and is described in Section 3.8.1.

Degrees-of-Freedom Formula for LAR and Lasso
Suppose that we ﬁt a linear model via the least angle regression procedure, stopping at some number of steps k < p, or equivalently using a lasso bound t that produces a constrained version of the full least squares ﬁt. How many parameters, or “degrees of freedom” have we used?
Consider ﬁrst a linear regression using a subset of k features. If this subset is prespeciﬁed in advance without reference to the training data, then the degrees of freedom used in the ﬁtted model is deﬁned to be k. Indeed, in classical statistics, the number of linearly independent parameters is what is meant by “degrees of freedom.” Alternatively, suppose that we carry out a best subset selection to determine the “optimal” set of k predictors. Then the resulting model has k parameters, but in some sense we have used up more than k degrees of freedom.
We need a more general deﬁnition for the eﬀective degrees of freedom of an adaptively ﬁtted model. We deﬁne the degrees of freedom of the ﬁtted vector yˆ = (yˆ1, yˆ2, . . . , yˆN ) as

df (yˆ )

=

1 σ2

N

Cov(yˆi, yi).

i=1

(3.60)

Here Cov(yˆi, yi) refers to the sampling covariance between the predicted value yˆi and its corresponding outcome value yi. This makes intuitive sense: the harder that we ﬁt to the data, the larger this covariance and hence df(yˆ). Expression (3.60) is a useful notion of degrees of freedom, one that can be applied to any model prediction yˆ. This includes models that are

78 3. Linear Methods for Regression
Forward Stepwise LAR Lasso Forward Stagewise Incremental Forward Stagewise

0.65

E||βˆ(k) − β||2

0.60

0.55

0.0

0.2

0.4

0.6

0.8

1.0

Fraction of L1 arc-length

FIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward stagewise (FS) and incremental forward stagewise (FS0) regression. The setup is the same as in Figure 3.6, except N = 100 here rather than 300. Here the slower FS regression ultimately outperforms forward stepwise. LAR and lasso show similar behavior to FS and FS0. Since the procedures take diﬀerent numbers of steps (across simulation replicates and methods), we plot the MSE as a function of the fraction of total L1 arc-length toward the least-squares ﬁt.

adaptively ﬁtted to the training data. This deﬁnition is motivated and discussed further in Sections 7.4–7.6.
Now for a linear regression with k ﬁxed predictors, it is easy to show that df(yˆ) = k. Likewise for ridge regression, this deﬁnition leads to the closed-form expression (3.50) on page 68: df(yˆ) = tr(Sλ). In both these cases, (3.60) is simple to evaluate because the ﬁt yˆ = Hλy is linear in y. If we think about deﬁnition (3.60) in the context of a best subset selection of size k, it seems clear that df(yˆ) will be larger than k, and this can be veriﬁed by estimating Cov(yˆi, yi)/σ2 directly by simulation. However there is no closed form method for estimating df(yˆ) for best subset selection.
For LAR and lasso, something magical happens. These techniques are adaptive in a smoother way than best subset selection, and hence estimation of degrees of freedom is more tractable. Speciﬁcally it can be shown that after the kth step of the LAR procedure, the eﬀective degrees of freedom of the ﬁt vector is exactly k. Now for the lasso, the (modiﬁed) LAR procedure

3.5 Methods Using Derived Input Directions 79
often takes more than p steps, since predictors can drop out. Hence the deﬁnition is a little diﬀerent; for the lasso, at any stage df(yˆ) approximately equals the number of predictors in the model. While this approximation works reasonably well anywhere in the lasso path, for each k it works best at the last model in the sequence that contains k predictors. A detailed study of the degrees of freedom for the lasso may be found in Zou et al. (2007).

3.5 Methods Using Derived Input Directions
In many situations we have a large number of inputs, often very correlated. The methods in this section produce a small number of linear combinations Zm, m = 1, . . . , M of the original inputs Xj, and the Zm are then used in place of the Xj as inputs in the regression. The methods diﬀer in how the linear combinations are constructed.

3.5.1 Principal Components Regression

In this approach the linear combinations Zm used are the principal components as deﬁned in Section 3.4.1 above.
Principal component regression forms the derived input columns zm = Xvm, and then regresses y on z1, z2, . . . , zM for some M ≤ p. Since the zm are orthogonal, this regression is just a sum of univariate regressions:

M

yˆ(pMcr) = y¯1 +

θˆmzm,

m=1

(3.61)

where θˆm = zm, y / zm, zm . Since the zm are each linear combinations of the original xj, we can express the solution (3.61) in terms of coeﬃcients of the xj (Exercise 3.13):

M

βˆpcr(M ) =

θˆmvm.

m=1

(3.62)

As with ridge regression, principal components depend on the scaling of the inputs, so typically we ﬁrst standardize them. Note that if M = p, we would just get back the usual least squares estimates, since the columns of Z = UD span the column space of X. For M < p we get a reduced regression. We see that principal components regression is very similar to ridge regression: both operate via the principal components of the input matrix. Ridge regression shrinks the coeﬃcients of the principal components (Figure 3.17), shrinking more depending on the size of the corresponding eigenvalue; principal components regression discards the p − M smallest eigenvalue components. Figure 3.17 illustrates this.

Shrinkage Factor 0.0 0.2 0.4 0.6 0.8 1.0

80 3. Linear Methods for Regression

•

•

•

•

•

•

•

•

• •

•

ridge pcr

•

•

•

•

•

•

2

4

6

8

Index

FIGURE 3.17. Ridge regression shrinks the regression coeﬃcients of the principal components, using shrinkage factors d2j /(d2j + λ) as in (3.47). Principal component regression truncates them. Shown are the shrinkage and truncation
patterns corresponding to Figure 3.7, as a function of the principal component
index.

In Figure 3.7 we see that cross-validation suggests seven terms; the resulting model has the lowest test error in Table 3.3.

3.5.2 Partial Least Squares
This technique also constructs a set of linear combinations of the inputs for regression, but unlike principal components regression it uses y (in addition to X) for this construction. Like principal component regression, partial least squares (PLS) is not scale invariant, so we assume that each xj is standardized to have mean 0 and variance 1. PLS begins by computing ϕˆ1j = xj, y for each j. From this we construct the derived input z1 = j ϕˆ1jxj, which is the ﬁrst partial least squares direction. Hence in the construction of each zm, the inputs are weighted by the strength of their univariate eﬀect on y3. The outcome y is regressed on z1 giving coeﬃcient θˆ1, and then we orthogonalize x1, . . . , xp with respect to z1. We continue this process, until M ≤ p directions have been obtained. In this manner, partial least squares produces a sequence of derived, orthogonal inputs or directions z1, z2, . . . , zM . As with principal-component regression, if we were to construct all M = p directions, we would get back a solution equivalent to the usual least squares estimates; using M < p directions produces a reduced regression. The procedure is described fully in Algorithm 3.3.
3Since the xj are standardized, the ﬁrst directions ϕˆ1j are the univariate regression coeﬃcients (up to an irrelevant constant); this is not the case for subsequent directions.

3.5 Methods Using Derived Input Directions 81

Algorithm 3.3 Partial Least Squares.

1. Standardize each xj to have mean zero and variance one. Set yˆ(0) = y¯1, and x(j0) = xj, j = 1, . . . , p.

2. For m = 1, 2, . . . , p

(a) zm =

p j=1

ϕˆmj

x(jm−1)

,

where

ϕˆmj

=

x(jm−1), y .

(b) θˆm = zm, y / zm, zm .

(c) yˆ(m) = yˆ(m−1) + θˆmzm.

(d) Orthogonalize each x(jm−1) with respect to zm: xj(m) = x(jm−1) − [ zm, x(jm−1) / zm, zm ]zm, j = 1, 2, . . . , p.

3. Output the sequence of ﬁtted vectors {yˆ(m)}p1. Since the {zℓ}m 1 are linear in the original xj, so is yˆ(m) = Xβˆpls(m). These linear coeﬃcients can be recovered from the sequence of PLS transformations.

In the prostate cancer example, cross-validation chose M = 2 PLS directions in Figure 3.7. This produced the model given in the rightmost column of Table 3.3.
What optimization problem is partial least squares solving? Since it uses the response y to construct its directions, its solution path is a nonlinear function of y. It can be shown (Exercise 3.15) that partial least squares seeks directions that have high variance and have high correlation with the response, in contrast to principal components regression which keys only on high variance (Stone and Brooks, 1990; Frank and Friedman, 1993). In particular, the mth principal component direction vm solves:

maxα Var(Xα) subject to ||α|| = 1, αT Svℓ = 0, ℓ = 1, . . . , m − 1,

(3.63)

where S is the sample covariance matrix of the xj. The conditions αT Svℓ = 0 ensures that zm = Xα is uncorrelated with all the previous linear combinations zℓ = Xvℓ. The mth PLS direction ϕˆm solves:

maxα Corr2(y, Xα)Var(Xα) subject to ||α|| = 1, αT Sϕˆℓ = 0, ℓ = 1, . . . , m − 1.

(3.64)

Further analysis reveals that the variance aspect tends to dominate, and so partial least squares behaves much like ridge regression and principal components regression. We discuss this further in the next section.
If the input matrix X is orthogonal, then partial least squares ﬁnds the least squares estimates after m = 1 steps. Subsequent steps have no eﬀect

