Linear algebra explained in four pages
Excerpt from the NO BULLSHIT GUIDE TO LINEAR ALGEBRA by Ivan Savov

Abstract—This document will review the fundamental ideas of linear algebra. We will learn about matrices, matrix operations, linear transformations and discuss both the theoretical and computational aspects of linear algebra. The tools of linear algebra open the gateway to the study of more advanced mathematics. A lot of knowledge buzz awaits you if you choose to follow the path of understanding, instead of trying to memorize a bunch of formulas.

I. INTRODUCTION

Linear algebra is the math of vectors and matrices. Let n be a positive

integer

and

let

R

denote

the

set

of

real

numbers,

then

n
R

is

the

set

of

all

n-tuples of real numbers. A vector v ∈ Rn is an n-tuple of real numbers.

The notation “∈ S” is read “element of S.” For example, consider a vector

that has three components:

v = (v1, v2, v3)

∈

(R,

R,

R)

≡

3
R

.

A matrix A ∈ Rm×n is a rectangular array of real numbers with m rows and n columns. For example, a 3 × 2 matrix looks like this:

 a11 A =  a21
a31

a12 

 R

a22  ∈  R

a32

R



R

R



≡

3×2
R

.

R

The purpose of this document is to introduce you to the mathematical operations that we can perform on vectors and matrices and to give you a feel of the power of linear algebra. Many problems in science, business, and technology can be described in terms of vectors and matrices so it is important that you understand how to work with these.

Prerequisites

The only prerequisite for this tutorial is a basic understanding of high school

math concepts1 like numbers, variables, equations, and the fundamental

arithmetic operations on real numbers: addition (denoted +), subtraction

(denoted −), multiplication (denoted implicitly), and division (fractions).

You should also be familiar with functions that take real numbers as

inputs and give real numbers as outputs, f : R → R. Recall that, by deﬁnition, the inverse function f −1 undoes the effect of f . If you are

given f (x) and you want to ﬁnd x, you can use the inverse function as

follows: f −1 (f (x)) = x. For inverse f −1(x) = ex, and the

einxvaemrspeleo, fthge(xfu)n=cti√onxfi(sxg)−=1(lxn)(x=)

has x2.

the

II. DEFINITIONS
A. Vector operations
We now deﬁne the math operations for vectors. The operations we can perform on vectors u = (u1, u2, u3) and v = (v1, v2, v3) are: addition, subtraction, scaling, norm (length), dot product, and cross product:

u + v = (u1 + v1, u2 + v2, u3 + v3)
u − v = (u1 − v1, u2 − v2, u3 − v3)
αu = (αu1, αu2, αu3)
||u|| = u21 + u22 + u23
u · v = u1v1 + u2v2 + u3v3
u × v = (u2v3 − u3v2, u3v1 − u1v3, u1v2 − u2v1)
The dot product and the cross product of two vectors can also be described in terms of the angle θ between the two vectors. The formula for the dot product of the vectors is u · v = u v cos θ. We say two vectors u and v are orthogonal if the angle between them is 90◦. The dot product of orthogonal vectors is zero: u · v = u v cos(90◦) = 0.
The norm of the cross product is given by u × v = u v sin θ. The cross product is not commutative: u × v = v × u, in fact u × v = −v × u.

B. Matrix operations
We denote by A the matrix as a whole and refer to its entries as aij. The mathematical operations deﬁned for matrices are the following:
• addition (denoted +)

C =A+B

⇔

cij = aij + bij .

• subtraction (the inverse of addition)

• matrix product. The product of matrices A ∈ Rm×n and B ∈ Rn×

is

another

matrix

C

∈

m×
R

given by the formula

a11 a21
a31

a12 a22 a32

C = AB
b11 b12 b21 b22

n

⇔

cij = aikbkj ,

k=1

a11b11 + a12b21 = a21b11 + a22b21
a31b11 + a32b21

a11b12 + a12b22 a21b12 + a22b22 a31b12 + a32b22

• matrix inverse (denoted A−1) • matrix transpose (denoted T):

α1 β1

α2 β2

α3 β3

T α1 = α2 α3

β1 β2 . β3

• matrix trace: Tr[A] ≡

n i=1

aii

• determinant (denoted det(A) or |A|)

Note that the matrix product is not a commutative operation: AB = BA.

C. Matrix-vector product

The matrix-vector product is an important special case of the matrixmatrix product. The product of a 3 × 2 matrix A and the 2 × 1 column vector x results in a 3 × 1 vector y given by:

y = Ax

⇔

y1 a11

y2=a21

y3

a31

a12 a22 a32

x1 x2

a11x1 + a12x2 = a21x1 + a22x2
a31x1 + a32x2

a11 a12

= x1a21+x2a22

(C)

a31

a32

(a11, a12) · x

= (a21, a22) · x.

(R)

(a31, a32) · x

There are two2 fundamentally different yet equivalent ways to interpret the matrix-vector product. In the column picture, (C), the multiplication of the matrix A by the vector x produces a linear combination of the columns of the matrix: y = Ax = x1A[:,1] + x2A[:,2], where A[:,1] and A[:,2] are the ﬁrst and second columns of the matrix A.
In the row picture, (R), multiplication of the matrix A by the vector x produces a column vector with coefﬁcients equal to the dot products of rows of the matrix with the vector x.

D. Linear transformations

The matrix-vector product is used to deﬁne the notion of a linear
transformation, which is one of the key notions in the study of linear algebra. Multiplication by a matrix A ∈ Rm×n can be thought of as computing a linear transformation TA that takes n-vectors as inputs and produces m-vectors as outputs:

TA

:

n
R

→

m
R

.

1A good textbook to (re)learn high school math is minireference.com

2For more info see the video of Prof. Strang’s MIT lecture: bit.ly/10vmKcL 1

Instead of writing y = TA(x) for the linear transformation TA applied to the vector x, we simply write y = Ax. Applying the linear transformation TA to the vector x corresponds to the product of the matrix A and the column vector x. We say TA is represented by the matrix A.
You can think of linear transformations as “vector functions” and describe their properties in analogy with the regular functions you are familiar with:

function f : R → R ⇔ linear transformation TA : Rn → Rm

input x ∈ R

⇔

input

x

∈

n
R

output f (x)

⇔

output

TA(x)

=

Ax

∈

m
R

g ◦ f = g(f (x)) ⇔ TB(TA(x)) = BAx function inverse f −1 ⇔ matrix inverse A−1
zeros of f ⇔ N (A) ≡ null space of A range of f ⇔ C(A) ≡ column space of A = range of TA

Note that the combined effect of applying the transformation TA followed by TB on the input vector x is equivalent to the matrix product BAx.

E. Fundamental vector spaces

A vector space consists of a set of vectors and all linear combinations of these vectors. For example the vector space S = span{v1, v2} consists of all vectors of the form v = αv1 + βv2, where α and β are real numbers. We now deﬁne three fundamental vector spaces associated with a matrix A.
The column space of a matrix A is the set of vectors that can be produced as linear combinations of the columns of the matrix A:

C(A)

≡

{y

∈

m
R

|

y

=

Ax

for

some

x

∈

n
R

}

.

The column space is the range of the linear transformation TA (the set of possible outputs). You can convince yourself of this fact by reviewing the deﬁnition of the matrix-vector product in the column picture (C). The vector Ax contains x1 times the 1st column of A, x2 times the 2nd column of A, etc. Varying over all possible inputs x, we obtain all possible linear combinations of the columns of A, hence the name “column space.”
The null space N (A) of a matrix A ∈ Rm×n consists of all the vectors that the matrix A sends to the zero vector:

N (A) ≡

x

∈

n
R

|

Ax

=

0

.

The vectors in the null space are orthogonal to all the rows of the matrix. We can see this from the row picture (R): the output vectors is 0 if and only if the input vector x is orthogonal to all the rows of A.
The row space of a matrix A, denoted R(A), is the set of linear combinations of the rows of A. The row space R(A) is the orthogonal complement of the null space N (A). This means that for all vectors v ∈ R(A) and all vectors w ∈ N (A), we have v · w = 0. Together, the null space and the row space form the domain of the transformation TA, Rn = N (A) ⊕ R(A), where ⊕ stands for orthogonal direct sum.

F. Matrix inverse
By deﬁnition, the inverse matrix A−1 undoes the effects of the matrix A.
The cumulative effect of applying A−1 after A is the identity matrix 1:

1

0

A−1A = 1 ≡  . . .  .

0

1

The identity matrix (ones on the diagonal and zeros everywhere else)
corresponds to the identity transformation: T1(x) = 1x = x, for all x.
The matrix inverse is useful for solving matrix equations. Whenever we
want to get rid of the matrix A in some matrix equation, we can “hit” A with its inverse A−1 to make it disappear. For example, to solve for the
matrix X in the equation XA = B, multiply both sides of the equation by A−1 from the right: X = BA−1. To solve for X in ABCXD = E, multiply both sides of the equation by D−1 on the right and by A−1, B−1 and C−1 (in that order) from the left: X = C−1B−1A−1ED−1.

2
III. COMPUTATIONAL LINEAR ALGEBRA Okay, I hear what you are saying “Dude, enough with the theory talk, let’s see some calculations.” In this section we’ll look at one of the fundamental algorithms of linear algebra called Gauss–Jordan elimination.

A. Solving systems of equations

Suppose we’re asked to solve the following system of equations:

1x1 + 2x2 = 5, (1)
3x1 + 9x2 = 21.

Without a knowledge of linear algebra, we could use substitution, elimination, or subtraction to ﬁnd the values of the two unknowns x1 and x2.
Gauss–Jordan elimination is a systematic procedure for solving systems of equations based the following row operations:
α) Adding a multiple of one row to another row β) Swapping two rows γ) Multiplying a row by a constant
These row operations allow us to simplify the system of equations without changing their solution.
To illustrate the Gauss–Jordan elimination procedure, we’ll now show the sequence of row operations required to solve the system of linear equations described above. We start by constructing an augmented matrix as follows:

12 39

5 21

.

The ﬁrst column in the augmented matrix corresponds to the coefﬁcients of the variable x1, the second column corresponds to the coefﬁcients of x2, and the third column contains the constants from the right-hand side.
The Gauss-Jordan elimination procedure consists of two phases. During the ﬁrst phase, we proceed left-to-right by choosing a row with a leading one in the leftmost column (called a pivot) and systematically subtracting that row from all rows below it to get zeros below in the entire column. In the second phase, we start with the rightmost pivot and use it to eliminate all the numbers above it in the same column. Let’s see this in action.

1) The ﬁrst step is to use the pivot in the ﬁrst column to eliminate the variable x1 in the second row. We do this by subtracting three times the ﬁrst row from the second row, denoted R2 ← R2 − 3R1,

12 03

5 6

.

2)

Next,

we

create

a

pivot

in

the

second

row

using

R2

←

1 3

R2

:

12 01

5 2

.

3) We now start the backward phase and eliminate the second variable

from the ﬁrst row. We do this by subtracting two times the second row from the ﬁrst row R1 ← R1 − 2R2:

10 01

1 2

.

The matrix is now in reduced row echelon form (RREF), which is its “simplest” form it could be in. The solutions are: x1 = 1, x2 = 2.

B. Systems of equations as matrix equations
We will now discuss another approach for solving the system of equations. Using the deﬁnition of the matrix-vector product, we can express this system of equations (1) as a matrix equation:

1 3

2 9

x1 x2

=

5 21

.

This matrix equation had the form Ax = b, where A is a 2 × 2 matrix, x
is the vector of unknowns, and b is a vector of constants. We can solve for x by multiplying both sides of the equation by the matrix inverse A−1:

A−1Ax = 1x =

x1 x2

= A−1b =

3 −1

−

2 3

1

3

5 21

=

1 2

.

But how did we know what the inverse matrix A−1 is?

IV. COMPUTING THE INVERSE OF A MATRIX
In this section we’ll look at several different approaches for computing the inverse of a matrix. The matrix inverse is unique so no matter which method we use to ﬁnd the inverse, we’ll always obtain the same answer.

A. Using row operations

One approach for computing the inverse is to use the Gauss–Jordan elimination procedure. Start by creating an array containing the entries of the matrix A on the left side and the identity matrix on the right side:

12 39

1 0

0 1

.

Now we perform the Gauss-Jordan elimination procedure on this array.

1) The ﬁrst row operation is to subtract three times the ﬁrst row from the second row: R2 ← R2 − 3R1. We obtain:

12 03

1 −3

0 1

.

2)

The second row operation is divide the second row by 3: R2 ←

1 3

R2

12 1 0

01

−1

1 3

.

3) The third row operation is R1 ← R1 − 2R2

10 01

3

−

2 3

−1

1 3

.

The array is now in reduced row echelon form (RREF). The inverse matrix

appears on the right side of the array.

Observe that the sequence of row operations we used to solve the speciﬁc

system of equations in Ax = b in the previous section are the same as the

row operations we used in this section to ﬁnd the inverse matrix. Indeed,

in both cases the combined effect of the three row operations is to “undo”

the effects of A. The right side of the 2 × 4 array is simply a convenient way to record this sequence of operations and thus obtain A−1.

B. Using elementary matrices
Every row operation we perform on a matrix is equivalent to a leftmultiplication by an elementary matrix. There are three types of elementary matrices in correspondence with the three types of row operations:

1m Rα : R1 ← R1 + mR2 ⇔ Eα = 0 1

Rβ : R1 ↔ R2

⇔

Eβ =

0 1

1 0

Rγ : R1 ← mR1

⇔

Eγ =

m 0

0 1

Let’s revisit the row operations we used to ﬁnd A−1 in the above section representing each row operation as an elementary matrix multiplication.
1) The ﬁrst row operation R2 ← R2 − 3R1 corresponds to a multiplication by the elementary matrix E1:

E1A =

1 −3

0 1

1 3

2 9

=

1 0

2 3

.

2)

The second row operation R2 ←

1 3

R2

corresponds

to

a

matrix

E2:

E2(E1A) =

1 0

0
1 3

1 0

2 3

=

1 0

2 1

.

3) The ﬁnal step, R1 ← R1 − 2R2, corresponds to the matrix E3:

E3(E2E1A) =

1 0

−2 1

1 0

2 1

=

1 0

0 1

.

Note that E3E2E1A = 1, so the product E3E2E1 must be equal to A−1:

A−1 = E3E2E1 =

1 0

−2 1

1 0

0
1 3

1 −3

0 1

=

3 −1

−

2 3

1

.

3

The elementary matrix approach teaches us that every invertible matrix

can be decomposed as the product of elementary matrices. Since we know A−1 = E3E2E1 then A = (A−1)−1 = (E3E2E1)−1 = E1−1E2−1E3−1.

3

C. Using a computer
The last (and most practical) approach for ﬁnding the inverse of a matrix is to use a computer algebra system like the one at live.sympy.org.

>>> A = Matrix( [[1,2],[3,9]] ) [1, 2] [3, 9]

# define A

>>> A.inv() [ 3, -2/3] [-1, 1/3]

# calls the inv method on A

You can use sympy to “check” your answers on homework problems.

V. OTHER TOPICS We’ll now discuss a number of other important topics of linear algebra.

A. Basis
Intuitively, a basis is any set of vectors that can be used as a coordinate system for a vector space. You are certainly familiar with the standard basis for the xy-plane that is made up of two orthogonal axes: the x-axis and the y-axis. A vector v can be described as a coordinate pair (vx, vy) with respect to these axes, or equivalently as v = vxˆı + vyˆ, where ˆı ≡ (1, 0) and ˆ ≡ (0, 1) are unit vectors that point along the x-axis and y-axis respectively. However, other coordinate systems are also possible.

Deﬁnition (Basis). A basis for a n-dimensional vector space S is any set of n linearly independent vectors that are part of S.

Any set of two linearly independent vectors {eˆ1, eˆ2} can serve as a basis for R2. We can write any vector v ∈ R2 as a linear combination of these basis vectors v = v1eˆ1 + v2eˆ2.
Note the same vector v corresponds to different coordinate pairs depending on the basis used: v = (vx, vy) in the standard basis Bs ≡ {ˆı, ˆ}, and v = (v1, v2) in the basis Be ≡ {eˆ1, eˆ2}. Therefore, it is important to keep in mind the basis with respect to which the coefﬁcients are taken, and if necessary specify the basis as a subscript, e.g., (vx, vy)Bs or (v1, v2)Be .
Converting a coordinate vector from the basis Be to the basis Bs is performed as a multiplication by a change of basis matrix:

v=

1

v

⇔

Bs

Bs

Be

Be

vx = ˆı · eˆ1 ˆı · eˆ2

vy

ˆ· eˆ1 ˆ· eˆ2

v1 . v2

Note the change of basis matrix is actually an identity transformation. The

vector v remains unchanged—it is simply expressed with respect to a new

coordinate system. The change of basis from the Bs-basis to the Be-basis
is accomplished using the inverse matrix: 1 Be [ ]Bs = (Bs [1]Be )−1.

B. Matrix representations of linear transformations

Bases play an important role in the representation of linear transformations T : Rn → Rm. To fully describe the matrix that corresponds to some
linear transformation T , it is sufﬁcient to know the effects of T to the n

vectors of the standard basis for the input space. For a linear transformation T : R2 → R2, the matrix representation corresponds to

|

|

MT = T (ˆı) T (ˆ) ∈ R2×2.

|

|

As a ﬁrst example, consider the transformation Πx which projects vectors onto the x-axis. For any vector v = (vx, vy), we have Πx(v) = (vx, 0). The matrix representation of Πx is

1 MΠx = Πx 0

0 Πx 1

=

1 0

0 0

.

As a second example, let’s ﬁnd the matrix representation of Rθ, the counterclockwise rotation by the angle θ:

MRθ =

Rθ

1 0

0 Rθ 1

=

cos θ sin θ

− sin θ cos θ

.

The ﬁrst column of MRθ shows that Rθ maps the vector ˆı ≡ 1∠0 to the

vector 1∠θ = (cos θ, sin θ)T. The second column shows that Rθ maps the

vector

ˆ

=

1∠

π 2

to

the

vector

1∠(

π 2

+ θ) = (− sin θ, cos θ)T.

C. Dimension and bases for vector spaces
The dimension of a vector space is deﬁned as the number of vectors in a basis for that vector space. Consider the following vector space S = span{(1, 0, 0), (0, 1, 0), (1, 1, 0)}. Seeing that the space is described by three vectors, we might think that S is 3-dimensional. This is not the case, however, since the three vectors are not linearly independent so they don’t form a basis for S. Two vectors are sufﬁcient to describe any vector in S; we can write S = span{(1, 0, 0), (0, 1, 0)}, and we see these two vectors are linearly independent so they form a basis and dim(S) = 2.
There is a general procedure for ﬁnding a basis for a vector space. Suppose you are given a description of a vector space in terms of m vectors V = span{v1, v2, . . . , vm} and you are asked to ﬁnd a basis for V and the dimension of V. To ﬁnd a basis for V, you must ﬁnd a set of linearly independent vectors that span V. We can use the Gauss–Jordan elimination procedure to accomplish this task. Write the vectors vi as the rows of a matrix M . The vector space V corresponds to the row space of the matrix M . Next, use row operations to ﬁnd the reduced row echelon form (RREF) of the matrix M . Since row operations do not change the row space of the matrix, the row space of reduced row echelon form of the matrix M is the same as the row space of the original set of vectors. The nonzero rows in the RREF of the matrix form a basis for vector space V and the numbers of nonzero rows is the dimension of V.

D. Row space, columns space, and rank of a matrix
Recall the fundamental vector spaces for matrices that we deﬁned in Section II-E: the column space C(A), the null space N (A), and the row space R(A). A standard linear algebra exam question is to give you a certain matrix A and ask you to ﬁnd the dimension and a basis for each of its fundamental spaces.
In the previous section we described a procedure based on Gauss–Jordan elimination which can be used “distill” a set of linearly independent vectors which form a basis for the row space R(A). We will now illustrate this procedure with an example, and also show how to use the RREF of the matrix A to ﬁnd bases for C(A) and N (A).
Consider the following matrix and its reduced row echelon form:

1 3 3 3  A = 2 6 7 6 
3 9 9 10

1 3 0 0 rref(A) = 0 0 1 0 .
0001

The reduced row echelon form of the matrix A contains three pivots. The
locations of the pivots will play an important role in the following steps.
The vectors {(1, 3, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1)} form a basis for R(A).
To ﬁnd a basis for the column space C(A) of the matrix A we need to ﬁnd which of the columns of A are linearly independent. We can
do this by identifying the columns which contain the leading ones in rref(A). The corresponding columns in the original matrix form a basis for the column space of A. Looking at rref(A) we see the ﬁrst, third,
and fourth columns of the matrix are linearly independent so the vectors {(1, 2, 3)T, (3, 7, 9)T, (3, 6, 10)T} form a basis for C(A).
Now let’s ﬁnd a basis for the null space, N (A) ≡ {x ∈ R4 | Ax = 0}. The second column does not contain a pivot, therefore it corresponds to a free variable, which we will denote s. We are looking for a vector with three unknowns and one free variable (x1, s, x3, x4)T that obeys the conditions:

1 0
0

3 0 0

0 1 0

0 0 1

x1

s

x3

 

x4

=

0 0
0

⇒

1x1 + 3s = 0 1x3 = 0 1x4 = 0

Let’s express the unknowns x1, x3, and x4 in terms of the free variable s. We immediately see that x3 = 0 and x4 = 0, and we can write x1 = −3s. Therefore, any vector of the form (−3s, s, 0, 0), for any s ∈ R, is in the null space of A. We write N (A) = span{(−3, 1, 0, 0)T}.
Observe that the dim(C(A)) = dim(R(A)) = 3, this is known as the rank of the matrix A. Also, dim(R(A)) + dim(N (A)) = 3 + 1 = 4, which is the dimension of the input space of the linear transformation TA.

4

E. Invertible matrix theorem
There is an important distinction between matrices that are invertible and those that are not as formalized by the following theorem.

Theorem. For an n × n matrix A, the following statements are equivalent:
1) A is invertible 2) The RREF of A is the n × n identity matrix 3) The rank of the matrix is n 4) The row space of A is Rn 5) The column space of A is Rn 6) A doesn’t have a null space (only the zero vector N (A) = {0}) 7) The determinant of A is nonzero det(A) = 0

For a given matrix A, the above statements are either all true or all false.
An invertible matrix A corresponds to a linear transformation TA which maps the n-dimensional input vector space Rn to the n-dimensional output vector space Rn such that there exists an inverse transformation TA−1 that can faithfully undo the effects of TA.
On the other hand, an n × n matrix B that is not invertible maps the input vector space Rn to a subspace C(B) Rn and has a nonempty null
space. Once TB sends a vector w ∈ N (B) to the zero vector, there is no TB−1 that can undo this operation.

F. Determinants

The determinant of a matrix, denoted det(A) or |A|, is a special way to combine the entries of a matrix that serves to check if a matrix is invertible or not. The determinant formulas for 2 × 2 and 3 × 3 matrices are

a11 a21

a12 a22

= a11a22 − a12a21,

and

a11 a21 a31

a12 a22 a32

a13 a23 a33

= a11

a22 a32

a23 a33

− a12

a21 a31

a23 a33

+ a13

a21 a31

a22 a32

.

If the |A| = 0 then A is not invertible. If |A| = 0 then A is invertible.

G. Eigenvalues and eigenvectors

The set of eigenvectors of a matrix is a special set of input vectors for which the action of the matrix is described as a simple scaling. When a matrix is multiplied by one of its eigenvectors the output is the same eigenvector multiplied by a constant Aeλ = λeλ. The constant λ is called an eigenvalue of A.
To ﬁnd the eigenvalues of a matrix we start from the eigenvalue equation
Aeλ = λeλ, insert the identity 1, and rewrite it as a null-space problem:

Aeλ = λ1eλ ⇒ (A − λ1) eλ = 0.

This equation will have a solution whenever |A−λ1| = 0. The eigenvalues
of A ∈ Rn×n, denoted {λ1, λ2, . . . , λn}, are the roots of the characteristic
polynomial p(λ) = |A − λ1|. The eigenvectors associated with the eigenvalue λi are the vectors in the null space of the matrix (A − λi1).
Certain matrices can be written entirely in terms of their eigenvectors
and their eigenvalues. Consider the matrix Λ that has the eigenvalues of
the matrix A on the diagonal, and the matrix Q constructed from the
eigenvectors of A as columns:

λ1 · · · 0 



|









Λ


=  

...

...









Q e ,


= 

λ1

0



···



e

λn

, 

then

A = QΛQ−1.











 0 0 λn



|

Matrices that can be written this way are called diagonalizable. The decomposition of a matrix into its eigenvalues and eigenvectors
gives valuable insights into the properties of the matrix. Google’s original PageRank algorithm for ranking webpages by “importance” can be formalized as an eigenvector calculation on the matrix of web hyperlinks.

VI. TEXTBOOK PLUG
If you’re interested in learning more about linear algebra, check out the NO BULLSHIT GUIDE TO LINEAR ALGEBRA. The book is available via lulu.com, amazon.com, and also here: gum.co/noBSLA .

