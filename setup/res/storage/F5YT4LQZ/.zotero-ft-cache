ETIENNE AMÉRICO CARTOLANO JÚNIOR
A model for trust under a suitcase word perspective
São Paulo (2017)

ETIENNE AMÉRICO CARTOLANO JÚNIOR
A model for trust under a suitcase word perspective
Thesis presented to Escola Politécnica da Universidade de São Paulo to obtain the title of Doctor of Sciences Supervisor: Antonio Mauro Saraiva Co-Supervisor: Robert David Stevenson
São Paulo (2017)

Este exemplar foi revisado e corrigido em relação à versão original, sob responsabilidade única do autor e com a anuência de seu orientador. São Paulo, ______ de ____________________ de __________ Assinatura do autor: ________________________ Assinatura do orientador: ________________________
Catalogação-na-publicação Cartolano Júnior, Etienne Américo
A model for trust under a suitcase word perspective / E. A. Cartolano Júnior -- versão corr. -- São Paulo, 2017.
139 p. Tese (Doutorado) - Escola Politécnica da Universidade de São Paulo. Departamento de Engenharia de Computação e Sistemas Digitais. 1.trust 2.trust model 3.suitcase word 4.simulation 5.behavioral decision I.Universidade de São Paulo. Escola Politécnica. Departamento de Engenharia de Computação e Sistemas Digitais II.t.

Dedicated to my beloved wife Flávia and to all who trust me

AGRADECIMENTOS
Agradeço ao meu orientador Prof. Antônio Mauro Saraiva por seu o meu mentor e amigo por tantos anos, desde os primeiros dias da minha graduação. Sob sua orientação e liderança tive muitas oportunidades para descobrir e crescer no mundo acadêmico e na minha vida pessoal.
Agradeço também ao Prof. Robert Stevenson por ter sido meu segundo orientador e um anfitrião paciente e interessado. Minha estadia em Boston não teria sido tão incrível, agradável e produtiva sem o seu suporte.
Além disso, agradeço ao Dr.(!) Allan Koch Veiga, sem quem eu não poderia andar na vida acadêmica. Pelo menos sem tropeçar! Certamente, ele é um dos grandes responsáveis pelo meu sucesso acadêmico.
Aos funcionários e professores da (gloriosa) Escola Politécnica da USP, agradeço por seu trabalho, conhecimento e paixão, que preservam a excelência e a tradição da nossa Escola.
Aos colegas do Itaú-Unibanco, agradeço pela paciência e incentivo durante esta jornada, especialmente, àqueles que, no dia-a-dia da nossa equipe, compartilharam os melhores e os piores momentos desta minha vida dupla.
Aos amigos de sempre que me ajudam a qualquer momento e por qualquer razão, e que contribuíram indiretamente com esse trabalho, e aos amigos de Boston, que fizeram daquela jornada algo tão especial.
Agradeço a minha família (Cartolano, Borges, e De Conti), para quem eu vivo, e que me faz quem sou. E aos meus futuros engenheiros: Bruno, Lucas, Melissa, Benjamin (Tet) e Felipe, digo: vão estudar!
E, de maneira especial, agradeço a quem tem estado ao meu lado a cada etapa da minha vida. Quem confia e faz de mim uma pessoa melhor todas as manhãs. Que vida sem amor eu teria sem a minha esposa Flávia ao meu lado!
O presente trabalho foi realizado com apoio do CNPq, Conselho Nacional de Desenvolvimento Científico e Tecnológico - Brasil

In God we trust (unknown)

ABSTRACT
Trust is a social phenomenon fundamental to relationships and a building block of our society. People experience it daily, such as in a borrowing between friends, in an e-commerce transaction, in a mother-son relationship, in a connection between autonomous agents, or to show faith in God (“In God we trust”). In the specific case of Biodiversity domain, trust is one of the pillars of the Citizen Science projects, which are helping to solve the lack of biodiversity data by engaging citizens to work as volunteers to address this problem. Measuring and simulating levels of trust on these projects might reveal or anticipate losses; for example, the disposal of data because a deficit of trust on the technical capacity of the volunteers, opening an opportunity to manage and improve it. However, trust is a hard concept to define. The word ‘trust’ may carry different meanings, such as honesty, security, integrity, competence, etc. and this is an attribute of the ‘suitcase words’. Adopting the ‘suitcase’ perspective would change the way as we define, model, and simulate trust, once people would identify, decode, and simulate many meanings of trust with a single approach. In this scenario, the main objective of our research was to verify the hypotheses 1) that trust is a suitcase word, and 2) that trust can be modeled and simulated under a suitcase word perspective. A network analysis of the Web of Science citation database was able to confirm the hypothesis that trust is a suitcase word, since a distribution analysis of articles showed that trust occurs across a wide range of disciplines, and since co-occurrence maps of keywords showed that trust meanings from these disciplines may be significantly different. To verify the second hypothesis, we proposed a framework to manage trust with three components: 1) a suitcase model to identify different meanings of trust, which is the main purpose of this work, 2) a procedure to detail trust situations in terms of the suitcase model, and 3) a behavioral decision model of confidence, which was required for our simulation, since trust and control play complementary roles in the development of confidence, and consequently, to generate a confident behavior to cooperate. In our suitcase model the decision to trust (or distrust) the trustee depends on the trustors’ general capacity to take risks (= trustfulness) and on the assessment of trustee’s interests and capacity to behavior as the trustor expects (= trustworthiness). In a practical and workable way, trustworthiness was considered a function of the trustor’s expectations (expected evidence) and the trustee’s previous

behavior (collected evidence) for each situation. We proposed a formalism to the suitcase model, and then replicated the PlayGround simulator to modify it and incorporate our model. The new simulator, the PlayGround 2.0, was used to run a case study using trust situations from Citizen Science projects. Our main goal with this case study was to test the hypothesis that trust can be simulated under a suitcase perspective. A successful simulation would plot agents in the field reacting differently according to each situation. Results were as expected, what demonstrated the comprehensive utility of our model, with potential to handle different meanings of trust in the context of Citizen Science in the Biodiversity domain.
Keywords: trust, trust model, suitcase word, simulation, behavioral decision.

RESUMO
Trust, que em português não possui uma tradução que reflita a sua complexidade, é um fenômeno fundamental para os relacionamentos e uma peça fundamental da nossa sociedade. Trust é vivenciada diariamente pelas pessoas, seja em um empréstimo entre amigos, em uma transação de comércio eletrônico, na relação entra uma mãe e um filho, na conexão entre agentes autônomos, ou para demonstrar a nossa fé. No caso específico da ciência da Biodiversidade, trust é um dos pilares dos projetos de Ciência Cidadã, que ajudam a resolver a falta de dados de biodiversidade engajando cidadãos para trabalhar como voluntários para solucionar o problema. Medir e simular trust nestes projetos pode revelar ou antecipar perdas, por exemplo, o descarte de dados devido à falta de trust na capacidade técnica dos voluntários, abrindo oportunidades para sua gestão e incentivo. Contudo, trust é um conceito difícil de definir. A palavra pode carregar muitos significados, tais como honestidade, segurança, integridade, competência, etc., e esta é uma característica das suitcase words (palavras-mala). Adotar uma perspectiva de suitcase pode mudar a maneira como definimos, modelamos e simulamos trust, pois as pessoas poderiam identificar, decodificar, e simular vários significados de trust com uma única abordagem. Neste cenário, o objetivo principal desta pesquisa foi verificar as hipóteses de que 1) trust é uma suitcase word e de que 2) trust pode ser modelada e simulada em uma perspectiva de suitcase com potencial para manipulação de diferentes significados. Uma análise de distribuição utilizando o banco de dados Web of Science foi suficiente para confirmar a primeira hipótese de que trust é uma suitcase word, pois uma análise dos seus registros de citações mostrou que trust é estudado por muitas disciplinas da ciência, e além disso, mapas de co-ocorrência de palavraschaves mostraram que os significados de trust nas disciplinas podem ser diferentes. Para verificar a segunda hipótese, nós propusemos um framework com três componentes: 1) um ‘modelo suitcase’ para identificar diferentes significados de trust, que é o objetivo principal deste trabalho, 2) um procedimento para detalhar as situações de trust em componentes do modelo suitcase, e 3) um modelo de decisão comportamental sobre confiança, que foi necessário para nossa simulação, uma vez que trust e controle tem papéis complementares no desenvolvimento da confiança, e consequentemente, na exibição de um comportamento de confiança que pode levar à

cooperação. No nosso modelo, a decisão de trust o trustee (aquele a quem trust é direcionada) depende da capacidade do trustor (aquele que direciona trust) em aceitar riscos (= trustfulness), e da avaliação do interesse e da capacidade do trustee em agir como o esperado (= trustworthiness). De uma maneira prática, trustworthiness foi considerada como uma função das evidências esperadas e coletadas em cada situação. Nós propusemos um formalismo para o novo modelo, e depois replicamos um simulador chamado PlayGround para depois modificá-lo e incorporar o modelo suitcase. O novo simulador, o PlayGround 2.0, foi utilizado para rodar um estudo de caso utilizando situações comuns em projetos de Citizen Science. Nosso maior objetivo foi testar a hipótese de que trust poderia ser simulada em uma perspectiva de suitcase. Uma simulação bem-sucedida plotaria os agentes em campo reagindo diferentemente de acordo com as situações apresentadas. Os resultados foram como esperados, o que demonstrou a utilidade abrangente do nosso modelo, com potencial para lidar com diferentes significados de trust no contexto dos projetos de Ciência Cidadã para Biodiversidade, ou em outros contextos.
Palavras-chave: trust, suitcase word, confiança, decisão comportamental, simulação.

LIST OF FIGURES
Figure 1 - Graphical representation of our research method ..................................................24 Figure 2 - Adopted hierarchy of categories from Web of Science...........................................30 Figure 3 - Distribution of articles among main areas of Science in Web of Science ..............30 Figure 4 - Co-occurrence of authors in the Psychology research area...................................33 Figure 5 - Psychology research area co-occurence map of keywords ...................................35 Figure 6 - Computer Science research area co-occurrence map of keywords.......................35 Figure 7 – Co-occurrence map of keywords from Computer Science, Psychology, Operation
Research Management Science, and Sociology .............................................................36 Figure 8 - Ngram distribution of types of trust..........................................................................43 Figure 9 – The trust continuum, extracted from (CHO; CHAN; ADALI, 2015) ........................44 Figure 10 - Confidence behavior model, extracted from (COFTA, 2007) ...............................50 Figure 11 - Basic building block for confidence, extracted from (COFTA, 2007)....................52 Figure 12 - Building blocks within a horizon of three entities, extracted from (COFTA, 2007)
.......................................................................................................................................... 53 Figure 13 – Proposed framework to define trust under a suitcase perspective ......................56 Figure 14 - Suitcase model components and their relations ...................................................57 Figure 15 - Confident behavior components (blue) and their relations with the suitcase model
of trust ...............................................................................................................................58 Figure 16 – ABX+C relation, an expanded three-part relation of trust ....................................59 Figure 17 - New basic building block for confidence, modified from (COFTA, 2007) .............61 Figure 18 - Alice's timeline of trust situations...........................................................................63 Figure 19 - Suitcase model version of the basic building block for confidence with
attenuators, modified from (COFTA, 2007)......................................................................65 Figure 20 - Graphical relation among the components of the suitcase model ........................67 Figure 21 - Alice's timeline of trust situations marked with the suitcase model formalism .....68 Figure 22 - Axelrod’s Tournament environment ......................................................................81 Figure 23 - A screenshot of the PlayGround, extracted from (MARSH, 1994) .......................82 Figure 24 - Initial agent's distribution in experiment F part I replication ..................................90 Figure 25 - Final agent's distribution in experiment F part I replication...................................90 Figure 26 - Percentage of successful cooperation in time for trusting agents ........................92 Figure 27 - PlayGround 2.0 screenshot ...................................................................................92 Figure 28 – Relation between entities in the ‘Suitcase Trustor’ strategy of the PlayGround
2.0. ....................................................................................................................................95 Figure 29 - Co-occurrence map for Operations & Management Science Research Area in

Web of Science .............................................................................................................. 129 Figure 30 - Co-occurrence map for Sociology Research Area in Web of Science .............. 130 Figure 31 - Co-occurrence map for Computer Science Information Systems Category in Web
of Science ...................................................................................................................... 131 Figure 32 - Co-occurrence map for Artificial Intelligence Category in Web of Science ....... 132 Figure 33 - Co-occurrence map for Hardware Category in Web of Science ........................ 133 Figure 34 - Co-occurrence map for Software Engineering Category in Web of Science..... 134 Figure 35 - Co-occurrence map for Theory Methods Category in Web of Science ............. 135

LIST OF ABBREVIATIONS AND ACRONYMS

ABX ABX+C
BT EGT GT IPD PD ST TDWG VOS WOS

Three-part relation definition for trust Modified three-part relation definition for trust Basic Trust Estimated General Trust General Trust Iterated Prisoner’s Dilemma Prisoner’s Dilemma Situational Trust Biodiversity Information Standards VOSviewer Thompson Reuter’s Web of Science

CONTENTS

1 1.1 1.2 1.3 1.4 1.5
2 2.1 2.2 2.3
3 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9
4 4.1 4.1.1 4.1.2 4.1.3 4.1.4 4.1.5 4.2 4.3 4.4

INTRODUCTION ......................................................................................... 21 Justification .................................................................................................. 21 Problem statement....................................................................................... 23 Objectives..................................................................................................... 23 Materials and methods ................................................................................ 24 Thesis structure............................................................................................ 27
SCIENTIFIC LITERATURE DATA MINING ................................................ 29 Distribution among areas of knowledge ...................................................... 29 Co-occurrence of keywords ......................................................................... 32 Considerations about the literature mining ................................................. 36
CONCEPTUAL FOUNDATIONS OF TRUST ............................................. 38 Many Definitions of Trust ............................................................................. 38 Actors, Risk & Context ................................................................................. 39 Situation, Belief, Probability, and Behavior Decision .................................. 40 Three-part relation ....................................................................................... 41 Group and Cooperation ............................................................................... 41 Types of Trust .............................................................................................. 42 Trust dynamics and measurement .............................................................. 43 Trustfulness and Trustworthiness ............................................................... 45 Models of Trust ............................................................................................ 46
MODELING TRUST UNDER A SUITCASE WORD PERSPECTIVE ........ 56 Suitcase model............................................................................................. 56 Trust belief ..............................................................................................................57 Trust and Confidence .............................................................................................57 Trust situation with a behavioral decision of confidence .......................................58 Trustworthiness computation .................................................................................60 Suitcase model formalization .................................................................................65 Defining a trust situation .............................................................................. 75 A generalized model of behavioral decision ............................................... 76 Considerations about modeling trust under a suitcase perspective........... 77

5 5.1 5.2 5.2.1 5.2.2 5.2.3 5.3 5.4

SIMULATING THE SUITCASE MODEL......................................................79 The Iterated Prisoner's Dilemma..................................................................79 Replicating the original PlayGround.............................................................81 The original PlayGround ........................................................................................ 81 Assumptions about original implementation.......................................................... 86 Replication results.................................................................................................. 87 PlayGround 2.0 - the suitcase version.........................................................92 Considerations about the model simulator ..................................................95

6

CASE STUDY IN CITIZEN SCIENCE .........................................................96

6.1

Scenario of Citizen Science in Biodiversity .................................................96

6.2

Defining the Citizen Science situations........................................................97

6.3

Reaction under different situations ........................................................... 101

6.4

Considerations about the case study........................................................ 105

7

DISCUSSION ............................................................................................ 108

8

FINAL REMARKS...................................................................................... 110

8.1

Conclusions ............................................................................................... 110

8.2

Future work ................................................................................................ 111

BIBLIOGRAPHY......................................................................................................... 113

A.

EXTRACTION OF ARTICLES RECORDS FROM WEB OF SCIENCE .. 117

B.

HIERARCHY RELATION AMONG WEB OF SCIENCE CATEGORIES . 119

C.

VOSVIEWER CONFIGURATION ............................................................. 127

D.

MAPS OF CO-OCCURRENCE................................................................. 129

E.

RESULTS OF MARSH’S PLAYGROUND................................................ 136

21
1 Introduction
This chapter presents the research justification, problem statement, specific objectives, methods, and the structure of this document.
1.1 Justification
Citizen Science projects engage volunteers to collect and/or process data as part of scientific enquiries (SILVERTOWN, 2009) with diverse purposes, such as water pollution control, birds migration observation, discover of new celestial corps, or social experimentations. Specially in the Biodiversity domain, in the last decade, citizen science projects are helping to solve the issue of lack of data to face problems such as invasive species, habitat degradation, managing ecosystem services, and climate change (BISBY, 2000).
Large scale citizen science projects such as eBird (E-BIRD, 2016), iNaturalist (INATURALIST, 2016), iSpot (ISPOT, 2016), and REEF (REEF, 2016) have rapidly increased data collection because of their ability to recruit large numbers of participants and because people volunteer their time. However, citizens may lack the training about how to make observations and/or deal with situations in which they are uncertain about observations (ALABRI; HUNTER, 2010) (WIGGINS; CROWSTON, 2011) and doubts remain about the quality of citizen science data (SOURCE, 2008) (WIGGINS et al., 2011) (GALLOWAY; TUDOR; HAEGEN, 2006).
A central question then becomes “How can project leaders and users have trust in the data collected by citizens?”. Whether or not to have trust in citizen science data is one of the many complex trust behavioral choices experimented in Citizen Science projects for biodiversity.
In fact, trust is an inherent building block of our society (FUKUYAMA, 1996) and it is experienced daily by people (BAIER, 1986). Trust is fundamental to social, interpersonal, and commercial relationships (BAUER, 2015; CHO; CHAN; ADALI, 2015; HARDIN, 2002; MCKNIGHT; CHERVANY, 2001). Societies in which members commonly cheat others and take advantage of them, a system of formal rules and regulations have to be negotiated, agreed to, litigated, and enforced, sometimes by

22
coercive means. This legal apparatus, serving as a substitute for trust, entails what economists call “transaction costs” (FUKUYAMA, 1996). These costs impact directly the economy by making business difficult to realize, personified in contracts, lawyers, certificates, and other bureaucratic items.
The buyer-seller relationship is a practical example of the economic impact of trust. Hopping to have a product or service of quality, replacement in case of failure, and a fair price, most part of customers rely in trust to buy their products or services, so that companies constantly measure trust in its brands as a portrait of customer loyalty. For instance, on Internet marketplace the brand or reputation (a kind of personal brand) are fundamental, once buyers sometimes do not have direct contact or do not know the sellers.
Generally speaking, we trust when engaging in a (social, interpersonal, economic) relationship, and there is always an associated risk that others (people, computers, societies, governments) may not behave according to our expectations (HARDIN, 2002). If there is no risk, then there is no reason to trust or distrust, because others will meet our expectations.
It is possible to use instruments such as contracts, laws, inspections, and social rules to mitigate risks, and increase the confidence to engage as a trustor in a trust situation. As the Russian proverb, made famous by Ronald Reagan, says, “Trust, but verify” (WIKIPEDIA, 2016). Trust and control (or verification) play complimentary roles in the basis for confidence in relationships (COFTA, 2007). However, we cannot have a relationship with 100% of control. We are not capable to control or verify all risks, so that we as humans would not even be able to face the complexities of the world without resorting to trust (LUHMANN, 1979).
Once trust supports, together with control, our behavioral decision to engage in a relationship, such as volunteers in a Citizen Science project, it is also important to measure it and cultivate it, as we do with control. Measuring levels of trust can reveal or anticipate losses of loyalty, customers, employees, voters, partners, friends or volunteers. Low levels of trust can be changed, and improved trust might well reduce disparities, increase access, improve outcomes (THOM; HALL; PAWLSON, 2004), or improve the use of citizen science data.

23
1.2 Problem statement
Although trust is everywhere in our daily lives, even in Citizen Science projects the word “trust” carries different meanings such as: honesty, security, quality, caring, integrity, benevolence, etc. that is context dependent. This variety of meanings invoked by “trust” may simplify our daily communication, so that people or machines do not have to explain whom they are or what they are saying every time; however, this variety also makes “trust” a complex concept to be defined by people and by science. Both tend to define trust according to their particular or momentary use of the word, or they tend to establish general definitions that leave aside uncountable meanings of the word.
Words that carry many meanings which change according to the context were termed “suitcase” words by Minsky (MINSKY, 2006). Conscience, experience, and morality are examples. Adopting this “suitcase” perspective may change the way as we define, model, and simulate trust. Instead of trying to capture all meanings of trust, or to create models that are superficial and cannot be used for certain meanings, the suitcase perspective may establish a model that allows us to handle different meanings of trust according to each situation.
This new approach to model trust intends to support people to identify, define, and simulate trust situations, with potential to handle different meanings of trust in Citizen Science in the Biodiversity domain, or in other daily situations, and this is the main contribution of this work.
1.3 Objectives
The main objective of this research is to model trust under a suitcase word perspective and apply it to a case study in the Biodiversity domain.
These objectives may contribute to science by proposing: 1. evidence that trust is a suitcase word, and how this can be modeled; 2. a comprehensive model with potential to handle many meanings of trust; 3. a less subjective method to measure trustworthiness; 4. a workable model in the “suitcase” perspective which can be simulated;

24
1.4 Materials and methods
Our research method started by splitting the main objective in two hypotheses to be tested: 1) Trust is a suitcase word and 2) Trust can be modeled and simulated under a suitcase word perspective with potential to handle different meanings.
Figure 1 shows a graphical representation of our research method. From a literature review, we conducted a mining process to collect evidence that trust is a suitcase word, and we collected conceptual foundations to support the formalization of a ‘suitcase model’ for trust. The literature review was directed by discussions with other scientists, such as Dr. Stephen Marsh and Dr. Piotr Cofta. After proposing the suitcase model, we created a replica of a simulator for trust, the PlayGround (MARSH, 1994), which was modified to carry the new model, originating the PlayGround v2.0. At the end, we ran a case study to simulate how our suitcase model could handle different meanings of trust in Citizen Science in the Biodiversity domain.
Figure 1 - Graphical representation of our research method The following four main steps provide more details about our method to test the hypotheses and achieve the objective of this research: 1. Problem domain study: After initially contextualizing the problem in Citizen
Science in the Biodiversity domain and the objectives, we performed a literature review across many disciplines of science (Computer Science,

25
Management, Psychology, Economy, Social Sciences, and others) to collect definitions, models, and conceptual foundations about trust. They helped us to refine our goals, realign our research method, and proceed with a new review. For instance, models from (MARSH, 1994) and (COFTA, 2007) were then more detailed because of their potential to be a good starting point to our suitcase model. After this review stage, we constantly followed trust research to consider additional relevant papers. Our work was influenced by scientific discussions with other scholars, such as Dr. Stephen Marsh and Dr. Piotr Cofta. These discussions took place during an interchange at University of Massachusetts – Boston, and during participations in international events, such as the Biodiversity Information Standards Conference (TDWG Conference, Costa Rica) and the International Federation for Information Processing – Trust Management Working Group (IFIPTM) Graduate Symposium (Germany). TDWG is the most important forum to discuss standards related to biodiversity data, and its 2016 conference took place in Santa Clara de San Carlos, Costa Rica. On another thread, after prior discussions about our work, we were invited by Dr. Marsh to participate in the 2016 IFIPTM Graduate Symposium on Trust and Trust Management (Darmstadt, Germany), which was followed by an opportunity to network with experts and other students in the field. Another important point of validation to our hypotheses was our complete paper being accepted in the IEEE 14th International Conference on Privacy, Security and Trust (Auckland, New Zealand).
2. Scientific literature data mining: We conducted a study with ‘trust’ articles from Thompson Reuter’s Web of Science to collect evidences that trust is a suitcase word. We tested this initial hypothesis by splitting it in two subhypotheses: 1) “Once trust is present in many situations of our daily lives, a wide range of disciplines must study it” and 2) “Trust has different meanings across the disciplines”. The first sub-hypothesis was tested with a distribution analysis using an article and journal-based classification from Web of Science. The second one was tested with a co-occurrence analysis of keywords supported by the VOSviewer (VOSVIEWER, 2016). Maps of keywords were created to compare the meanings of trust among disciplines.

26
Both analyses were then used as evidence to decide if trust could be considered a suitcase word.
3. Modeling the suitcase model: We proposed a framework to manage trust with three components: 1) a suitcase model to identify different meanings of trust according to each specific situation, which is the main purpose of this work, 2) a procedure to detail trust situations in terms of the suitcase model, and 3) a behavioral decision model of confidence to simulate cooperation, which is required to simulate the new model. We started by creating our suitcase model considering the conceptual foundations found in the literature review. After that, we proposed a sequence of instructions to define a real life trust situation in terms of the suitcase model. The last step uses the notion of the joint role of trust and control to produce a confident behavior (COFTA, 2007), and a modified version of Marsh’s computational formalization of trust (MARSH, 1994) to create our generalized model of behavioral decision of confidence. This behavioral decision model is required to simulate trust effectively and verify its performance with different meanings of trust, once trust and control work together to produce a confident behavior and consequently a cooperation action.
4. Suitcase model application: We proposed a simulator to apply our suitcase model to a case study. We started by replicating the PlayGround in a web version. Results from this replication were important to test Marsh’s formalism and served as basis to our modified version with the suitcase perspective, the PlayGround v2.0. This simulator was used to apply our new model to handle different meanings of trust in Citizen Science in the Biodiversity domain.

27
1.5 Thesis structure
This document is organized into eight chapters:
• Chapter 2 presents a scientific literature data mining aiming to collect evidence that trust is a suitcase word;
• Chapter 3 presents conceptual foundations of trust which were the basis to our proposed model;
• Chapter 4 describes the foundations of the suitcase model and the generalized model of behavioral choice of confidence;
• Chapter 5 presents a replication of the PlayGround simulator and how it was modified to support the suitcase perspective;
• Chapter 6 presents an application of our suitcase model by discussing a case study in Citizen Science in the Biodiversity domain;
• Chapter 7 discusses the results of this research and point the main challenges and contributions of this work;
• Chapter 8 presents the conclusions and the proposed future steps of our research;
In addition, six appendixes complement this thesis.
• A – Extraction of articles records from Web of Science describes how articles were extracted to literature scientific data mining, including download restrictions.
• B – Hierarchy relation among Web of Science categories presents a complete hierarchy of categories used in scientific literature data mining;

28
• C – VOSviewer configuration presents in details the settings used to produce the co-occurrence maps of keywords.
• D – Maps of co-occurrence shows the co-occurrence maps of keywords from Operations & Management Science and Sociology research areas, and from Computer Science categories.
• E – Results of Marsh’s PlayGround presents the original results from a PlayGround experiment.

29
2 Scientific literature data mining
The main goal of this analysis was collect evidence in scientific literature that trust is a suitcase word. Our hypotheses to be tested in this analysis were 1 – if trust is a concept important in many situations of our daily lives, a wide range of disciplines must study it, and 2- trust has different meanings across the disciplines. We selected as source of our analysis the Thomson Reuters' Web of Science (WOS) database1. Besides the fact that it indexes respectable journals, from different areas of science, it is able to export complete records (title, abstract, citations, keywords, and references) for local manipulation. We searched for articles written in English language that contained the words “trust”, or “trustworthy” or “trustworthiness” in the title, abstract or keywords. Appendix A describes how this search and the extraction of records were conducted.
To test hypothesis 1, the articles were organized in categories, originally assigned by WOS, that allowed us to identify from which discipline was each article was related. It was observed that there is not an authoritative area of knowledge for ‘trust’, once that articles were classified in a wide range of disciplines. The hypothesis 2 was tested by verifying the words that co-occur with ‘trust’ in each discipline. A premise for this test was that the different meanings of ‘trust’ could be identified by the words that occur with it. For instance, Computer Science articles use ‘trust’ with different meanings from Psychology articles, so that the words that co-occur with ‘trust’ in both disciplines might be different in each case. This was observed by creating maps of keywords for each discipline, which were different among the disciplines. The method applied and results observed are described in the next items.
2.1 Distribution among areas of knowledge
First step to test hypothesis 1 was to understand the categorization used by the Web of Science. Articles are assigned in main areas (Social Sciences, Life Sciences & Biomedicine, Technology, Physical Sciences, and Arts & Humanities), in “Web of
1 The access to the platform was offered by the University of Sao Paulo, through agreement with the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (Capes) of Brazilian Ministry of Education.

30
Science Research Areas”, and in “Web of Science Categories” (Figure 2). Research areas and categories are respectively article-based and journal-based terms assigned by Thompson Reuters. An article can have assigned more than one category and more than one research area if these categories come from different research areas. Web of Science does not offer a complete and explicit hierarchical relation between these terms (area and categories); however, we considered that they were related as presented in Table 21 in Appendix B, where we also provided a description of the method used to relate these terms.
Figure 2 - Adopted hierarchy of categories from Web of Science From a total number of 55,249 articles selected2, trust-related papers occur in roughly equal percentage across Social Sciences (35%), Life Science & Medicine (29%), and Technology (29%), whereas the Physical Sciences and Art & Humanities, make up the remaining 6% of the papers (Figure 3).
Figure 3 - Distribution of articles among main areas of Science in Web of Science A more granular analysis (Table 1) shows that interest in trust occurs across
146 research areas and 252 categories. As an article may be classified in more than one category, including from other research areas, the number of appearances
2 Accessed on November 17th 2016.

31

(92,765) is greater than the number of articles selected from the database (55,249).

Table 1 - Trust-related articles distributed in Web of Science hierarchy

Main Areas Social Sciences

Research Areas

Categories Appearances

%

22

43

32,477

35.0

Life Sciences & Biomedicine

73

95

27,000

29.1

Technology

21

52

26,950

29.1

Physical Sciences

16

36

4,050

4.4

Arts & Humanities

14

26

2,288

2.5

146

252

92,765

An analysis over the top 10 categories in terms of appearance shows that they are distributed among the three top main areas (Technology, Social Sciences, and Life Sciences & Biomedicine) and that interest in trust occurs across different categories (Table 2). For instance, in the domain area of Technology the topic of trust is important for Management, Computer Science Information Systems and Computer Science Theory Methods.

Table 2 - Top 10 Web of Science Categories by appearances of trust

#

Web of Science Category

1 Management 2 Business 3 Computer Science Information Systems 4 Public Environmental Occupational Health 5 Economics 6 Law 7 Computer Science Theory Methods 8 Health Care Sciences Services 9 Medicine General Internal 10 Political Science

Appeara nces

Main Area

4,594

Technology

4,236

Social Sciences

3,800

Technology

3,447 Life Sciences & Biomedicine

2,848

Social Sciences

2,344

Social Sciences

2,296

Technology

1,890 Life Sciences & Biomedicine

1,853 Life Sciences & Biomedicine

1,741

Social Sciences

32

Another interesting fact is a substantial number of articles that belong to one or more research areas. This is another indication that trust is not one authoritative category or research area but instead found across disciplines. For instance, 746 articles of WOS assigned to “Social Sciences - Other Topics” were also assigned to “Business & Economics”. In Computer Science research area, 49.8% of 7,839 articles appear in more than one research area (Table 3).

Table 3 - Appearances of Computer Science (CS) articles in other Research Areas

Appear only in CS Research Area Appear in CS and 1 other Research Area Appear in CS and 2 other Research Areas Appear in CS and 3 other Research Areas Appear in CS and 4 other Research Areas

Total of Articles

3,939 50.2%

2,437 31.1%

1,359 17.3%

72

0.9%

32

0.4%

7,839

Total of Appearances

3,939 29.5%

4,874 36.5%

4,077 30.6%

288

2.2%

160

1.2%

13,338

2.2 Co-occurrence of keywords

After the preliminary analysis for hypothesis 1, we proceeded with the test for hypothesis 2: “Trust has different meanings across the disciplines”. We assumed that the keywords of an article identify the meaning that trust carries on it, and that the articles of a specific discipline have similar meanings for trust. So that it was expected that the map of co-occurrence of keywords from articles of a specific discipline could identify the meanings of trust used in this discipline. Once these maps were different among the disciplines, we could validate the hypothesis 2.
We created the maps of keywords using a bibliometric tool, the VOSviewer version 1.6.5 (VOSVIEWER, 2016), which was developed by the Centre for Science and Technology Studies, Leiden University. This freely available software creates distance-based maps, which means that the distance between two items in the map reflects the strength of their relation. A stronger relation is represented with a smaller distance between two items. This feature can make it easy to identify clusters of related items (VAN ECK; WALTMAN, 2010), which are assigned by clustering techniques and

33
differentiated by colors. A direct relation between two items (co-occurrence) is represented by a line connecting both. Details about these techniques can be found in (WALTMAN; VAN ECK; NOYONS, 2010). Essentially, the graphics are generated over a network data already created, a network data imported from other software, or from bibliographic data (such as authors, references, citations, keywords, and organizations) extracted from scientific databases such as Web of Science.
VOSviewer offers a Network Visualization (Figure 4) in which the size of the label and the size of the circle are determined by the item’s importance (frequency) among all items (VAN ECK; WALTMAN, 2010). It is also possible to set parameters to fine-tune this visualization, which are detailed in the user manual (VOSVIEWER, 2016).
Figure 4 - Co-occurrence of authors in the Psychology research area VOSviewer creates its maps in three steps: extracting the similarity matrix, calculating the coordinates of nodes, and then translating, rotating, and reflecting the map (VAN ECK; WALTMAN, 2010). The similarity matrix is obtained from a normalization of the co-occurrence matrix, which corrects the differences in the total number of occurrences or co-occurrences of items. Details about this normalization can be found in (VAN ECK; WALTMAN, 2009). The similarity matrix obtained in step one is an input to calculate the coordinates of all nodes using the “Visualization of

34
Similarities” (VOS) mapping technique, for which the higher the similarity between two objects, the higher the weight of their squared distance. The main goal of VOS is to minimize the weighted sum of squared Euclidean distances between all pairs of objects. A detailed description of the technique is presented in (VAN ECK; WALTMAN, 2007). This minimization does not have a unique solution, so that three transformations are applied at the end to ensure the same map from the same co-occurrence matrix.
Co-occurrence maps
We extracted bibliographic data from Web of Science to create maps for four Research Areas: Psychology (Figure 5), Computer Science (Figure 6), Operation Research Management Science (Figure 29, Appendix D) and Sociology (Figure 30, Appendix D). A ‘general’ map with these four research areas was also created (Figure 7). Colors in this general map do not represent the research areas, like in other maps the colors represent clusters of keywords using techniques described in (WALTMAN; VAN ECK; NOYONS, 2010), that is, research areas are mixed.
We decided to get these areas to test our hypotheses of many meanings due to their large number of articles and their multidisciplinary. A more granular analysis of our hypothesis was done here using maps created from the five Computer Science categories: Information Sciences (Figure 31), Artificial Intelligence (Figure 32), Hardware (Figure 33), Software Engineering (Figure 34), and Theory and Methods (Figure 35). These maps are available in Appendix D.
See Appendix A for further details about bibliographic data extraction, Appendix C for details about maps settings on VOSviewer, and Appendix D for complementary maps.

35 Figure 5 - Psychology research area co-occurence map of keywords Figure 6 - Computer Science research area co-occurrence map of keywords

36
Figure 7 – Co-occurrence map of keywords from Computer Science, Psychology, Operation Research Management Science, and Sociology
2.3 Considerations about the literature mining
Distribution analysis showed that trust articles occur across a wide range of disciplines, and sometimes in more than one discipline the same time (Table 3, pg. 32). Considering the classification offered by the Web of Science, there is not a concentration of articles in one main area, Research Area, or WOS Category. For instance, Life Sciences & Biomedicine and Technology research areas have almost the same number of appearances, 27,000 and 26,950 respectively, and the difference between the first and the second in the rank is around 20% (Table 1, pg. 31). In an analysis of categories, the difference between different disciplines such as Computer Science Information Systems and Public Environmental Occupation Health is only 10% approximately (Table 2, pg. 31). Our literature analysis was able to confirm hypothesis 1 – “if trust is present in many situations of our daily lives, a wide range of disciplines must study it” - is true.
VOSviewer was very helpful to execute our co-occurrence analysis. Considering that different meanings of ‘trust’ could be identified by the keywords that occur with it, maps from Computer Science (Figure 6, pg. 35) and Psychology (Figure

37
5, pg. 35) research areas showed that trust meanings from these areas are significantly different. Computer Science meanings were mainly related to security, recommendation, management in social networks, and behaviors in e-commerce environment; whereas Psychology has meanings related to personality, cooperation, satisfaction, and trustworthiness. Even in a specific research area the categories may have different meanings. In Computer Science, Artificial Intelligence (Figure 32, Appendix D) has meanings like cooperation, communication, and similarity, whereas Software Engineering (Figure 34, Appendix D) has meanings such as cryptography, provenance, and reliability. We conclude from these different meanings across disciplines that our hypothesis 2 - “trust has different meanings across the disciplines” - is also true.
This analysis conducted from a respectable scientific database and method described above presented significant evidence about the suitcase perspective of trust.

38
3 Conceptual foundations of trust
Once the previous chapter showed the evidence that trust is a suitcase word, this chapter presents some conceptual foundations that support our model of trust under a suitcase perspective.
3.1 Many Definitions of Trust
Giving the many contexts in which trust appears in daily experiences, it is not surprising that trust is studied widely in applied and academic disciplines such as Business, Philosophy, Law, Psychology, Computer Science, Sociology, and Biology. One reflex of this multidisciplinary, associated whit the quantity of meanings, is that there is no consensus about what trust is.
Unfortunately for scholars (BAUER, 2015; CHO; CHAN; ADALI, 2015; JOSANG; ISMAIL; BOYD, 2007; MAYER; DAVIS; SCHOORMAN, 1995; MCKNIGHT; CHERVANY, 1996), there are dozens of definitions and models for trust, and some of them are considered elusive and ambiguous. See (BAUER, 2015) for a list of definitions. Furthermore, no one author, definition, or model is authoritative. Each area tends to view trust from its own perspective (MCKNIGHT; CHERVANY, 2001), focusing on sub-concepts to create its particularized version of trust (BAUER, 2015).
These definitions of trust are developed, in general, over empirical works that fit a determined type of research (MCKNIGHT; CHERVANY, 2001) or have a lack of concept-measurement consistency (BAUER, 2015). On the other hand, some initiatives created general models using as many definitions as possible, in attempt to escape from a narrow view. However, these models are highly conceptual in a way that are not workable.
Despite this plurality, some authors are largely cited as references across disciplines, such as (BAIER, 1986; BARBER, 1983; CASTELFRANCHI; FALCONE, 2000; GAMBETTA, 2000; HARDIN, 2002; LUHMANN, 1979; ROTTER, 1967). For instance, (MCKNIGHT; CHOUDHURY; KACMAR, 2002) and (MAYER; DAVIS; SCHOORMAN, 1995), originally from Management Sciences, are largely cited in Psychology Research Area, as observed in a co-citation analysis produced with the VOSviewer (Figure 4, pg. 33). (MAYER; DAVIS; SCHOORMAN, 1995) is the most

39
cited reference with 418 citations and (MCKNIGHT; CHOUDHURY; KACMAR, 2002) is the fifth with 279 citations from a universe of 177.794 citations in the area.
Models from (COFTA, 2007) and (MARSH, 1994) propose general views that encompass most part of other widely cited definitions, and although they are not suitable individually to be used with the suitcase perspective, together they enabled us to compose our new suitcase model. See item 3.9 for further details about these views.
3.2 Actors, Risk & Context
This item brings some ideas about actors, risk, and context related to trust.
Actors
Actors lead a trust situation. The trustor is the actor that has an attitude of trust, mistrust, or distrust, toward another actor, the trustee. In its turn, the trustee “can be anyone who may perform a task, upon whom one might wish to rely” (O’HARA, 2012).
Trustor and trustee can be the same person when you trust yourself. Both of trustor or trustee might be humans, but they can be also machines, software, animals, roles (doctor, lawyer, teacher, football referee, etc.), institutions, and others. The use of non-humans in trust situations depends on the understanding of their intentions, capacities and incentives. For example, “A piece of software has no intention to work in anyone’s interest, although it may be intended to work in the interests of those who have paid for it” (O’HARA, 2012). We decide not to turn on philosophical discussions about it, so that our suitcase’s model of trust is open to any kind of actor.
Sometimes a third actor can be part of a trust situation. This third person is “someone with the credentials to issue a representation of behavior to which the trustee will conform” (O’HARA, 2012); However, the trustor has to give this authority to him and trust him. At this point, we establish a recursive trusting behavior. The presence of the third actor in trust situation is frequently associated in social networks and online services studies (GOLBECK, 2006; JOSANG; ISMAIL; BOYD, 2007).

40
Risk
There is not a consensus in literature about the relationship between trust and risk, if risk is a requisite/antecedent to trust, is trust, or is an outcome of trust (MAYER; DAVIS; SCHOORMAN, 1995). However, for (LUHMANN, 1979) trust presupposes a situation of Risk. Without risk, trust does not exist, because others will meet our expectations. Without a possibility to choose, there is not a trust situation. The trustor can choose to accept the risk, or not. Once taking a behavior decision to trust, the trustor is willing to accept the risk (LUHMANN, 1979), and he is willing to risk some of his assets in the transaction (O’HARA, 2012).
Context
Context are restrictions of the circumstance in which the trustee is claimed to be trustworthy. It is used to give importance to a trust situation and delimitate the horizon of our assessments. The restrictions might be a task, a particular role, or a specific people. Besides that, we can add environmental events, economic circumstances, among many other factors to specify the context. However, it has to be delimited with useful information.
Furthermore, a capacity to trust in some contexts would not imply a capacity to trust elsewhere (O’HARA, 2012). The trustee can be trustworthy in a context, but not in another. For example, we can trust someone drives safely in good weather conditions, but not in a context of pressure with rain, wind, and traffic.
3.3 Situation, Belief, Probability, and Behavior Decision
In a “Trust Situation” the trustor has to decide about the future. The trustor expects a specific behavior from the trustee, and there is always a risk involved that may take place that the trustee does not behave such as expected. It is not certain that this risk will occur because the action is in the future. The trustor acts by deciding to trust, mistrust, or distrust the trustee (see item 3.7 for definitions). This behavioral decision is based on the trustor’s probability to take risks, the importance of situation and the involved risk (context-dependence), and the trustee’s capacity and interest to

41
behave such as expected. Probabilities are involved in this behavior decision because the results are over a promised action, that can be realized, or not.
When our perception about a trustee’s capacity and interest to behave as we expect are high, and our perception about the risks are low, our confidence to engage in a trust situation will be high. However, even if we are highly confident, not necessarily we produce a trusting behavior. We can trust that airplanes are safe, but produce a trusting behavior only when flying one.
Trust that drivers will respect the law and stop their cars before the crosswalk is an example of everyday trust situation. Ready to cross a street, pedestrians have to assess the probability that drivers have ability to operate the car, that drivers are not intending to deliberately hit them, that drivers are paying attention to the way, and so on. The risk of being hit by a car if the driver does not stop is very significant. We can collect some evidence about these expectations, looking for how they conduct their cars, if they are slowing down, if they are looking at the signal and us, and so on. However, we will not have all information to get the perfect assessment. Our decision will be based on the best judgement with the available data, composed with our capacity to take risks in general.
3.4 Three-part relation
Trust can be explained by a three-part relation (BAIER, 1986; HARDIN, 2002; LUHMANN, 1979). This relation is translated as “a trustor A that trusts, i.e. judges the trustworthiness of a trustee B with regard to some behavior X” (BAUER, 2015). A behavior in this case can also be interpreted as an action. (BAUER, 2015) named this relation as ‘ABX’, where A and B are actors and X the expected behavior.
3.5 Group and Cooperation
Trust is a major aspect of cooperation of any kind (MARSH, 1994). Agents working together implies Cooperation, when people “work and act together on a task, and share profits or benefits from doing so”; implies Collaboration, when “one works jointly with other on a task”; and implies Coordination, when “the act of causing parts

42
to function together or in a proper order”. However, cooperation is not necessarily a result of trust, while trust is a result of cooperation-based mutual benefit of helping each other (GAMBETTA, 1988). Group formation is a deep and complex phenomenon, which will not be discussed in this material. However, we will consider that groups may be formed by cooperation behavior.
3.6 Types of Trust
Trust has many “types” (or “classes” or “sub-concepts”), such as Situational, Social, Interpersonal, Political, and General. Although they carry different meanings and have distinct limits, ABX relation can explain all of them. For instance, in case of Political Trust, which express our belief that the political class will behave such as expected in many contexts (O’HARA, 2012), B is set as the political class and A and X can vary according on the context. In Interpersonal Trust, which is an expectancy held by an individual or a group about the word or promise of another individual or group (ROTTER, 1967), A and B are individuals or groups for any expected behavior X. In general, these types have a simple variation of actors and expected behaviors.
Other types of trust are essentially related to non-rational components, results of traits of personality forged by family relationship, religion, or past remarkable experiences, such as moral, faith, reciprocity, and others. These types such as Basic Trust (MARSH, 1994), Disposition to Trust (MCKNIGHT; CHERVANY, 2001), and Propensity to Trust (MAYER; DAVIS; SCHOORMAN, 1995) can be explained also by an ABX relation. Nevertheless, in this case, A and B are the same actor and X is irrelevant.
In an attempt to study the variety and impact of types of trust in time, we performed an n-gram analysis with Google Books Ngram, which is an online search engine that charts frequencies of any set of comma-delimited search strings using a yearly count of n-grams found in sources printed between 1500 and 2008. Further details about the tool operation can be found in (GOOGLE, 2017). An n-gram is a contiguous sequence of n items such as syllables, letters, or words. For instance, ‘suitcase word’ is a 2-gram (or bigram). Figure 8 shows trends in seven bigrams from 1900 to the early 2000’s.

43
Figure 8 - Ngram distribution of types of trust The x-axis shows the percentage of these bigrams over all the bigrams contained in Google sample of books written in English and published in the United States. Comparing terms among them, General trust, Political trust, and Trust disposition have been largely used since 18th century, although the use of the last one is in a decline since the 1940’s. Considering that Trust disposition and Basic trust have similar use, we can assume that the first has been replaced by the second. The use of Political and Social trust started to rise after the 1920’s, and increases after the 1960’s, reflecting the economic and political issues of the period. In general, we may consider that types of trust are reactions to temporal facts and reflect meanings of trust that are important at that time.
3.7 Trust dynamics and measurement
Distrust is not a complement for trust. If we cannot trust someone, it does not mean that we automatically distrust this person. We must have reasons to do it. Sometimes we do not have enough information to decide about the situation (Trust or Distrust). We can be in a state of complete ignorance, or we can have weak evidence to trust or distrust that cannot pass the “Limit of forgivability” (Undistrust) or the “Cooperation threshold” (Untrust).

44
These limit and threshold are change points in trust dynamics (Figure 9). We can treat the behavioral decision to trust as a binary decision in which trust is evaluated in either discrete or continuous scale. When trust surpasses the Cooperation threshold (trust) we can produce a cooperative behavior, whereas a value lower than a Limit of forgability (distrust) produces a non-cooperative behavior (CHO; CHAN; ADALI, 2015).
Figure 9 – The trust continuum, extracted from (CHO; CHAN; ADALI, 2015)
In a general way, we can define the states of trust as:
• Trust – trust values are equal or greater than a Cooperation threshold;
• Untrust – trust values are positive but not enough to reach the Cooperation threshold;
• Undistrust – trust values are negative but greater than a Limit of forgivability;
• Distrust – trust values are lower than the Limit of forgivability;
Daily experiences show that transitions among these states are not equal. In general, it takes time to trust someone, but it is easy to distrust it; and once in a distrust state, return to a trust state is harder than before.
Trust has “been broken, misplaced, abused, shaken, and violated. Occasionally it is repaired and rebuilt”, but if we do not measure it, “we can ignore it, fail to cultivate it, and ultimately lose it” (THOM; HALL; PAWLSON, 2004).

45
3.8 Trustfulness and Trustworthiness
This item presents trustfulness and trustworthiness concepts.
Trustfulness
Trustfulness is the trustor’s general capacity to take risks (TULLBERG, 2007). This capacity is very similar to “disposition to trust” (MCKNIGHT; CHERVANY, 2001) and “propensity to trust” (MAYER; DAVIS; SCHOORMAN, 1995). Trustfulness concerns only the trustor and it is independent of trustee assessment. For example, one can have all reasons to trust in someone, but is not capable to take the associated risk for reasons that are related to himself. The opposite view can be also true; in certain cultures, a family member is always trusted, independent of assessment.
Trustfulness is essentially compounded by non-rational components, results of traits of personality forged by family relationship, religion, or past remarkable experiences, such as moral, faith, reciprocity, and others. (MCKNIGHT; CHERVANY, 2001) said that the “disposition to trust [such as the trustfulness] comes primarily from trait psychology, which says that actions are modeled by certain childhood-derived attributes”.
Religion also affects trustfulness in many ways as a behavior guide. As examples, “for religion, in the absence of proof, believing is necessary”, or “If you believe that a potential cooperator believes she will be punished if cheating on you, this is a reason for you to be trustful” (TULLBERG, 2007). Faith in humanity is a belief that others, without assessment, are usually competent, benevolent, honest/ethical, and predictable (MCKNIGHT; CHERVANY, 2001).
Some disappointments or good experiences can affect trustfulness, but it tends to be stable across situations (MAYER; DAVIS; SCHOORMAN, 1995) and people (MCKNIGHT; CHERVANY, 2001).
Trustworthiness
Trustworthiness and trust are distinct concepts that should be viewed as interrelated concepts. They are independent because one can possess trustworthiness

46
even when there is no call for trust (BAUER, 2015). However, trustworthiness is an antecedent of trust (MAYER; DAVIS; SCHOORMAN, 1995), so that trustworthiness is essential to understand trust (O’HARA, 2012).
If someone is motivated to do what we trust him to do, then this person is trustworthy (HARDIN, 2002). In (O’HARA, 2012) words, trustworthiness is “the property that they will do what they say they will do. If they fail, then it will typically be for some reason outside their control”, and “a person is trustworthy when she does what she says she will do.”
(MCKNIGHT; CHERVANY, 2001) suggest that this property can be measured with 16 trust-related characteristics, organized in second-order conceptual categories such as predictability, integrity, benevolence, and competence. However, these characteristics, even wide, may limit the meanings of trust considered.
3.9 Models of Trust
This item present in details (MARSH, 1994) and (COFTA, 2007) models of trust, which were used as basis to model and simulate trust under a suitcase perspective.
Stephen Marsh’s model of trust
Marsh’s thesis (MARSH, 1994) is widely cited by computer science scholars, especially among those studying social networks, peer-to-peer systems, multi-agent systems, e-commerce management, and wireless sensor networks. His work develops a formalization and a computational model for trust, implements the model in a simulator called PlayGround and undertakes some experiments to demonstrate its feasibility.
The formalism (Table 4) is centered on the agent concept developed in Artificial Intelligence (AI) (MARSH, 1994). The key idea is to extend an AI agent’s reasoning capabilities to reproduce trust behavior - that is to decide when and with whom to cooperate. The top-down methodology adopted by Marsh does not start with a particular definition of trust but instead, observes “the phenomenon and attempt to define a formalism which behaves in the same manner”.

47

Table 4 - Summarized notation of Marsh’s formalism

Name

Notation

Range

Description

Situations Agents Societies Utility
Importance Knowledge

, , . .. a, b, c, ... S1 , S2 ...  ()  ()  ()



→ is a point in time for a specific agent

A

→ concept developed in Artificial Intelligence

Sx ∈ A → agents grouped according to some metric

[-1,+1] → utility of situation  to agent x

[0,+1] → importance of a situation  for agent x

True / False → x met y at time t

To demonstrate the feasibility of his formalism, Marsh proposed an application in terms of a decision model for trust. There are three types of trust in this application: Basic, General, and Situational (Table 5). The Basic Trust (BT) derives from agent’s past experiences, but does not have a connection with other agents, a specific situation, or the environment. Simply, “Alice trusts”3, and this can be applied to anything. However, it may increase or decrease in the time considering her new experiences of life. The General Trust (GT) denotes how much an agent trusts another in a complete perspective: “Alice trusts Bob”. The GT represents a moment in the time, whereas the Estimated General Trust (EGT) considers all interactions between Alice and Bob in the time. Otherwise, the Situational Trust (ST) represents the trust in an agent in a specific situation: “Alice trusts Bob in this situation”.

Table 5 - Types of trust in Marsh’s formalism

Name

Notation

Basic General Situational



 

 ()

̂  ())

 (, )

Range

Description

[-1,+1) [-1,+1) [-1,+1) [-1,+1)

→ basic disposition to trust someone or something → how much x trusted y at the time t → how much x trusted y (estimated from all interactions) → how much x trusted y in situation  at the time t

3 Alice and Bob are widely used as personas in security problems.

48

To cooperate in a situation ( ), Alice’s (x) trust in Bob (y) must be greater than a threshold (eq. 1). This Situational Trust (Tx(y, α), eq. 2) depends on the Importance (Ix(α)) and the Utility (Ux(α)) of the situation for Alice, that are respectively a rational economic and a subjective judgments of the situation, and on the Estimated General Trust (EGT or T̂ x(y)) of Alice in Bob.

Tx (y, α) > Coop_Threshold x (α) ⟹ _(, , )

(1)

Tx (y, α) = Ux (α) × Ix (α) × T̂ x(y)

(2)

The General Trust (GT or Tx(y)) denotes how much an agent trusts another agent in general when the decision is being made without reference to a specific situation. For instance, Alice may say “I trust in Bob” based on an analysis of historic interactions with him.
The definition of the EGT depends on Alice’s memory span and her Disposition to Trust. Alice can use the highest past value of GT in Bob in a similar situation (Optimistic disposition), the lowest value (Pessimistic), or a mean of values (Realistic, eq. 3) to define the EGT.

T̂ x(y)

=

1 ||

∑∈

 ()

(3)

A in eq. 3 represents the set of all situations similar to ( ) which Alice has experienced with Bob. If Bob is unknown in a similar situation, GT past values in any situation are used to define the EGT. When there is not a past value for GT, then Alice’s Basic Trust (BT) is used.
The Cooperation Threshold (eq. 4) is an estimate of how much Perceived Competence and EGT mitigate the Perceived Risk, multiplied by the Importance of the situation to Alice. The Perceived Risk and the Perceived Competence of Alice in Bob depend on the knowledge between them from previous interactions. See (MARSH, 1994) for further details about formulas and concepts definitions.

Coop_Threshold x (α)

=

Perceived_Risk x (α) PerceivedCompetencex (y,α) + T̂ x(y)

× Ix (α)

(4)

49
Despite its benefits, avoiding a particular definition for trust brings some important weaknesses to the formalism. As pointed by (MARSH, 1994), “the kind of trust we define (implicitly or explicitly) using the formalism, that is, the kind of behavior we produce from trusting agents, is indicative of only one subclass of trust (and there are many)”. Furthermore, “however abstract the formalism may be, it still has to be founded on some aspect of trust, which is one of many types”.
The “resultant behavior” produced by Marsh’s formalism is a representation of trust, but does not do “a deep consideration of the aspects of a particular situation”. For him, this is a strength of the formalism, since it is not limited by a specific situation, making it generalizable. Here, we partially agree with him. The formalism must be simple and generic and an attempt to consider all aspects of situations involving trust would be a disaster in terms of complexity and practicality. Thence, the “loss of specificity is, from this viewpoint, no bad thing”, and can be accepted by most part of situations. However, the formalism must be also extensible to consider additional aspects of trust when necessary, once not all the meanings of trust are represented.
Piotr Cofta’s model of trust
(COFTA, 2007) aims to understand how confidence is built and how it is influenced by others. Confidence is presented as “the belief that the other part will behave in a way that is beneficial and desired for the first party”, allowing people to engage in relationships by externalizing a set of actions (confident behavior). The trigger to the confident behavior can be defined by a limiting factor. When trustor’s confidence value is higher than this threshold, the confident behavior can be produced.
Cofta’s model for confidence is a self-referring sum of control and trust, driven and limited by complexity. Control and trust work together, and no one alone is an alternative. Figure 10 shows a graphic representation of how a confident behavior is produced in this context, which is also formalized in equations.

50

Figure 10 - Confidence behavior model, extracted from (COFTA, 2007)

On the assumption that confidence, and consequently the confident behavior, is produced by an assessment about trustee’s trustworthy behavior (eq. 5), trust is defined as an estimate about trustee’s trustworthiness (eq. 6), which is defined as the trustee’s intentions toward the trustor in the transaction. The gap between these intentions and trustee’s trustworthy behavior can be enforced by instruments of control (eq. 7).

 →  =  ( )

(5)

  →  =  ( )

(6)

  →  =  ( ) − ()

(7)

where:

•  – the function to produce trustor’s estimate of its argument • – confidence • – trustworthy behavior •  – trustworthiness • – control

Considering all elements of those formulas in terms of subjective probability and numeric values, Cofta defines:

51
• Trust as an “estimate of a subjective probability of certain desired course of action in the absence of control”.
• Control as “the difference between the estimate of the subjective probability of trustworthy behavior and trust.”
One of the key points of Cofta’s model is the complexity-horizon exchange. A trustor cannot include everyone and everything in his confidence behavior decision. The extent of the horizon for a transaction depends on several factors, such as physical and psychological predispositions, current cognitive capabilities, tiredness, etc., including trustor’s ability to manage the number of entities and the relationships between them. The trustor has to eliminate irrelevant entities until he reaches an optimal complexity that provided a certainty for his decision.
Another key point is that trust and control require evidences (opinion, fact, signal), and the significance of each evidence is attenuated depending on the perceived honesty of the source. According to Cofta there are three classes of evidences for trust: continuity, competence, and motivation; and there are three classes for control: influence, knowledge, and reassurance. See (COFTA, 2007) for further details about these categories of evidence.
Recursion is also a key point for Cofta’s model based in the “basic building block” for confidence (Figure 11). In this block, numbers from 1 to 5 are inputs to trust and control processing, and output 6 is the result of those inputs. The operators and
have the intuitive meaning of addition and multiplication, respectively; however, “this does not imply that actual semantics of those operators or any numeric metric of confidence”.

52
Figure 11 - Basic building block for confidence, extracted from (COFTA, 2007) Trust and control processing depend on proofs (or evidences) that come from honest sources. The confidence in source honesty may be assessed with another block, i.e. entries 2 and 4 can be another building block for confidence. However, confidence in instruments are different from confidence in the trustee; while the trustee is expected to behave in a manner that is beneficial to trustor, instruments are only expected to be able to control the trustee. Figure 12 shows an example of recursion extracted from (COFTA, 2007). Alice must assess her confidence in three entities: Bob, Carol, and Dave. Carol is the main source of evidence about Bob, and Dave is an instrument to control him.

53

Figure 12 - Building blocks within a horizon of three entities, extracted from (COFTA, 2007)

The basic building block for confidence is part of the confidence behavior model presented in Figure 10. Table 6 presents a summary from the formalization proposed by Cofta, extracted from (COFTA, 2007). See pg. 36 in (COFTA, 2007) for further details about the formalism.

Table 6 - Summarized notation of Cofta’s formalism

Name

Notation

World



Entities



Entity

1

Pieces of Evidences



Evidence

1

Description
 = { ,  }  = { , , … , 1, …  }
1 = (, )  = { 1, … ,  }
 = ( , , , → , → )

54

where:

•  − entity that holds evidence •  − entity that an evidence is about •  − complexity that is associated with dealing with such an entity •  − label that is used to distinguish evidence • → − level of confidence in an evidence • → − level of certainly about confidence in an evidence
Using the formalism, a trustor (Alice) confidence in a particular trustee (Bob) can be described as:

→ = →() ∑=1 →(|) where:

→ (8)

•  () − level of trust from available evidence of trust  •  (|) − level of control using instrument ; from evidence 

 = {  ℎ→  }

(9)

 = {  ℎ→  }

(10)

where:

•  − evidence related to trust in subject that is coming from  •  − evidence related to control in subject that is coming from  • ℎ→  − level of Alice’s confidence the honesty of  •  ∈  and  ∈ 

Our literature review across many disciplines of science showed us that trust is a widely discussed concept, with dozens of definitions and models, but no one of them

55
is authoritative. However, the some authors are largely cited among disciplines, and have definitions and models that may support the suitcase perspective, such as (TULLBERG, 2007), (BAUER, 2015), (MARSH, 1994) and (COFTA, 2007). This chapter showed details about:
• the importance of the context to trust; • how a trust situation and the types of trust may be decoded in a three-
part relation; • the importance of trustfulness and trustworthiness to trust decision; • a description of the model and formalism of Cofta’s and Marsh’s works;
These concepts were essential to model trust under a suitcase perspective, as presented in the next chapter.

56
4 Modeling trust under a suitcase word perspective
In real life, suitcases may have many objects with different purposes. We take out these objects according to a specific need, and to do it, we must be capable to identify the objects and operate their functionalities. Model trust under a suitcase word perspective (i.e. a suitcase model) is an opportunity to go beyond a restrict trust model, and use a procedure to help people, scientists, governments, and companies to identify, decode, measure, and consequently, manage different meanings of trust according to each situation.
To achieve these goals, the suitcase model must go beyond (MARSH, 1994) and (COFTA, 2007) models and define what is expected as “General Trust” (Table 5, pg. 47) or “trust-related proofs” (Figure 11, pg. 52) for each situation. The new model must consider, as a guide, what is expected and what is evidenced in each situation, to measure trust between the trustor and the trustee. Without that, the measures may treat distinct situations in an equal and erroneous way.
Inspired by this analogy, we propose a framework with three components: 1) a suitcase model to handle different meanings of trust, 2) a procedure to define trust situations in terms of the suitcase model, and 3) a behavioral decision model of confidence to simulate cooperation (Figure 13). These components work together, since we need a reference (model) to define and extract trust values from real life situations, and then simulate cooperation or grouping.
Figure 13 – Proposed framework to define trust under a suitcase perspective
Next items present these components and how they may work together.
4.1 Suitcase model
The suitcase model should be able to identify and structure in components as many meanings of trust as possible, making clear what are the expected behaviors in

57 each situation.
Our proposal considers the conceptual foundations presented in previous chapter to combine them to support the suitcase perspective, building a more comprehensive model, which is one of the main contribution of this work.
Figure 14 - Suitcase model components and their relations Figure 14 shows the components of the new suitcase model. Broadly speaking, decision to trust (or distrust, untrust, undistrust) is a belief constructed from trustfulness and trustworthiness. Trustworthiness in its turn is a function of the evidence collected with influence of trustfulness. These components and their relation are described in the next items.
4.1.1 Trust belief
Trust is a belief that the counterpart will behave as expected in a context.
4.1.2 Trust and Confidence
Trust is not confidence. Trust works together with control to improve confidence

58 (COFTA, 2007), and consequently produces a confident behavior after a behavioral decision. A confident behavior allows engaging in a relationship, which may derive to a grouping phenomenon. We changed Figure 14 to include the relation between the suitcase model and the other components that produce an action of cooperation (Figure 15).
Figure 15 - Confident behavior components (blue) and their relations with the suitcase model of trust
The component ‘confidence’ in Figure 15 was originally described by (COFTA, 2007) using Figure 11 and eq. 8. Other components, i.e. the ‘behavioral decision’ and their subsequent components, are described in item 4.3.
4.1.3 Trust situation with a behavioral decision of confidence
We expanded the original idea of three-part relation (ABX) from subsection 3.4 to include the control component (Figure 16). Since, control work together with trust to produce confidence, and consequently a cooperation action, it seems more workable

59
for trust simulation if all components that determine a behavioral decision of confidence are represented in the same situation. This new relation was named as ABX+C.
Figure 16 – ABX+C relation, an expanded three-part relation of trust
In this relation, actor A believes that actor B will behave such as X in a specific context, but there is a risk that B may not behave such as expected. A can be any type of actor (humans, societies, machines, animals, etc.) capable to trust another actor. The specific context, which is composed by known variables, such as political, economic, emotional, environmental, and others, can be simplified as ‘favorable’, ‘neutral, or ‘unfavorable’ to reduce complexity of trustworthiness computation.
The importance of the expected behavior is also part of the situation. For instance, in situations where the importance is very low, we can accept higher risks. This risk does not have connections with trustee’s capabilities and interests to behave such as expected. Risk in ABX+C situation is related to undefined and unexpected factors that are outside control, but can cause the trustee to fail.
Importance was also simplified in categorical values: favorable, neutral, or unfavorable for context, and high, medium, or low for importance. This was done to reduce the complexity of analysis, and minimally reduce the subjectivity of values. It is unpractical to decide how the many variables of the environment could affect trustworthiness and behavioral decision of confidence.
Control (C) is our contribution to the original ABX relation inspired by (COFTA,

60
2007). Instruments of control represented by C, such as law, influence, force, contracts, social rules, or hierarchy, can enforce B to behave such as expected (X). The inclusion of control inspires the name of our three-part relation version (ABX+C).
4.1.4 Trustworthiness computation
Trustor’s belief about the future is defined by trustor’s trustfulness and trustee’s trustworthiness.
Trustfulness is the trustor’s capacity to take risks, and it will be considered as a subjective probability in the scope of this work, i.e. trustfulness will be a probability derived from a personal judgement about the trustor’s capacity to take risks in the future.
Trustworthiness is a probability about the trustee’s capacity and interest to behave such as expected in the future. (COFTA, 2007) uses ‘trust-related proofs’ to compute this probability (see input 1 in Figure 11, pg. 52). Proofs are also related with control, see inputs 3, although in this work we consider control as a subjective probability.
The role of these proofs (or evidence)4 is to give to the trustor a verification or confirmation that the trustee had capacity and interest to behave such as expected in the past.
It is not the role of an evidence, in our proposed computation, to give an opposite confirmation about the trustee capacity and interest, i.e. if we expect an evidence that Bob can drive a car, only the times when he did a successful road drive test will be collected as evidence. The other many times when he failed will be ignored to the computation. We adopted this strategy because the opposite confirmation can be part of an evidence: Bob passed the road test without fail.
Evidence may be about the expected behavior directly, or about other evidence that may lead to it. For instance, in a situation where Alice expects Bob to be a good student, Alice will remember past situations when Bob was her student, or she will expect evidence about his interest to learn, attention in class, and respect with the
4 Evidence is “the available body of facts or information indicating whether a belief or proposition is true or valid”. Proof, verification, attestation, and support for, are synonyms of evidence. Source: Oxford Dictionaries. Accessed on February 4th 2017 at https://en.oxforddictionaries.com/thesaurus/evidence.

61
teacher. This leading evidence are important for new situations, i.e., situations that have not be experienced by them as trustor and trustee.
In new situations or if Alice and Bob did not know each other, evidence can be collected by provided recommendation by others, or a reputation score in an ecommerce platform, for example. In these cases, the trustor must be confident to use the source, generating an iterated trust assessment. If the trustor distrusts the source, the evidence may lose impact in trustworthiness computation (See input 2 in Figure 17). In the scope of this work, we will not incorporate evidence collected from thirdparties, recommending this as an evolution of the suitcase model. If it is not possible to collect an evidence from past situations, or from recommendations, then the trustor will count only on trustfulness to decide.
We proposed a formula to compute trustworthiness in terms of expected and collected evidence. Creating a list of expected evidence is a strategy to define the meaning of trust for the trustor in the specific situation, and to make trustworthiness assessment less subjective, and this is one of the main contributions of this work.
The trustor will assess how the list of collected evidence matches the list of expected evidence. The more complete is the match, the higher is the positive impact of the evidence on trustworthiness computation.
We modified the basic building block for confidence, Figure 11, to relate the evidence with trustworthiness, not to trust, to make clear the role of evidence in trustworthiness computation; and we added an input for expected evidence also (see input 1 and 7 in Figure 17). Control inputs and outputs were not changed.
Figure 17 - New basic building block for confidence, modified from (COFTA, 2007)

62

Alice’s assessment about Bob’s trustworthiness depends on the list of expected evidence, the list of collected evidence, and the situation’s components. See further details about the components in item 4.1.3. The situation’s components have impact on the selection of expected evidence. For instance, in an unfavorable context of a highly important situation, the list of expected evidence tends to demand more from the trustee.
The trustor must limit the number of expected evidence to be collected, because it is impossible to verify everything, and the expected behavior must be considered as an expected evidence, because in cases where there is no leading evidence, the expected behavior will be the only one to be collected.
An evidence may be collected from similar or different situations in past. Figure 18 shows a summary of Alice’s timeline of trust situations. The current situation (Situation 7) is preceded by other six trust situations. For instance, in June of 1995 (Situation 3), Alice expected that Charles could have capacity and interest to pilot a plane. She expected as evidence, in addition to the expected behavior, that he could have courage and quick reasoning. She collected as evidence that he had quick reasoning and courage, although after a behavioral decision, she decided to defect (D), as well as Charles (for unknown reasons). Table 7 shows a description of the notation used to cooperation between Alice and the trustees (Charles and Bob).

Table 7 - Cooperation notation in Alice's timeline of trust situations

Alice’s action Cooperates Cooperates Defects Defects

Trustee’s action Cooperates Defects Defects Cooperates

Notation C/C C/D D/D D/C

Figure 18 - Alice's timeline of trust situations

63

64
The conditions under which the evidence are collected has impact on trustworthiness computation. For instance, the distance of an evidence from present time has impact on trustworthiness computation, just like recurrence in time. For some people, evidence from long past situations are less valuable the ones from a recent past. In Figure 18, for example, ‘attention’ evidence from Situation 4 has less value than ‘attention’ from Situation 6. However, ‘attention’ is recurrent in time, and this may bring a positive impact to trustworthiness.
Evidence from different situations may lose importance in trustworthiness computation. For instance, ‘attention’ evidence from Situation 4 may have less value then ‘teach English’ and ‘patience’ evidence from Situation 2, once last situation is identical to the current one in terms of expected behavior and expected evidence (which reflects the context and the importance).
Another important factor from past situations is reciprocity. If the trustor distrusted the trustee in the last situation, any past evidence about the trustee may lose impact in current trustworthiness computation. The trustee may adopt voluntarily, or involuntarily, a ‘Tit-for-Tat’ strategy, when he will repeat the trustor’s action in last situation. If the trustor did not cooperate in the last situation, the trustee will not cooperate in next situation too. See item 5.1 and (AXELROD, 1984) for further details about ‘tit-for-tat’ strategy. This is not the case in Figure 18, since Alice cooperated with Bob in Situation6.
To deal with these conditions, we included attenuators of the collected evidence in the basic building block of confidence: 1) distance from present time, 2) recurrence, 3) reciprocity, and 4) different situation. In other words, evidence from a distant past, evidence with low recurrence, defection in the last situation, and evidence from a different situation will have their importance diminished for trustworthiness computation (Figure 19).

65
Figure 19 - Suitcase model version of the basic building block for confidence with attenuators, modified from (COFTA, 2007)
Trustor’s memory span () has an important role in attenuation, because it limits the number of situation remembered for each trustee. Trustor will remember the last  situations with the specified trustee. In Figure 18 (pg. 63), for example, if Alice has a memory span of 4, she could remember four trust situations with Bob (Situation 6, Situation 5, Situation 4, Situation 2). The Situation 1 will not be remembered because the memory span, and Situation 3 will not be remembered because concerns to other trustee (Charles).
Attenuators are calibrated by trustor’s trustfulness. For instance, if Alice in Figure 18 has a low trustfulness, she tends to ignore the ‘attention’ evidence from Situation 4, because it comes from a different situation, and the ‘patience’ evidence from Situation 2 will lose value for her, because it occurred a long time ago. However, a high trustfulness trustor would tend to ignore these conditions and consider both pieces of evidence in trustworthiness computation.
4.1.5 Suitcase model formalization
Once we defined the components of the suitcase model (Figure 14, pg. 57) and how they are related with the components that produce the confident behavior (Figure 15, pg. 58), we present a formalization of our suitcase model (Table 8), which is based on (COFTA, 2007) formalism (Table 6, pg. 53).

66

Table 8 – Components of the suitcase model formalism

Name

Notation

Description

Actor



 = {, , … , 1, … }

Importance



 =  |  | ℎℎ

Context



 =  |  | 

Subject

subject

 =   ℎ   ;  = {1, … , }

Expected Behavior 

 =    ℎ

Situation



Piece of Evidence

,

Collected Evidence



Collected Evidence Value

̂

 =       ,  ,    ℎ;  = (, , );  = {1, … , } |  =< 
, =             ; , = (, )
 =    = {1, … , } |  =<  ;  =          ℎℎ      ;
 = {,1, … , ,} | ,  
̂ =          ;       all
Piece of Evidence related to ℎ ;

Expected Evidence



 = {1, … , }

67 The Figure 20 shows a graphical summary of the formalism components.

Figure 20 - Graphical relation among the components of the suitcase model

The new suitcase model computes trustfulness as a subjective probability, whereas trustworthiness is a probability computed from expected and collected evidence. Therefore, trust is defined as a sum of these constructors:

 → (, , ) =  +  → (, , )

(11)

where:

•  = ’    •  → (, , ) = ’   ’ ℎ •  =    ℎ      •  = {1, … , } =
        •  = {1, … , } =          o  = {,1, … , ,} =        

68

Figure 21 - Alice's timeline of trust situations marked with the suitcase model formalism

69
In the case of Figure 21 (pg. 68), considering that the current situation is  = 7 =  7, the memory span is 4, and the trustfulness if 0.45 (realistic), these variables have the following values:
•  = ′ℎ ℎ′ •  = ′′ •  = ′′ •  = 7 = (, , ) = (′′, ′′, ′ℎℎ′) •  = {1, 2, 3} = {′ℎ ℎ′, ′′, ′′} •  = {1, … , } = {1, 2, 3} o 1 = {1,2} = {(′ℎ ℎ′, 2)} o 2 = {2,2} = {(′′, 2)} o 3 = {3,6, 3,4, 3,2} = {(′′, 6), (′′, 4), (′′, 2)} o 3,6 = {(′′, 6} o 3,4 = {(′′, 4} o 3,2 = {(′′, 2}
The convex combination of trustfulness and trustworthiness (eq. 11) will result in a probability value between 0 and 1. This sum may be weighted, although in this work both values have the same weight (0.5). Trustfulness is also a value between 0 and 1, as well as the trustworthiness value.
Eq. 12 computes trustworthiness as a sum of all Collected Evidence Values (̂), divided by the cardinality of the set of Expected Evidence (||). The division brings to computation the match between the expected and collected evidence lists.
The Collected Evidence Value (̂) is a numerical representation of the Collected Evidence (). Since the Pieces of Evidence (,) of an Evidence () were collected their values are ‘1’. It means that the trustor can rely on this Piece of Evidence to trust the trustee. Considering that the Collected Evidence Value (̂) is a mean of all Pieces of Evidence collected, the value of ̂ is also ‘1’ by default.
However, the Piece of Evidence, and consequently the Evidence, might be attenuated to reflect the conditions under which they were collected. With this purpose, we apply four attenuators:

70

1. a ‘reciprocity’ attenuator (recip) applied to the mean of all Collected Evidence Values, to reflect a defection from trustor in a previous situation, which must compromise any Collected Evidence (eq. 12);

2. a ‘recurrence’ attenuator (recur) applied to the mean of all Pieces of Evidence, to reflect a low recurrence of an Evidence (eq. 13);

3. a ‘distance’ attenuator (dist) applied to the Piece of Evidence to reflect how distant is the situation from the present (eq. 13);

4. ‘different situation’ attenuator (diff_sit) applied to the Piece of Evidence that comes from a different situation than the current one (eq. 13);

The trustworthiness computation is done by:

 → (, , ) =

 → ∗

|| ∑=0

̂ 

||

(12)

where:

• → −  ;      , → = 1;  ,    Table 9
• || =   ℎ   • ̂ =    of a Collected Evidence ()
Collected Evidence of a subject  () may contain  Pieces of Evidence (,) obtained from  situations  |  = 1  

•  = {,1, … , ,} = evidence related to  collected from  situations

In the case of Figure 21 (pg. 68), these variables have the following values:

71

•  = {1, … , } = {1, 2, 3 } → || = 3 •  = {1, … , } = {1, 2, 3} → || = 3 • 1 = {1,2} = {(′ℎ ℎ′, 2)} • 2 = {2,2} = {(′′, 2)} • 3 = {3,6, 3,4, 3,2} = {(′′, 6), (′′, 4), (′′, 2)} o 3,6 = {(′′, 6} o 3,4 = {(′′, 4} o 3,2 = {(′′, 2}

The Collected Evidence Value (̂) of a Collected Evidence () is done by:

̂

=  ∗

∑|=1| ,
(

∗

_ ,  , )

||

(13)

where:

• , = 1 = trustor can rely on this    to trust the trustee • || = cardinality of   •  = recurrence attenuator; if the subject  appears in all past
situation with Bob, then  = 1, if not,  will follow Table 9 • _, = different situation attenuator for the  ,; if the
   comes from an equal situation, then , = 1, if not,  will follow Table 9
• , = distance attenuator for the  ,; if the    comes from the last situation, then , = 1, if not, , will follow Table 9

In the case of Figure 21 (pg. 68), these variables have the following values:

• 1 n = 1 |1| = 1

72
• 1,2 = 1 • 2 n = 2 |2| = 1 • 2,2 = 1 • 4 n = 3 |3| = 3 • 3,6 = 3,4 = 3,2 = 1
Once we defined how the evidence are computed in trustworthiness, and how they are attenuated to reflect the conditions under which they were collected, the next item present our strategy to calibrate the attenuators.
Attenuators calibration
Attenuators in the suitcase model are calibrated by trustfulness value. Table 9 presents the calibration strategy adopted for attenuators using three categories of trustfulness values: optimistic, realistic, and pessimistic. Marsh proposed these categorical values for agents disposition to compute the Estimated General Trust (MARSH, 1994).
In the suitcase model, the optimistic agent is defined as an agent with high value of trustfulness, whereas the pessimistic has a low value. Realistic agents have a medium value of trustfulness. The range of values of these categories are subjective, such as the trustfulness value. The range must be set, by a subjective assessment, to reflect the distribution of optimistic, realistic, and pessimistic agents in the simulation.
For our analysis, we considered an equal distribution among the categories. Trustors with low values, from 0 to less than 0.33, tend to have a pessimistic assessment over an evidence, whereas high values of trustfulness, from more than 0.66 to 1 make them more optimistic to accept an evidence. The values out of these ranges reflect a realistic view of evidence.

73

Table 9 - Attenuators calibration by trustfulness range of values

Range of trustfulness values

Attenuators

Notation Pessimistic

Realistic

Optimistic

Distance from present
Evidence recurrence
Evidence from different situation Reciprocity for defection in last situation

dist recur diff_sit recip

2 | | 2 trustf
trustf

 | |  trustf
trustf

1 1 trustf trustf

The distance of an evidence from present () is defined as the number of past situations between the evidence situation and the present, including the referred situation. Those situations must concern only the trustor and the trustee under consideration. Trustors with a pessimistic trustfulness tend to consider this distance as much greater than it really is. On the other hand, high level of trustfulness overlaps this distance. Situation results (cooperation or defection) is irrelevant in this case.
For example, in Figure 21 (pg. 68), Alice is a Realistic agent, the attenuators have the following values:

• 1,2 =  = 6 ∗ 1 + 5 ∗ 1 + 4 ∗ 1 + 3 ∗ 0 (ℎ) + 2 ∗ 1 = 4 • 2,2 =  = 6 ∗ 1 + 5 ∗ 1 + 4 ∗ 1 + 3 ∗ 0 (ℎ) + 2 ∗ 1 = 4 • 3,6 =  = 6 ∗ 1 = 1 • 3,4 =  = 6 ∗ 1 + 5 ∗ 1 + 4 ∗ 1 = 3 • 3,2 =  = 6 ∗ 1 + 5 ∗ 1 + 4 ∗ 1 + 3 ∗ 0 (ℎ) + 2 ∗ 1 = 4

Recurrence of an evidence has similar calibration to distance from present. It is defined as the number of appearance of an evidence, considering all piece of evidence remembered from the trustee (total), including those not collected. Situation results is also irrelevant in this case.
In the case of Figure 21 (pg. 68), the variable total will be computed as follows:

74

•  = 5 →  = |1| + |2| + |3| = 1 + 1 + 3 = 5

•

1,2 =

|1| = 1/5 = 0.2


•

2,2 =

|2| = 1/5 = 0.2


•

3,6 =

|3| = 3/5 = 0.6


•

3,4

=

|3| 

=

3/5

=

0.6

•

3,2 =

|3| = 3/5 = 0.6


Although the ‘attention’ evidence appears three times (in Situation 2, Situation 4, and Situation 6), the recurrence will be computed as 2/3, because only Situation 6, Situation 5, and Situation 4 fit in memory, where ‘attention’ appears only two times. If memory span is 4, then Situation 2 is considered again, once Situation 3 is related to another trustee.
Calibration for evidence from different situations (diff_sit) and reciprocity (recip) are done directly using the trustfulness value (trustf).

Confidence computation

To simulate effectively the suitcase model, we also needed to formalize the other components that produce the cooperation action (Figure 15, pg. 58). For that we present the confidence computation extracted from Cofta (eq. 8), but modified to incorporate the suitcase model (eq. 11) by defining confidence as an convex combination of trust and control, and to define control as a multiplication of evidence from instruments of control and the confidence in these instruments, resulting in:

→ =  → (, ,  ) + ∑=1 →(|) ∗ → (14)
We change the original notation for evidence of control to . Confidence is a probability value between 0 and 1, and trust and control are weighted, although in this work both values have the same weight (0.5).
As mentioned before, although control can be modeled as a function of

75

expected and collected evidence, in this work we simplify control summation as a subjective probability, because trust is the focus of the new model.
Once we defined our suitcase model, we needed to create a procedure to decode real life trust situations in the components of our model.
4.2 Defining a trust situation

To manage trust under a suitcase word perspective, we need to define the meanings of trust in terms of the components of the suitcase model. We proposed a form (Table 10) to guide this definition.

Table 10 – Guide form to define trust in terms of the components of the suitcase model

Model components Situation Actors
• Trustor o name o trustfulness
• Trustee’s name Expected behavior Expected evidence
• evidence 1 • evidence 2 •… • evidence n Context Importance Memory span

Types of values text
string optimistic, realistic, or pessimistic string string
string string string string ‘favorable’, ‘neutral, or ‘unfavorable’ ‘high’, ‘medium’, or ‘low’ integer number

Table 11 shows an example of a trust definition from a hypothetic situation.

76

Table 11 – Example of definition from a trust situation.

Components
Situation
Actors • Trustor o name o trustfulness • Trustee’s name
Expected behavior Expected evidence
• evidence 1 • evidence 2 • evidence 3 • evidence 4 Context Importance Memory span

Types of Values Alice believes that Bob will be a good teacher of mathematics. The expected behavior is highly important, because she has a final exam in two days, and she needs to learn fast. The context in unfavorable, because she did not attend the mathematics classes in the last 2 months
Alice realistic Bob be a good teacher of Math
Good teacher Knowledge in mathematics Teaching method Patience unfavorable high 10

4.3 A generalized model of behavioral decision
Our generalized model of behavioral decision is based on (MARSH, 1994), and its main goal is to reproduce the ‘behavioral decision’ component in Figure 15 (pg. 58), which might produce a confident behavior, and a cooperation action, consequently.
We started by adapting eq. 1 to carry the idea that the ‘confident behavior’ is produced by confidence (Figure 15, pg. 58):
→ > _ℎℎ  () ⟹ _ℎ(, , ) (15)
As a consequence of this change, the cooperation threshold became a confidence threshold. In the original threshold (eq. 4), the perceived risk was mitigated

77

by the perceived competence and the estimated general trust (EGT) together; and the importance affected directly the risk.
Perceived competence and EGT might be considered as measures of trustee’s capacity and interest to behave such as expected, respectively, so we replaced them by our confidence value, which has the same goal: estimate the trustee’s capacity and interest to behave such as expect. This change also brings control as an additional option to mitigate the risk (eq. 16).

∗ ℎℎ  () =

() → 

()+() 2

(16)

Considering eq. 15 and eq. 16, the trustor will produce a confident behavior if trustor’s confidence in Bob to behave such as expected is greater than the risk multiplied by the importance and mitigated by the confidence in Bob.
At the end, the eq. 15 can be rewritten as:

(→

2
)

>

()

∗

()+() 2

⟹

_ℎ(,

,

 )

(17)

4.4 Considerations about modeling trust under a suitcase perspective

Getting evidence that trust is a suitcase word was the first step to model trust under this perspective. After that, we guided our review under this perspective to collect conceptual foundations, definitions and models of trust from scientific literature.
That knowledge supported our model, the suitcase model, which is a result of trustor’s trustfulness and trustee’s trustworthiness (TULLBERG, 2007). Considering that trust works together with control to improve confidence (COFTA, 2007), and that a confident behavior allows engaging in a relationship, we also proposed a generalized model of behavioral decision of confidence. This decision model was based on (MARSH, 1994) implementation. This complementary model was required to make our suitcase model workable, since trust alone could not produce a confident behavior to

78
promote cooperation. We also proposed a less subjective method to measure trustworthiness in terms
of expected and collected evidence. Creating a list of expected evidence was a strategy to identify which meanings of trust are being considered to computation. It would be difficult to find evidence for trustworthiness if we do not know what is important for the trustor in a situation. Besides that, a finite list of terms may reduce the complexity of analysis, once we have to measure only the most important evidence for each situation (important in trustor’s eyes).
The list of expected evidence might be considered an over simplification of the trustworthiness assessment, since we, as humans, are able to remember and process a more elaborated list of evidence. However, our proposed list does not have a limit, and its size can be adapted to each situation. We intended to keep the model workable allowing it to be used in trust management by nonscientists users.
To bring the suitcase model even closer to real-life, attenuators were proposed to capture the conditions in which the evidence was collected. It is possible to refine this idea by identifying and including other kinds of attenuators; however, this might increase the complexity of trustworthiness assessment, and might reduce its workability, since it could increase the information to be retrieved. Evidence and instruments of control were considered as subjective probabilities in this work. Although it is also possible to measure control in terms of expected and collected evidence of control, in this work, we decided to focus in trust.
Our model of trust under a suitcase perspective was designed to be more comprehensive and workable as possible. We proposed also a procedure to define a real trust situation in the components of this suitcase model, and we presented a behavioral decision model of confidence, which completes our framework to manage trust, making it possible to create a simulator for the suitcase model.

79

5 Simulating the suitcase model
This chapter presents a simulation of our suitcase model built as a modified version of the PlayGround simulator (MARSH, 1994). We start this chapter with a presentation of the Iterated Prisoner’s Dilemma, which is a central concept for the original PlayGround, and then we present a replica of the PlayGround developed for Internet version. Finally, we show how the suitcase model was incorporated to this replica to create the PlayGround 2.0.
5.1 The Iterated Prisoner's Dilemma

The Prisoner's Dilemma (AXELROD, 1980) is a well-known two-players game studied by social sciences, business, computer science, and other fields of science. The game takes place in a prison, where two prisoners will be charged in 5 years of reclusion. At a certain moment, both receive an offer to testify against the other prisoner in return of a reward. The offer is done individually, not being allowed the communication between them. Both players (prisoners) have to decide either to cooperate with one each other (remaining in silent) or to defect (incriminating the other player).
Table 12 gives the canonical matrix that represents the payoff considering both decisions, and also gives the original scoring adopted in (AXELROD, 1980). If both players decide to cooperate, they receive a Reward (R) with 3 years of payoff and go to prison for 2 years. If one player defects whereas the other player cooperates, the defector will set free, receiving a Temptation (T) payoff of 5 years, and the other player will serve 5 years in prison (with a Sucker’s payoff of zero). Finally, if both players defect, both will serve 4 years in prison, receiving a Punishment (P) payoff of 1 year.

Table 12 - Payoff Matrix of Prisoner’s Dilemma, extracted from (AXELROD, 1980)

Cooperate

Defect

Cooperate

Defect

Cooperate

R, R

S, T

Cooperate

3, 3

0, 5

Defect

T, S

P, P

Defect

5, 0

1, 1

80
The condition T > R > P > S must be respected to assure the incentive of both sides to be selfish (individual rationality), and to assure the higher payoff to both sides for mutual cooperation over mutual defection (group rationality) (AXELROD, 1980).
If two players play the Prisoner’s Dilemma more than once in succession, they can remember their play, and they aiming to maximize their total payoffs changing strategies, then we have an Iterated Prisoner’s Dilemma (IPD) game (AXELROD, 1980). In IPD game, the basic Prisoner’s Dilemma is a game stage in a repeated game. Maximizing the total payoff is the central point of the IPD.
Many strategies have been proposed to achieve this goal. For instance, using the ‘Tit for Tat’, one of the most successful strategy (AXELROD, 1984), a player will first cooperate, and after that he will always replicate the opponent's previous action. If the opponent had been cooperative, the agent is cooperative; if not, the agent will not cooperate until a cooperation action from the opponent. The ‘Alternator’ player strategy will alternate between cooperating and defecting ignoring the counterpart actions. Tournaments were create to determine the most successful strategy (AXELROD, 1980, 1984).
In Axelrod’s Tournament, players propose their own strategy and the winner player (prisoner) achieves the lesser year of imprisonment at the end. Libraries are available to run and develop these tournaments in modern languages such as Python (KNIGHT, 2015).
An Axelrod’s Tournament environment (Figure 22) is composed by Players, who Play against each other in a series of Turns driven by a Strategy. The strategy is a set of instructions that produce an Action (Cooperation (C) or Defection (D)) for each turn. The possible outcomes for each turn are: Mutual cooperation (C, C), Defection (C, D) or (D, C), and Mutual defection (D, D).
A Match is a consecutive number of turns between two specific players, and a Win is attributed to the player who has the higher total score at the end of a match. All matches between all players, ignoring the order of assignment, is a Round Robin. A Tournament is a predetermined number of Round Robins.

81
Figure 22 - Axelrod’s Tournament environment
5.2 Replicating the original PlayGround
In this section, we present the replication of the original playground. We started by describing the original implementation, then we listed some assumptions about operation details, and finally discussed the replication results.
5.2.1 The original PlayGround
(MARSH, 1994) developed two implementations of his formalism. The first considered trust as a strategy in an Axelrod’s Tournament and proved the feasibility of the formalism. The second, called the PlayGround, implemented the formalism more completely, by introducing some additional features to the tournament concept, aiming to create an environment closer to the real world. The PlayGround was a graphical application (Figure 23) developed in HyperCard that simulated interactions among agents.

82
Figure 23 - A screenshot of the PlayGround, extracted from (MARSH, 1994) Agents moved around a cellular grid. An interaction occurred when an agent tried to move to a place that was already occupied. If occupied, agents played a round of the Iterated Prisoner’s Dilemmas game in which they ‘Cooperate’ or ‘Defect’. The payoff matrix varied according to the situation assigned to the interaction, which was randomly assigned, or defined as the same for all interactions. A generic payoff (T=>5, R=>3, P=>1, and S=>0) was used for most experiments. At the end of all movements, the agent with highest score won. The follow items are important details extracted from (MARSH, 1994) about PlayGround operation which were considered essential to the replication. Cycle of operation The code bellow extracted from (MARSH, 1994) represents the full cycle of the PlayGround:

83

repeat for each agent do begin move if another agent ‘met’ then interact adjust trust values if necessary save results end if end
until maximum number of moves reached

Trusting agent

Like in the tournament, agents have strategies (Random, Cooperator, Defector, Tit for Tat, and others) to decide to ‘Cooperate’ or to ‘Defect’. Marsh introduced a new strategy, which implements the proposed formalism for trust. These ‘Trusting’ agents consider ‘trust’ to act and may use it to move. Their memory is limited, so that only “x” interactions for each opponent can be considered to decide the actions.

Disposition to Trust

As described on item 3.9, the Estimated General Trust depends on trustor’s Disposition to Trust as follow (Table 13):

Table 13 - How Estimated General Trust is calculated

Disposition to trust

How Estimated General Trust is calculated

Optimistic Realistic Pessimistic

Use the maximum value of all General Trust past values Use the average of General Trust values in similar situations (eq. 3)
Use the minimum value of all General Trust past values

84
The historic values assessment is limited by trustor memory.
Directed movement
The direction of movement for all agents in the field is chosen randomly among North, South, West, and East, except for the trusting agent, which is able to choose a movement toward a trustworthy agent. This directed movement was designed to reflect human behavior, in which an individual usually has access to a circle of trustworthy people allowing agents to form groups, and also to reflect external factors that make impossible the desired movement. A group is a cluster of neighbor agents in the grid.
This choice is limited by a configurable ‘range of vision’, equal for all directions, for example, two positions in each direction. However, although the trusting agent can choose the direction, not necessarily he can take it. Depending on his set up, he can: (1) choose his movement, (2) receive a random movement, or (3) influence the movement. This last option is a mix of the previous two, which uses a weighting of probabilities to determine the influenced direction: 50% for desired direction, 10% to opposite direction, and 20% for the others. Which means that he may move to an undesired direction. In both cases, random or influenced movements, an interaction can occur between the same agents on next round.
Trust adjustments
Other feature in which the PlayGround approximated real life was the adjustment of trust values after an interaction. These adjustments apply to Basic Trust, General Trust, and Perceived Competence. After each interaction, adjustments are done in trust values for both trusting agents. In general, Marsh proposes standard variations in which trust increase if other cooperates and decreases if not. For instance, after an interaction between Alice and Bob, from an Alice perspective as a trustor, if Alice cooperates and Bob defects, Alice’s Basic Trust will decrease 1%, her General Trust on Bob will decrease 10%, and her Perceived Competence about Bob will decrease 1%. Table 14 show details about all trust values adjustments after an interaction.

85

Table 14 - Trust values adjustments after an interaction between Alice and Bob

After an interaction between Alice and Bob
where

Alice’s Basic Trust

Alice’s General Trust
on Bob

Alice’s Perceived Competence about Bob

Both Cooperate

+1%

+10%

+1%

Alice Cooperates and Bob Defects

-1%

-10%

-1%

Alice Defects and Bob Cooperates

+5%

+1%

+1%

Both Defect

-5%

-10%

-1%

Experiments

The first experiment run by Marsh wanted to test if trust could be educated. Divided in two parts, the first one forced two Realistic agents to play a IPD. They had an unbound memory and they could not move around the grid. The initial values of A were extremely low and set up to distrust B (defect); whereas B was inclined to cooperate. After one hundred interactions, agent A seemed to move slowly to a cooperative behavior, whereas B always cooperated. Considering that A was a Realistic agent with unbound memory, all past interactions were composing the mean for its Estimated General Trust in B, so that if Basic trust values were increased due to B’s cooperation, the mean of all values pushed A to a slow movement toward cooperation.
In part II of this experiment, A became C, an Optimistic agent, and B became D, with the exactly same characteristic and trust values. Memory of both agents remains unbounded. After only a dozen interactions, C chose to cooperate with D. Some assumptions were done by Marsh, among which the difference of learning was done by the way the Realistic and Optimistic agents compute the Estimated General Trust. It seems that the mean, in the first case, held a slow learning, whereas a selection of highest past value showed a faster result. Tables with detailed outputs extracted from the original work can be found in Appendix E.
In fact, Experiment A was essential and enough to support our replication. Experiments B and C realized tests to determine memory span effects in the decision. Sequential experiments tested the influence of other strategies, movement style,

86
evolution, and number of interactions, especially in group formation. For instance, experiment F Part II tested the grouping behavior among 21 agents (6 Random, 5 Optimistic, 5 Pessimistic, and 5 Realistic) in the situation a, with directed movement and range of vision 2, after 250 interactions. See further details about these experiments in (MARSH, 1994).
5.2.2 Assumptions about original implementation
Implementation information was collect in (MARSH, 1994). However, it was not enough to a successful replication of original simulator. Listed below are some important factors about the implementation that were assumed by us:
• Memory Span: If an agent has a memory span of 4, and in his life this agent had 21 interactions equally distributed with 3 different agents, the trustor will remember at maximum 12 interactions.
• Grid definition: The field is ‘flat’, which means that the boundaries are well defined. An agent cannot move to East if he is on the eastern most position of the field. He will remain in the same position. However, we decide to allow configuring a wrapped field.
• Directed movement 1: when deciding a directed movement, the trusting agent will consider only the first agent that he sees on a direction. If the agent sees two neighbors in the North, he will consider the closest one to decide his movement, even if the other is more trustworthy.
• Directed Movement 2: The trusting agent will consider only the first (and closest) agent that he sees in a direction to choose a movement. A simulated interaction using the generic situation is done with existent neighbors in four directions. The greatest difference between the Situational Trust and the Cooperation Threshold (ST-CT), even negative values, will define the chosen movement. In case of absence of neighbors, the movement will be random.

87
• Interaction: When two trusting agents interact, both play the roles of trustor and trustee for the exactly same situation, because both need trust to decide the action, so that in a unique interaction there are two plays (“A trust B” and “B trust A”) to the same situation. Trust values are adjusted for trustors and trustees after each play.
• Trust adjustments: For a Realistic agent, before calculating Situational Trust, a mean of past General Trust (GT) is saved as a new GT value in memory to use in future calculations. After each interaction, the adjusted value from the mean will be saved as another new GT value. This is not true for Optimistic and Pessimistic agents, for whom only the GT originated from adjustments are saved as new values.
• Values: Although they had formulae, or suggestions of calculations, Importance, Utility, Risk, Costs, and Perceived Competence values did not have their origin detailed in (MARSH, 1994). In addition, it was only possible to get initial values for Alice and Bob. Other agents did not their initial values known. Therefore, we decided to use random values between Alice and Bob’s lowest and highest values to initialize other agents. We adopted the same strategy for Basic Trust. General Trust started with null values. Zero and one (1) values cannot be accepted for Competence, Control and Perceived Competence values.
5.2.3 Replication results
The original PlayGround replica for Internet was implemented in PHP and it is available online at (CARTOLANO, 2016). The first goal of the new simulator was replicate 1) Experiment A Part II, 2) Experiment F Part I, and 3) Experiment F part II. Output tables extracted from (MARSH, 1994) were essential during development to propose and test the assumptions described above. Without these outputs, we could not replicate the original simulator.
Our result for Experiment A Part II was the same as Marsh’s; the optimistic agent C started to cooperate at the 13th interaction. However, we found unexpected

88

output values for agent C. Adjustment for Basic and General Trust values appears to be reversed until agent C starts to cooperate as discussed in (CARTOLANO; SARAIVA; STEVENSON, 2016). Table 15 shows how this inversion produced a different output from what was expected according to Table 14 or (MARSH, 1994). We named columns from A to F to facilitate identification.

Table 15 - Difference between Marsh's and expected output for agent C

Expected output values for agent C

A # Basic 13 0.143668

B General 0.090146

C Coop. Thresh
0.572641

12 0.136827 0.089253

0.600133

11 0.130311 0.088369

0.628908

10 0.124106 0.087494

0.659009

9 0.118196 0.086628

0.690498

8 0.112568 0.085770

0.723436

7 0.107208 0.084921

0.757881

6 0.102103 0.084080

0.793899

5 0.097241 4 0.092610

0.083248 0.082424

0.831561 0.870933

3 0.088200 0.081608

0.912075

2 0.084000 0.080800

0.955076

1 0.080000 0.080000

1.000000

Marsh’s output values for agent C

D Basic 0.090147

E General 0.143666

F Coop. Thresh
0.572918

0.089253

0.136827

0.600438

0.088369

0.130311

0.629233

0.087494

0.124106

0.659361

0.086628

0.118196

0.690885

0.085770

0.112568

0.723855

0.084921

0.107208

0.758334

0.084080

0.102103

0.794395

0.083248 0.082424

0.097241 0.092610

0.832097 0.871504

0.081608

0.088200

0.912702

0.080800

0.084000

0.955752

0.080000

0.080000

1

Considering that agent C defected and agent D cooperated until interaction number 12 (included), it was expected that agent C’s Basic Trust should be increased by +5% and General Trust by 1% after each interaction until both cooperate. In this case it was expected that Basic Trust A1(0.080000) would be A2(0.080000 * 1.05 = 0.084000), General Trust B1(0.080000) would be B2(0.080000 * 1.01 = 0.080800), and Cooperation Threshold would be C2(0.955076) according to the equations on item 3.9.

89

However, Marsh’s output shows that Basic Trust D1(0.080000) was adjusted +1% resulting in D2(0.080000 * 1.01 = 0.080800) and the General Trust E1(0.080000) was adjusted by +5% resulting in E2(0.080000 * 1.05 = 0.084000). This produced an inverted result for General and Basic Trust which directly impacted the other columns Situational Trust and Cooperation Threshold.
We considered this inversion in the original output unexpected because there are no reasons for that described in (MARSH, 1994), and because after cooperation (13º interaction) adjustments became in line with expected. Despite this, both Marsh’s original and replication output produced a Situational Trust value greater than Cooperation Threshold after 12º interaction.
Table 16 shows our output for agent C in Experiment A Part II replication extracted from (CARTOLANO; SARAIVA; STEVENSON, 2016). Table 25 and Table 26 in Appendix E show the output extracted for agent C in (MARSH, 1994), whereas Table 27 and Table 28 show output for agent D.
We obtained the exact same values of the original output for agent D.

Table 16 - Agent C partial results of Experiment A Part II replication

# Basic
13 0.143668 12 0.136827 11 0.130311 10 0.124106 9 0.118196 8 0.112568 7 0.107208 6 0.102103 5 0.097241 4 0.092610 3 0.088200 2 0.084000 1 0.080000

General Competence Situation

0.090146 0.089253 0.088369 0.087494 0.086628 0.085770 0.084921 0.084080 0.083248 0.082424 0.081608 0.080800 0.080000

0.007212 0.007141 0.007070 0.007000 0.006931 0.006862 0.006794 0.006727 0.006660 0.006594 0.006529 0.006464 0.006400

0.620646 0.591093 0.562944 0.536138 0.510607 0.486294 0.463139 0.441085 0.420081 0.400075 0.381024 0.362880 0.345600

Threshold Action

0.572641

C

0.600133

D

0.628908

D

0.659009

D

0.690498

D

0.723436

D

0.757881

D

0.793899

D

0.831561

D

0.870933

D

0.912075

D

0.955076

D

1.000000

D

90
In a further effort to validate Marsh’s simulations, we reran his experiment F Part I on the speed and efficiency in group formation. The grid was populated with 6 Random, red, and 5 Optimistic, green, Realistic, yellow, and Pessimistic, gray agents. Agents were configured to move directed toward trustworthy agents using a range of vision of 2 cells. Initial random values were generated for Competence, Importance, Risk, and Basic Trust, considering the range of values from Experiment A part II. Because the initial conditions are generated randomly, and the direct movement has a random component, it is not possible to duplicate Marsh’s results exactly. However, after being assigned randomly on the grid at the beginning of the experiment (Figure 24), agents were also grouped after 250 interactions (Figure 25) like in original experiment.

Figure 24 - Initial agent's distribution in experiment F part I replication

Figure 25 - Final agent's distribution in experiment F part I replication

Table 17 presents the top and bottom five agents that held highest and lowest scores after 250 and 10,000 movements. Our results are different from Marsh’s results, because none of our Random agents appears among first agents. Instead, Random agents are among the last in rank.

91

Table 17 - Top and bottom agents after 250 and 10,000 movements

After 250 Movements

Agent

Score # Int

1

P

Maria

456

143

2 R Hardin 408

144

3R

Kyle

354

123

4R

Ian

339

134

5O

Fred

321

134

17 O Charles 214

127

18 RD Silva

201

65

19 RD Peter

199

81

20 RD Tom

171

84

21 RD Zoom 139

60

After 10,000 Movements

Agent

Score # Int

R

Bob 13,340 5,579

O Charles 13,243 5,669

O Etienne 13,220 5,507

P

Neo 12,848 5,211

P

Oddin 12,567 5,129

RD Silva 6,650 2,822

RD Rafa 6,242 2,777 RD Peter 6,218 2,788

RD

Tom 5,938 2,767

RD Zoom 5,669 2,667

This result is related to the number of interactions that agents have during their movements. An agent may need one or more movements to interact whit other agents. Trusting agents were more efficient in terms of interactions per movements, once they tend to direct their movement to more trustworthy agents.
After replicating the experiments, we proposed a new test for the original model by analyzing successful cooperation in time. Initial random values were saved from Experience F Part I and used in five rounds. Optimistic, Realistic, Pessimistic, and Random values in Figure 26 are means of the values of their agents in the five rounds. When simulations are done until 10,000 movements, clear differences among the strategies emerge. Optimistic and realistic agents have a mean of 71% and 60% of successful cooperation among all interactions and they increase successful cooperation consistently, while the pessimistic and random agents have lower means and show decreasing percentage of successful cooperation.

92
Figure 26 - Percentage of successful cooperation in time for trusting agents
5.3 PlayGround 2.0 - the suitcase version
We present in this item the PlayGround 2.0 (Figure 27), which was created using the Internet replica presented before with minimal changes. We implemented the suitcase model on it as a new strategy to players, the ‘Suitcase Trustor’. The agents that adopt this strategy are the ‘suitcase agents’.
Figure 27 - PlayGround 2.0 screenshot

93
We did not change the original ‘Trustor’ strategy from (MARSH, 1994); so that the trustor agent and the suitcase agent could play together in the field, in companion with the ‘Cooperator’, ‘Defector’, and ‘Random’ agents. As well in the original simulator, the winner strategy in the PlayGround 2.0 achieves the highest score at the end of all movements.
General features of version 2.0
The concept of agents on the field, moving around, and playing when meet each other, is the same of the Internet replica, and the cycle of operation, described in item 5.2.1, is followed systematically.
The suitcase agent has the same ‘field’ features of its predecessor. It has a range of vision in the field and a memory span, and may decide or influence its movements according to a trust assessment of neighbors.
An important evolution is the random assignment of situations during the cycle of operation. Until reach their maximum number of moves, both trusting agents will be confronted with different situations, but only the suitcase agents will decide according to the different meanings of trust presented.
Main changes
The main difference between both trusting agents are related to the trust decision and the behavioral decision that leads to cooperation.
Although the results are possibly the same (‘cooperates’ or ‘defects’), the trustor agent decides to cooperate using trust (eq. 1), whereas the suitcase agent uses confidence (eq. 15). This is an important difference, since confidence is a wider concept that incorporates control to enforce the expected behavior (eq. 14).
Furthermore, the suitcase agent considers trust as result of the trustor’s trustfulness and the trustee’s trustworthiness values (eq. 11). By its turn, the trustfulness is computed in terms of expected collected evidence for each situation (eq.12), and this allow the trustor have a different understood about the many meanings of trust.

94
Simulator configuration
Users must previously decide the area of the grid and the number of agents in the field. After create the grid and spread the agents on it, users can configure the number of movements in each cycle, and if these movements are directed, random, or partially random. The rank of agent by score is updated after each cycle.
In terms of initial configuration of the suitcase agent, it is required to define their types, and their importance, context, and their list of expected evidence for each situation. When choosing a type among optimistic, realistic, or pessimistic suitcase agent, a random value is assigned to trustfulness considering a pre-defined range: pessimistic, from 0 to less than 0.33, realistic, from more than 0.33 to 0.66, and optimistic, from more than 0.66 to 1. These ranges were chose assuming an equal distribution of types in the population (see item 4.1.5).
The Importance and Context categories are translated to values in the simulator. The high level of Importance and the unfavorable context are translated to 0.75. The medium level and the neutral context are translated to 0.5, and finally, the low level and the favorable context are 0.25.
The differences of initial configurations from previous simulator required a minimal change on the interface.
List of expected evidence
The list of expected evidence is central for the operation of the ‘Suitcase Trustor’ strategy, because it is used as a filter to collect evidence from past situations. Before starts an interaction, the suitcase agent recovers in memory, i.e., remembers, all past situations with the trustee, limited by the memory span, then their pieces of evidence remembered are filtered using the list of expected evidence. When a piece of evidence is collected, some additional information about the situation (expected behavior, importance, context, and trustor’s action) are collected too.
Figure 28 presents a relation between these entities in the ‘Suitcase trustor’ strategy.

95
Figure 28 – Relation between entities in the ‘Suitcase Trustor’ strategy of the PlayGround 2.0.
Attenuation As a final main aspect of the ‘Suitcase Trustor’ we defined the strategy to attenuate the collected evidence. By considering that the role of these attenuators is diminish the importance of a piece of evidence if necessary, to reflect the conditions in which it was collected, we defined the value of a piece of evidence as ‘1’ by default. This means that under the best condition the trustor can rely on the piece to trust the trustee. However, in different conditions the value will be attenuated.
5.4 Considerations about the model simulator
The main goal of this chapter was to present how our simulator was developed and some its details. We decided to use the original PlayGround as a basis to our simulator, so that we could make use of a known structure to run our model.
We had to replicate the original tool in an Internet version, because the code was no longer accessible, which demanded from us some time to understand the deeper details of the PlayGround, including make assumptions about some operational details, once the information was not find in the original text, and finding some discrepancy in the results of the original work. However, this replication effort was essential to understand the model and adapt it to the suitcase model, and to modify the original simulator to create the PlayGround v2.0.

96
6 Case study in Citizen Science
The main goal of this case study was to test the hypothesis that trust can be simulated under a suitcase perspective with potential to handle different meanings in Citizen Science projects (item 1.4, pg. 24). We did not want to compare strategies or to analyze actors grouping aspects in this work. A successful simulation would use the procedure to define real-life trust situations in terms of the suitcase model (item 4.2), and plot the agents in the field, being able to trust using what is the most important to them in each one of the situations.
6.1 Scenario of Citizen Science in Biodiversity
A target for real-world study in Citizen Science in the Biodiversity domain is eBird, a very successful citizen science project managed by the Cornell Lab of Ornithology (E-BIRD, 2016); eBird’s goal is to engage people, mostly non-scientists, to register and share occurrences of birds from across the globe. In 2016, e-Bird was producing 5-10 million observations of bird species per month.
Two real world decisions involving eBird are of interest: 1) Should the eBird team accept the data collected by an eBird participant as valid? 2) Should a scientist use a data set published by eBird team? To answer these questions one would want to have a way to check the data quality and to know if the participant and the eBird team are trustworthy.
e-Bird address issues of data quality by using machine learning to quantify and control observer variability (KELLING et al., 2015a) and to correct both spatial and temporal bias in data (KELLING et al., 2015b). These processes are supported by a review system that is based on local experts. New initiatives to enhance data quality using the similarity of observer profiles are being developed.
Ultimately, however, people managing eBird must trust the observers, and people using eBird data must trust the eBird staff to provide “good” data.

97
6.2 Defining the Citizen Science situations
Our case study recreated hypothetical trust situations from the eBird project in the PlayGround 2.0. We tried to place in the field a sample of situations with potential to have different meanings in the eyes of trustor, so that the procedure could demonstrate how the suitcase model may be capable to define many meanings of trust and make them workable to be simulated.
We proposed for this case study three actors (Table 18): the eBird, a scientist, and a participant (‘a citizen scientist’). The eBird represents the institutions that run the eBird Project. The scientist is a professional of science that has interest in citizen science data, and the participant is a citizen with experience in bird watching.
Table 18 - Actors of the citizen science case study simulation
The actors (or agents) would interact under nine trust situations:
1. eBird believes that participants can report data accurately. 2. eBird believes that participants are committed to the project. 3. eBird believes that participants may work in groups. 4. Scientist believes that eBird have good data. 5. Scientist believes that eBird has a professional reviewer team. 6. Participant believes that eBird will protect its sensitive data. 7. Participant believes that eBird does not have economic ambitions. 8. Participant believes that eBird is a serious project.
We decided to define some combinations among actors and situations. This means that the scientist would define two situations, the participant would define three situations, and finally, eBird would define three situations.

98
These combinations could expose different reactions, because each actor might have a unique understanding about trust in each situation. For instance, the participant may expect from eBird a large number of participants as an evidence that it is a serious project, or it may be satisfied with a respectable institution behind the project. The participant may also have different meanings for different situations, i.e., the expected evidence for ‘is a serious project’ may be different for ‘do not have economic ambitions’.

Table 19 - Map of situations and expected evidence for the actors of the citizen science case study simulation

99

100
Applying the definition procedure
The simulation started by applying the procedure to define the situations in terms of the suitcase model components (item 4.2, pg. 75). Table 19 lists the 14 trust situations defined in this case study. Each situation is described with its trustor, trustee, expected behavior, context, importance, and until three expected evidence.
We decided to duplicate the situations involving eBird and the P1 actors, to create situations with the same expected behavior, but with different importance and context. For instance, the eBird expects that the participants can report data accurately in Situations 1 and 2; however, in the first situation, the context is ‘unfavorable’ and the importance is ‘high’, whereas in the second the context is ‘neutral’ and the importance is ‘low’. This difference reflects, in the list of expected evidence, which one is more demanding in terms of expected evidence for the more important and unfavorable situation.
Some expected evidence was also shared among situations, to make it possible that a piece of evidence would be collected from different situations in the future. For instance, eBird could collect expected evidence that a participant ‘share space and time with others’ in two types of situations ‘are committed to the project’ and may work in groups’.
Considerations about the definition procedure
Table 19 is an evidence that we can use the procedure to define different trust situations, with different meanings, using the suitcase model.
Possibly, other trust managers could select another list of expected evidence for all the situations in this simulation, or make an interview with the actors to find the real expected evidence, or replace them by quality measures (completeness, integrity, consistency, etc.), or even subjective values.
However, the possibility to have infinite arranges of expected evidence is a consequence of the suitcase perspective, which is one of the main contributions of this work. This possibility to use what is the most important for the trustor, in his eyes, or in the eyes of the trust manager, to make a trust decision is an important evolution.
Cofat’s model (COFTA, 2007) goes closer to this, but in a subjective way. Since

