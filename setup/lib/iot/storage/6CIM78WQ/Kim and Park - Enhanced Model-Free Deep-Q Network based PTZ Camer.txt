Enhanced Model-Free Deep-Q Network based PTZ Camera Control Method

Dongchil Kim
Information&Media Research Center Korea Electronics Technology Institute
Seoul, Korea dckim@keti.re.kr

Sungjoo Park
Information&Media Research Center Korea Electronics Technology Institute
Seoul, Korea bpark@keti.re.kr

Abstract‚ÄîRecently, the algorithm based on reinforcement learning for controlling PTZ(Pan/Tilt/Zoom) of a camera has been significantly increasing. To improve the accuracy of video analysis in a video surveillance environment, it is essential to get good video sources from a camera such as an appropriate object size or an object not covered by the other object. In this paper, we propose a model-free reinforcement learning based PTZ control method to support improving the accuracy of video analysis. The proposed method finetunes the DQN (Deep-Q Network) for smoothly controlling PTZ camera. The simulation results show that the proposed method can automatically and seamlessly manage the PTZ camera according to moving objects.
Keywords‚ÄîDeep-Q network; reinforcement learning; PTZ camera; video surveillance system
I. INTRODUCTION
Reinforcement learning (RL) studies the way that natural and artificial systems can learn to predict the consequences of and optimize their behavior in environments in which actions lead them from one state or situation to the next, and can also lead to rewards and punishments [1]. Such environments arise in a wide range of fields, including video surveillance. The video surveillance system was designed to provide a means for preserving personal security from various security threats. It also allows to record and plays live videos from multiple surveillance cameras such as PTZ(Pan/Tilt/Zoom) camera. PTZ camera can adjust its view by varying pan, tilt, zoom parameters according to the target location. The PTZ camera control can support the enhancement of the video analysis, but currently, the manager is controlling the camera manually. To solve this problem and help to improve the accuracy of video analysis, we propose an enhanced model-free Deep-Q Network based PTZ camera control method. The proposed method places the object or event position in the camera at the center and appropriately scales it. The proposed method helps improve the accuracy of video analysis by solving the problem that the size of the object or event is too large or too small to perform the image analysis well. Also, the proposed method can track an object in real time. To control PTZ cameras more automatically and smoothly than the existing algorithm [2], the proposed method fine-tunes the DQN (Deep-Q Network). The remainder of this paper is organized as follows: Section 2 describes related works for deep reinforcement learning. In Section 3, we describe the concepts and algorithms introduced in the model-free Deep-Q network
This work was supported by the ICT R&D program of MSIP/IITP. [20170-00250, Intelligent Defense Boundary Surveillance Technology Using Collaborative Reinforced Learning of Embedded Edge Camera and Image Analysis.

based PTZ camera control method. Simulation results are described in Section 4. Finally, Section 5 presents our conclusion.
II. RELATED WORKS
Deep Reinforcement Learning (DPL) has already been applied to a wide range of problems such as robotics, where control policies for robots can now be learned directly from camera inputs in the real world [3]. Reinforcement learning methods can be divided into model-based and model-free. Model-based approaches assume an explicit model of the environment and the agent. The model describes the consequences of actions and the associated returns. From this, optimal policies can be inferred. The model-based descriptions apply to goal-directed decisions, in which choices reflect current preferences over outcomes. Model-free approaches forgo any explicit knowledge of the dynamics of the environment or the consequences of actions and evaluate how good actions are through trial-and-error learning. Model-free values underlie habitual and Pavlovian conditioned responses that are emitted reflexively when faced with certain stimuli. While model-based techniques have substantial computational demands, model-free methods require extensive experience [4]. DQN [5] successfully uses deep neural networks as Q-function to approximate the action values Q(s, a; Œ∏), where the term Œ∏ is parameters of the network and (s, a) represent a state-action pair. Two essential ingredients used in DQN are target network and experience replay. The parameters of the neural network are optimized by using stochastic gradient descent to minimize the loss, where the term Ì†µÌºÉÌ†µÌºÉÃÖ represents parameters of a target network as (1) [6]. To automatically and smoothly control PTZ cameras, we fine-tune DQN architecture.

ÔÄ†

(Ì†µÌ±üÌ†µÌ±üÌ†µÌ±°Ì†µÌ±° + Ì†µÌªæÌ†µÌªæ mÌ†µÌ±éÌ†µaÌ±é‚Ä≤ x Ì†µÌ±ÑÌ†µÌ±ÑÌ†µÌºÉÌ†µÌºÉ(Ì†µÌ±†Ì†µÌ±†Ì†µÌ±°Ì†µÌ±°+1, Ì†µÌ±éÌ†µÌ±é‚Ä≤) ‚àí Ì†µÌ±ÑÌ†µÌ±ÑÌ†µÌºÉÌ†µÌºÉ (Ì†µÌ±†Ì†µÌ±†Ì†µÌ±°Ì†µÌ±°, Ì†µÌ±éÌ†µÌ±éÌ†µÌ±°Ì†µÌ±°))2ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ®ÔÄ±ÔÄ©ÔÄ†

III. PTZ CAMERA CONTROL METHOD
To automatically and smoothly control PTZ cameras, our system has to receive the position and size of an event or an object. In this paper, we assume that the location and size of the object or event information are received in real time from the video analysis system. The goal of PTZ camera controls is to place the object or event position in the camera at the center and appropriately scales it.

978-1-7281-1340-1/19/$31.00 ¬©2019 IEEE

251

ICUFN 2019

A. Network
The proposed method fine-tunes DQN architecture. DQN can outperform humans in various Atari games. Using the visual information of the screen as input and the game‚Äôs intrinsic reward, the DQN agent learns the Q-values of the individual actions for the input image. To deal with conventional instabilities of reinforcement learning when dealing with highdimensional inputs, a technique called experience replay is utilized to decorrelate the observations and to smooth out the distribution of the training data. Instead of using the observation in sequence, the network‚Äôs observations are saved in memory and randomly sampled when updating its weights. Unlike the Atari playing DQN, the proposed method does not use the image as input data but uses x, y, width, height information of the object or event received in the video analysis system. To train PTZ camera control actions, we modified the DQN structure as shown in Fig. 1. We update the parameters of a network(Œ∏) that estimates the value function, directly from on ‚Äìpolicy samples of experience, st, at, rt, st+1, at+1, drawn from the algorithms interactions with the environment. We update the network parameters to minimize the error difference between the current Q value and the predicted Q value as (2). We train PTZ camera control value using modified DQN.

Input Layer size : 4

Hidden Layer 1 Hidden Layer 2 Hidden Layer 3 Hidden Layer 4

size : 128

size : 128

size : 128

size : 128

Output Layer size : 8

‚Ä¶‚Ä¶

action value by using (5), (6), (7) when PTZ camera movement about Pan and Tilt occurs. Where errC is the difference between the position of the object and the center of the frame in the state before the camera movement, errN is the distance between the location of the object and the center of the frame changed after the camera movement.

Ì†µÌ±üÌ†µÌ±üÌ†µÌ±ÉÌ†µÌ±É = {Ì†µÌ±ìÌ†µ1Ì±ì (Ì†µÌ±íÌ†µÌ±í+Ì†µÌ±íÌ†µÌ±í3, Ì†µÌ±í,Ì†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í)Ì†µÌ±í,Ì†µÌ±í‚â§‚ÑéÌ†µÌºèÌ†µÌºèÌ†µ1Ì±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í

(5)

Ì†µÌ±ìÌ†µ1Ì±ì (Ì†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í, Ì†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í) = {‚àí+11, ,Ì†µÌ±íÌ†µÌ±íÌ†µ‚ÑéÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µ‚â§Ì±íÌ†µÌ±íÌ†µÌ†µÌ±íÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ†µÌ±íÌ±íÌ†µÌ±í

(6)

Ì†µÌ±üÌ†µÌ±ü = Ì†µÌ±üÌ†µÌ±üÌ†µÌ±ÉÌ†µÌ±É ‚àí (Ì†µÌ†µÌ±íÌªºÌ†µÌ†µÌ±íÌ†µÌªºÌ±í1Ì†µÌ±íÌ†µÌ±íÌ†µÌ±í)

(7)

The Reward for Zoom is calculated using (7), (8). Where
errCZ and errNZ are the object size information before and after the camera movement respectively, Œ±1 and Œ±2 are the normalization constants 100 and 10 respectively, Œ≤ is a constant 1.2 reflecting errNZ and Ì†µÌºèÌ†µÌºè2 is the target size 70 of the Zoom.

Ì†µÌ±üÌ†µÌ±ü

=

{2

+

‚àí1, Ì†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í ‚â§ Ì†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í Ì†µÌ±ìÌ†µ2Ì±ì (Ì†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í, Ì†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í), ‚ÑéÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í

(8)

Fig. 1. Network architecture for controlling PTZ camera.

min
Ì†µÌºÉÌ†µÌºÉ

‚àëÌ†µÌ†µÌ±°Ì±áÌ†µÌ†µÌ±°Ì±á=0

[Ì†µÌ±ÑÃÇÌ†µÌ±Ñ (Ì†µÌ±†Ì†µÌ±†Ì†µÌ±°Ì†µÌ±° ,

Ì†µÌ±éÌ†µÌ±éÌ†µÌ±°Ì†µÌ±° |Ì†µÌºÉÌ†µÌºÉ)

‚àí

(Ì†µÌ±üÌ†µÌ±üÌ†µÌ±°Ì†µÌ±°

+

Ì†µÌªæÌ†µÌªæ

max
Ì†µÌ±éÌ†µÌ±éÃÅ

Ì†µÌ±ÑÃÇÌ†µÌ±Ñ(Ì†µÌ±†Ì†µÌ±†Ì†µÌ±°Ì†µÌ±°+1,

Ì†µÌ±éÌ†µÌ±éÃÅ |Ì†µÌºÉÌ†µÌºÉÃÖ))]2ÔÄ†ÔÄ†ÔÄ†ÔÄ†ÔÄ®ÔÄ≤ÔÄ©ÔÄ†

B. Reward function
Before defining reward policies, we defined action values for PTZ camera control. The action value of the PTZ camera is defined as (0.02, 0.06, 0.1, 0.14, 0.18, 0.22, 0.26, 0.3), and the defined action value is selected as (3). The action value Ì†µÌ±éÌ†µÌ±éÌ†µÌ±°Ì†µÌ±° is randomly selected only if the reduced …õ value is greater than the threshold value rand, otherwise, the best action value is selected at the current state Ì†µÌ±†Ì†µÌ±†Ì†µÌ±°Ì†µÌ±°. …õ value is updated as (4).

Ì†µÌ±ìÌ†µ2Ì±ì (Ì†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í, Ì†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í) = {3‚àí+((Ì†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ†µÌ±íÌ±íÌ†µÌ†µÌ†µÌ†µÌ±íÌªºÌ±íÌªºÌ†µÌ†µÌ±íÌ†µÌ†µÌªºÌ†µÌ±íÌªºÌ±í21Ì†µÌ†µÌ±íÌ±íÌ†µÌ†µÌ±íÌ±í√óÌ†µÌ±íÌ†µÌ±íÌ†µ)ÌªΩÌ†µÌªΩ),,Ì†µÌ±íÌ†µÌ±í‚ÑéÌ†µÌ±íÌ†µÌ†µÌ±íÌ†µÌ±íÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µ‚â§Ì±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ†µÌºèÌ±íÌ†µÌ†µÌºèÌ±í2

(9)

C. Training

To train PTZ camera control values, we collected the number of PTZ camera control actions about 4000 as shown in Fig. 2. We generate the random number of events based on a predefined camera preset so that the position of events can be randomly generated. It randomly assigned 335 locations for each preset to collect about 4000 training data from a total of 12 presets.

Event status information before camera moves
(position x, position y, width, height)

Moving prediction value and Reward
value (moving value, reward)

Event status information after camera movement
(position x, position y, width, height)

Episode Termination Conditions (episode done flag)

Ì†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±éÌ†µÌ±éÌ†µÌ±éÌ†µÌ±éÌ†µÌ±éÌ†µÌ±éÌ†µÌ±éÌ†µÌ±é, Ì†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±üÌ†µÌ±ü ‚â§ Ì†µÌºÄÌ†µÌºÄ

Ì†µÌ±éÌ†µÌ±éÌ†µÌ±°Ì†µÌ±°

=

{Ì†µÌ±éÌ†µÌ±éÌ†µÌ±éÌ†µÌ±é

max
Ì†µÌ±éÌ†µÌ±é

Ì†µÌ±ÑÌ†µÌ±Ñ (Ì†µÌºôÌ†µÌºô(Ì†µÌ±†Ì†µÌ±†Ì†µÌ±°Ì†µÌ±° ))

,

‚ÑéÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±íÌ†µÌ±í

(3)

‚Ä¶

Ì†µÌºÄÌ†µÌºÄ = Ì†µÌºÄÌ†µÌºÄ √ó Ì†µÌºÄÌ†µÌºÄÌ†µÌ±ëÌ†µÌ±ëÌ†µÌ±ëÌ†µÌ±ëÌ†µÌ±ëÌ†µÌ±ëÌ†µÌ±ëÌ†µÌ±ëÌ†µÌ±†Ì†µÌ±†Ì†µÌ±†Ì†µÌ±†Ì†µÌ±†Ì†µÌ±†Ì†µÌ±†Ì†µÌ±†Ì†µÌºÄÌ†µÌºÄ > Ì†µÌºÄÌ†µÌºÄÌ†µÌ±öÌ†µÌ±öÌ†µÌ±öÌ†µÌ±öÌ†µÌ±öÌ†µÌ±ö

(4)

‚Ä¶

After moving the PTZ camera according to the action value selected from the defined action value, Reward is adaptively applied to the positional change of the moving object. First, we calculate Reward corresponding to the movement direction and

Fig. 2. Collected training DB sets.

252

IV. SIMULATION RESULTS
We evaluate the proposed method that can automatically and smoothly control the PTZ camera according to the movement of objects or events. We use the programming language of Python 3.6 to implement the enhanced model-free DQN based PTZ camera control method. The proposed method received the position and size of the object and event from the video analysis system. To test the proposed method, we generated a random number of events from eight pre-defined presets. It collected 240 test data from a total of 8 presets by specifying ten random positions of objects per preset about three cameras. All tested network configurations were trained for 35 epochs with fixed learning rates, as shown in Table 1, with no early stops applied.
Fig. 3 shows the results of PTZ camera control based on moving objects or events. The average accuracy of the proposed method is 61.4%. The reason why the average accuracy is low is that the location and size of the object or the event received by the video analysis system are inaccurate or lack of data learning in various environments. Fig. 4 shows that the implementation results of the proposed method in real situations. The proposed method can position the object in the center of the camera regardless of the position of the object in the camera. The proposed method can automatically and smoothly track the object by control PTZ camera. Through this experiment, it is possible to improve the accuracy of video analysis by appropriately adjusting the position and size of the object using the proposed method.

TABLE I.

HYPER-PARAMETERS VALUES EXPLORED

Hyper-parameters Optimizer
Loss function Activation function
Learning rate Batch size Epochs Episode

Vales explored Adam
MSE(Mean Square Error) ReLU(Hidden layer) Linear(Output layer) 0.001
128
35
1000

Accuracy

Camera1

Episode Camera2 Camera3

Average Accuracy

Fig. 3. Simulation results of the proposed method.

Fig. 4. Implementation results of the proposed method.
V. CONCLUSIONS
This paper proposes the model-free reinforcement learning based PTZ control method to support improving the accuracy of video analysis. The goal of the proposed method is to automatically and smoothly control PTZ camera to minimize the distance between the event position and camera center position and adjust the size of objects properly. Through simulation and experiment results, the proposed method automatically and smoothly regulates the location and size of the object by controlling the PTZ camera based on the modified DQN algorithm.
REFERENCES
[1] P. Dayan and Y. Niv, ‚ÄúReinforcement learning: The Good, The Bad and The Ugly,‚Äù Current Opinion in Neurobiology, vol. 18, pp. 1-12, Sept. 2008.
[2] D. Kim and S. Park, ‚ÄúAutomatic PTZ Camera Control Based on Deep-Q Network in Video Surveillance System,‚Äù Proceedings of International Conference on Electronics, Information and Communication, pp. 479-481, Jan. 2019.
[3] K. Arulkumaran, M. P. Deisenroth, M. Brundage, A. A. Bharath, ‚ÄúA Brief Survey of Deep Reinforcement Learning,‚Äù IEEE Signal Processing Magazine, Special Issue on Deep Learning for Image Understanding, pp.1-16, Sept. 2017.
[4] Q. J. M. Huys, A. Cruickshank, and P. Seri√®s, ‚Äú Reward-based Learning, Model-based and Model-free,‚Äù In Encyclopedia of Computational Neuroscience, pp. 2634‚Äì2641. Feb. 2014.
[5] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland et al, ‚ÄúHuman-level Control Through Deep Reinforcement Learning,‚Äù Nature, pp. 5829-533, Feb. 2015.
[6] Z. Lin, T. Zhao, G. Yang, and L. Zhang, ‚ÄúEpisodic Memory Deep QNetworks, ‚Äù Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, pp. 2433-2439, May 2018.

253

